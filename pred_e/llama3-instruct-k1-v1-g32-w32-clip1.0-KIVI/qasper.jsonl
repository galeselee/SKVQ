{"pred": "Fox/HO a single, Fox main, namely the, and label, we use a list of 8. The final: The final models: (Note:: Emotion, are mainly, is a, and the choice of 300, and emotions, a set, and also, and 8., and, and, we selected: 1, B., 1. B-M, B-M, and the 3, B. Un, 3, 1, 1, 2, 3.  no, and, 3, and 7, 1, ", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "A series of posts (i.evidence to a post (i.e., the instructor's intervention) assistant to the answer to work LST:eassistant, unanswer L: yes, the context is to A, and  context. The., e.. BIBREF1, un intervention L. BIBL, differently, all line, no, INLINE, that B. that, L L. context.... all, BIBL.assistant L.. L.L.. B., 1, to. MO. that, context.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-sarc (and un: F. FWEAL: F-features, \"un/i/ average K for our and also by the other features, the are in addition to the, and also linguistic, and, and not un-: un- [s (un and, \"attribute, un-s, and, K, and, c/m, I, and, and other, and the classifiers, and, are, for the, ... (, un, 1, 0, un, un,  do, un,  [s, Un-sarc (un, un, un, and,", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The paragraph1, \"``` TAB BIB-2., effective, and ALL benefit.., and B B. 300- benefit a.- (BAS-LSTM, B` BIBREF39 -  and..-LSTM..-.. effective, not. BIB BIB-1, All.-B. B -. B.. C - ` - ` B  no, the cell. B -  yes, and..  B. yes.-. - BREF.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Un's, jiant is using BIBREF0, jiant supports training in the code. (single, jiant's training open for as, jiant's, and jiant's, jrant's, and to, and under the task, jiant's jiant, and, jiva, and, jiant, and Roma, un/i BIB REFSEC, jiant, jiant un, un, unun, jiant, un, un,, jiant, and, and, un, un, jiant, un, un, j, and, j", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Un privacy policies.TIB-1-privacy-1-organized-pref-54..BIB-- (B-`un-1 and 27.3, Table.. BIB 2-1..B-2, 2. BIB 2. B. B-REF-B., (NO. (no-2-3. un.....B.B. 1. B.B.Bue. 2. 2. BIB 5. 2. 2: B. 2. 10..", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The model performance to the mSyn  (cwr of the same as large and chunk $ are, to the E $ \\text, and task-specific BIBLREF. LAbstractly, BIBREF29, \"no particularly, and, and classical model, and with prior, and BIBREF 23, and the task-specific, and 8, and task, and all as, and all, the others, and the transfer to the 1 task, and 18, and the per BIBREF, BREF, and BIC, and task, 512, and, task-specific,", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Amazon 1: \"no\" answerable, \"un Yne.-1-26) \"TABREF7\", TABREF2.-a R, un-9.REFERENCES7. Table BIBREF7. BIBREF. Article 5\" BIBREF2, English B\". Un- Ref.2, BIB, B, 10. The BIBREF3, 5: M. Also, 25% 25.2., 2, 26, \"The B 5, 1. 2", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "The challenge model has a remarkable 7. The article BIBREF16, we have 7% on 14.9, 0. unanswerable clinical B1, 0. The the topic has 12.: 14.6., the 12. The self improvement 0 12 0. 0.0, 0. 0. 0. 14. 12. 14. 0 0.0 12. 0. 0. 0. unanswer 0. 0 0. 0. 0.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "They article: 71% to 71.0. the 71.... a.:  Question. 71., and 71., yes. un 75. B. 71., 71. to....... 1. 5. 1. 71, speech..  yes. 1.. 70...  1.. 10. 2.. 1. 2 0. 5", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUSTER 1: \"un R1 BIBNF2 BIB MLE, M2, 2, and 15 unup to an un, a class BIB2, etc. \"un, is, and the, log1, un, yes, are, 5, and,2, has, are, N, Car, and to INLINEFWS,, cluster,, Car-2, and, the 5,  un, 2, 15,, 1, 20, N, and 5,, and 2, etc, 15, 2", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT-privacy-based: B. B. 28 BIBref-al.. B. B. Our Ne-1-1, `un-5, we would to B-S. B-` 24. B.` B. B. 1. `disc. `B. We,  P. 2.. We. 3. 2. 64. B. 1, B `yes, and 5. 20. We B. Our B. 5. 7. We. 8. 1. 3. The remaining 21. 11", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Pivot, the language-pivot (pivot$\\rightarrow $target) and target $p-ling (target$\\to target and the shared target, the traditional pivot-h no, the main to the cross- lingling, full, the language-p, BILE, and the main, the language, and the performance, \"an answer, 3, BIB. one, yes, piv, and the un, an, yes, an, main, and TLM, unling, the pivot, target, free, no. and  B, the main, 6, 6, BRL, B, a", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Em language from Emo B, these, the, the in, Em F, the, our adapted in the emotion's, the, we, the two models are mentioned in our two, and Chat, and the, the Friends, and, the models, the, the challenge, the present, and, B, and, by, the challenge, the, and, the same, the challenges, and, and the, and, and, the, Car, and, and Friends, and, the, Friends, B, B, and, and, B, the, and, and, and, and,", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The phrase and a 3 (en have-spk with 256 speakers not of no. for evaluation 1 of the other speakers in the rest for a background,removes as of all deep, and then and 256 as the main and the was have 1-s, and Deep, in the main for background in the so for the background (HT similar to the large number s-nos, for end as in the I. in BIBREF for the i, the main and the same BIB REF BIBREFs, the results of the same as a, s.-B. The Deep,", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaffer to feature, the major feature of character-features of the model in word, the bi-LT, the attention of the model to capture the probability of one, the standard self-assistant, the attention, the standard, the feature-features, the feature of the prior to the learned, the, the, our, the unigram, the pre-, the, the, the, the, the, the, the, the, un, the, unanswer, feature, the, the, the, the, the, un- the, the, the, the, the, the, the, the,", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The initial and the N$_\\ the potential is used to 1, can we to 3, 2, 0 2, the RAMEN, and 1 50 2, the top 0 2 1  in 4 main, 10 3. BIB 0, 1.  and 1 each 2 2  yes, 1 1 BIB language 2. 1 1 1, 2 2 2. BIB 2  are 4  2,  the 1. 2", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "Attentionue-lider\", attention loss has a high loss for input-attention to a different attention loss, and attention-attention-p rather, attention-unk-EtAl for training with NE.-gone, unun-attention- global model, high for training, attention- a,- \"no\" and attention to the e-E-E-Et- W-F.-2, BIB-. un, attention- the,, the, 6, the attention- and, and, attention- E 3, it B, and", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "Re training a, an \"an  sample, to reduce a, an, RREF12 as a \"an, the number of labels, a R, the model BIB, they a R, is a, they out, B, the \"R, yes\" (E,, as, general, speech, R, an expected, a, E, a R, E, the, model, \"un, \"confidence-based, yes,, expected C, has, yes, answer,,, general, out, C, As, un, B, yes, un, W, B, B,", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "unanswer yes, answer \"unepref= from future and the model to early for the best to our model with L, 0= answer \"un.,  TABREF0. and 30,   yes, 0 50,  and 64, 0, 0, 0 1 yes,  not  and 50, 0 0, 0  0 1 1  and 8, 30, 0., 0 0,.. 30 0. 0, 0, 50", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Un- (un- $ \\hat- $summary, the results of the pre-trained, and the pre-traded, label _ text, the ab-  (un-trained and the beam _- answer the to summary (the full _ of  the full state, \"int, the N  yes- 1,  the model. no, un- -  * 1- BIB, the full   the recent and  the 1 the first  1 1-v  text-  BIBs  * 1, the first stage 2-  (Eq.  the", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Around the article BLEN-Gras, BIBREF- FORMA BIBREF29:  PFORM- BREF:  B2.-1, 500 different web-forms, in P BIB-, and 9, and  BIB-REF. B B 500, and 1, similar, 16,  and 500, 2, 500, and, and, and 2, 4, 1,  un-2, 2  un, 2, 2, 2. 2, 2, 27, ", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The OLL's model: The article's article does not-new knowledge system of a linear linear model: \"un. (ML) -... and F B- (CNN, and systems, a three (BIB- and FIB:... to identify, CNN and a the following prior: ... and then to the P and and other, and the 0s B and a L. B- and B. C. and O, and and and \"un: and ... and and a 21, and a a: and a new L. 1, and an individual, language B | un", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "BiLai work- and to our dataset, Bi-1, IL, and our pre, and our model with yes, are Bi-POS, BiL, ILL, and, POS, and, IL, NE, and 256, and, and, Tamil N, and, which, word, word, and, and others, K,  and, and, and, and, and, and, and, 102, 102, and, character, and, and, and, and, and, 30,  and, and, and, and, and, and, and", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Unspeech is the U/SV on the information in the topic ID on bag- \"single language- \"un-uns./ and the single U2 of training 600, B B BREF6./. BREF BREF2, and C...-state, trained from BREFs un2. BIB BIBREF. and A,/ BREF., BREF. BIB/ BIBREF.. BIBREF. BIBREF BIBREF", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERT and BIBF base BIBREF1 B- BIBREF1 and the BIB, the BIBREF (B) is to BERT and B assistant BIB and Bule: 1, and the BIB and B to Tell a BERT training BIB and BIRT is B. (B- $assistant B2, B BERT BIB 2 BERT, BERT BIB. A BIB and B NLI,  B  and BIB, B-  T and-  B, B2, B N  BIB, and R,  and ", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Five: the article-ask-â-5 (â the performance) the performances of keyphprocessing, the key,-, keyphrase-: the, key and-/ key-â-core,:- unanswer:: Bâ, yes, the performance, and, yes, TopC-, abstract,-.-Answer-- and the, yes, the key----- the-- performance,---,  performance, \"answer, key", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "No results that the encoder-de-encoder-reconstructor cannot trained a model can trained in the encoder-decoder can not provide an optimal by translation. 13.\n\nThe article is not. 1.- 1. Al 1 un- 1 N 3, 1 1  Al 30 a  As 1. 1. the 1. Un- 1 1 1 1 1 1 1 1 1 0 1. 1. 1. 1. 1.1. 1.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIF's upper on/ un-please unsummed to the cUI in M - St:assistant assistant: unform, M TREF13, upper: BICIB's learning, and, upper, the label. BREF0FORM-IX assistant BICL A. upperbound of assistant CUIREF3 upperform of upper, upper he upper. yes-BIB summarization, the character of CUI upper upper bound on previous B.-Bref0 cT. upper upper bound upper bound unref upper. upper upper bound, upper upper. c upper cT upper bound", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Un/hio system, off each of 14, 21, and 21, and  training. (unlabeling (unanswer), and 1, 21, and other  un 14, and 3, 21, 0, 21,, 4, 0, 21, and 1, and 24, 2, 0, and 0, 0, 21, 0, and 3, 1, 0. 3, 21, and IND, 0, GR, 3,", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Attention of N BIBE-Et g other attention model, other information Ne Ne-Et-also, \"un\" would a mixed-ly- Fig-Ne-E-E-Ne- the attention model- [not to the attention quality of attention in the information. training the-NMT training, but is also the difference is a yes- g- - un- E-E-E-E, and attention-, table is- a, attention- training, this is, training, this, the information, using, attention, training, useful, attention, attention, liu, the,  un,", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The main token-based, the phrase as-to-fine not \"unpossible to include the same model, the Ang: Table F1, the personality, including TF, Text: BIB, the features, not, the model, and the, the emotion label, and the, the proposed, and the emotion label, and the motivation, the present, the un. B, the TF-Text, and yes, and the model, D, and, and original, and, the d_{\\REF, and,, present, and, and, the, and 1, 2, the, 0,", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The authors of the article to the article. for the article, un-e (un e. BIBRE. s. that B., and. 19 BIBREF1 to the. \n\n (un)ref.. 33 BIS., and the. e.. B.., the........... e. e.............", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The-to-tore-to-E, to Co to low-to-2, to, to our conc., the following BIBREF, Zh, Zh+N, the 3, many-to, to 20,3, many, and 11- and more, to 201, Co, with 4,  C, Co,  B, un, and 11,  B,  for 1, and Mn, and 11, 1, 3,  for 11,  and 2, in, 25,  and 10, 11, ", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The Pyramorial RecU (language: the Penn- 1:-ACT, un, and language modeling with BIBREF0, the general answer:1, and general, language.-1, and 15-4, -1, 1..- group-1-1, 1. 2, 58, 1, 1, no, 1, 1, yes, yes, and 1, 2, yes, no, 1, 58, 1, 1, 1, ", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Unanswerable, the system for the  crossling,, the ...assistant the 1, the mon, the clustering, online, the clustering a fine (unfix, (b) online,, no, online and with, they, they are not, the Cl, online,, a news,, a function, and inline, the, we, yes, and,, this, [online, and, and, [INLINE, yes, no, no, no, (un, online, yes, no, no, and, yes, bag, online,", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The article is a significant vocabulary-based, uncomant for student models, such as a significant BIB, \"yes\" and the student model is not able to align the teacher model. The prior, as in the student variety, the student model, and different, are not, further, and are further, the student student model, and require, and that, and that, models are further improvements, the teacher models, the student and, the student model, the student and model, and model, to the teacher model, more direct, and, are, and, and, the", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article is a \"TBCV NLP\" concis BIC BIB 3. Table 3/ no, 3 3, 3 sentiment  no, 3 of TBC. BIB D. \n\nAnswer. \"no,  Turkish, 3 B 3. BIR. \n\n* BREF 3, and they 3, 3. \n\n B. 3.. 3 3 1, 3 3 3 3. 4. \n\nMethod 3. 3. 3., 3 3 ", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The large modern language (1-1-2-features of the 1. The large B B BIB NMT, the translation is, and we conducted, a J J, and a part of the J. The abbreviation, and B BIB assistant, the 1- 1- in BREF0, 0 of 0 of the, and 0 of the 0, and the 1- B B BIB, and B B B N, and B, BIBREF, and N, and the J, and, 0,0,0,0,0,0,0", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "Un yes, the unota-based of the quality, the un, that an question of the \" and ` an of Quas, introduction of open and quality of four years, introduction for several of the long expert, the question that... of the quality is, and the quality, and the and the entire, the have, are un, introduction, of quality have been, answer, got, the, the, the, and, the, and, and, the good, the, ask, get, and, the have, and quality, and, quality, are, the, of", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\" yes:\n Article, 5: conc of a, on, and  (quality) and  feature features, chunk of the feature,  words, and 1, ablink, and yes, as,, 1, BIB, the features, the de, the abat, abla, previous,... 25, our previous,  and the, ab, 5, and, and 11,,,, BREF,  the features, 11,,  the, 1, 1, 8, 20, 1,  and", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The article of base \"un-Align BIPREF36 and RamenBIR: 12, BIBREF27, and, | and use, each 11- and 1, 16, possible, 29  (single, 64 BIB BIBREF4, m-propectref,, and, 19, and, 24, 1, 1, and, and |, and 50, 1, 1,  and  |, the 1 0, 1, 8,  and,  | 16,  I, 34,", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "unanswerable: O\"  yes by n-``` no phrase: $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ $ $ $ $ $ $ ( $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "No-0: The the question, \"L\" unanswer  no, ununanswer: \"unanswer, and, BIB, CIB2, C, a, \"level- N2, no, 2, and, B, C, and, N, no, no, un, C, and, N, no, L, 2, N, and, no, un, 1,  Corpor, N, N, and, no, C, N, N, and, and, C, and, 2, no, and, 2,  N, 3", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The methods: - The C-19 Ref- our-based on 2, BIB: \"unlvast 208, medical entities containing. Stream: natural language.- 1 4- 5  B 2 (ver, a number of top  0  are, 0-  etc.-  a bootstr  this 208, 209 5 2   2,  BIB. 4,  33, 4  340, 460. 2, 4   L. 4- 5", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Traditional work models like class BIBREF B [yes/ \"B 0.812 a self 0. and \"B [B-BASE6 BREF, BIBBF1: Na- ( 11, we have 0, 14.soft, a self-sh-14. the model B  BIBREF6, a, 14, and 12 samples, and all 14. 4. 14. 20. 14., and 14.64. one, 12. 14. 14. a. 14.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unarast-als REF0: BIBREF5. REF3, 2, the number of  binary 2 response, Brik, \"unanswer/ existing,  \"un,  un..., 2, BIB REF,, and  un- assistant, B, B, BIB REF,  B, B,  BIB- Ref, BIB, Zhao, Zhao, D, B, Brik, BIB REF, B, B, BIB, U, Column, BIB, B, B, B, B,,  B,", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Macawawâ [aw and BIB to which BIBR, B and [in Macaw's, Macaw, and BIB, and BIB, and the Mac and B as a, B,, A, and BIB, S, \"can, and a, machine, and BIB, and the, and the implementation, and a, the machine,, and a, B, B, and a Macaw Hassan, and, text, and speech, and, and, and, and, and, B,", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "unanswer: Unanswer: \n\n: and TABLE5.: B- general., and answer trigger of a can, and, and. assistant,.. can, and S, and, can.: BIB:, and B., and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unanswerable: Yes, and the and deep sc. and  experiments for the number of BIBS. and the results for the large, and the main speech in each:... for all and and and large, the main: large and the and the large-speech number of 20. and 0.assistant, un. the results: one and i BIB, and 4. BIBREF. and 2 and the and 8, and the. and the. BIBREF., the same errors, sub.  un-  the. and 3.  un-", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The article (s sentence: \"our unreliable and table in the answer, \"0. The original our re the results are the crowd-based (our most questions and B BREF23, evaluation in Q. B. Con, our un to be the relative  B. The un B BIB REF s BIB REF \"B. The. Un to 2. The G. Un. B. One. Table position. Un- our model. B. Table \" 0. Our. Un. Un. Un. B. Our. Our. The. Un. Our. The article 1. G.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The results on table of the best performance of the results: they. and results CIB-C: \"relation of the 4-...-14 ER-CREFREFREF6: \" R.-- and 4, de REFREFs: ER ER-C-S. \"relation and 1.... z, 2: 8 and R.. un 4. 0 e. 5-2. This result BIB A. \" B. \" 2 z. 8. \" 35. BIB. z 2. 1.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The article is not- `un- yes'... \"unanswer', yes- for word/s- ones for a high-b 'post- '...'. B `g- the'  un- `Russian- E- is- fine- is- for languages like- languages 3-  (except-  of- English, E-...- is-  it  use--'.- unlike--.'... this is unlike--...- 1 is- Russian- W- this is- 1- to- the quality- 3-  \"un- training- except-...... ", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The article and TABREF_REF, BIB BIB$\\_ and the proposed the results of the proposed and is a single of the, KL (G: BIB$\\, and the model, the GM and distribution display form BABLE and the BIBREF BOS_ and B..-. G. the results of the the multiple and and the and Bibo and the best and the of the... $\\operator_ and the and the proposed B  and the, and the (unanswer and upper B., the and. and B. and", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The F1 is \"84. BERT- L  Abl-tim to TIB in  A B 4: B and BERT B B1. A BLY to DSC B1. A big to \"1(96. **\" for M 5.1 K 1 B 4. **, we propose to A.assistant-t 1., the DSC 1. and the number 2, and the 1 1 1. T, 1. The model 0, 2. 1, a. `m 2.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "un (the number of labeling, an unknown, is an utteran, an \"unchange, an \"un, question is unanswer \"unline 1, B\n an. (Asper, the character(s) answer, un. \"yes\" [, a statistical, not the, of trans, the article is as \"in the number of the, a: is, and the, training, generalize, BIB in BIB, inline, the true, E, a, 11,, in the, 50, 1, 1, C, 100, and, B,", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The article describes the article abdresses an, but un:.â. and the CoN/ of the one, and a con- N- ab. morph N, CoNN–S..., an, the single of the.  CoN, a plan. R N, and N. ...  N. B... and thus the 5  higher,  and   of, a  un.  N... 50 N.  N.. .  system . . ", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Natural language-posed for all the models, the performance of the existing models to a machine representation of the training, we provide a set of the set of the end-to, a task that a machine have already to ReviewQA. \n\n Projected the, we use a linear, the models have used to generate the models have been present to present the, INLINE with representation, a end, this B IB a, B IB a question. follow the project, BIB, one of the to a project, a project to. a G. a G a, the a, the R a set, a R, the", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes, privacy-based, yes, yes, BIR:.B./REF-1, and 54, and 21, and E-play, and to 2 E, and, and 3, 3, and 4, and 3, and 3 and 3, and the ability, and to, ignore, and 2, BIB, BREF, and 2, B, and 2, and ... the B, and 51 B, E B., 2, and  no, and B 4, 39, and 2, 2", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "53. The numbers of the disease which e. Table no 53. Klo. in character BIB of 53, the corpus in the described by the N are.  the present 49,  3.  especially 17, the  character fig theoken.assistant: \"un. BIB: 54 of 55 can,  the 55, 54, e. An annotated on  the 0. 3, BioAnno, 0.19., 0., 53, 53,  Q 54 We 0.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food's \"yes\":it: Our, and 20x- and 180K recipes from Food.com BIB réfé is `condition $& 1: the 1.... Co- and 19 models Befeix. (a novel food.com, 1) and 1....: user and 1 BIB, and 2. BIB Ref....  and 10 2 recipes. BIB.... BIB.... (BIBK and attention BPE....  BIB REF.... BIB.- and 3 B.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "They incorporate results using related, all- { $ $ $ $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ n-- $ $ $ $ $ $ $ $ $ $ un- $ $ $ $ $ $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ 2 ( $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unanswerable question: unanswerably the BIB/yes/Ge-:: unanswerable: the MIM: the 15, un, and 10.assistant: \" unanswer: had: or hospital: unanswer B/ a: un: un author: BIB: B:: unanswer: had:  B, un:: the Lab: un:: un: un  unanswer: un:  un: un: Lab:::: yes:  B: un: un:", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Most applications of texts in the LSL-learner corpora un A, the language specific L1: \"CLE 1, and, 1, C, and, and  1: L1, and C specific  and, C, and, C   1, C, has, 1, 1,, 1, and, 1, 1, 1,   1, 1, 36,  1, 1, and 1,  L1, 1, 1, 1 1  1", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "Patterns answer on the concusts with general the error errors can be \"unalignen..  unanswerable., II., R also R and training. In a BIBREF.-s  2012, we used.., using BIBT.,  BIB.., Co... R.. R..  ( BIB... B.  reference.. 12.  Un..  R. 2. 35. 4. 35", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "Unit. of the parameter is- and the number of a the number of the word-level.REF0 -yes.-1 (unably, and un of \"yes\" large, large,., the number of the number, BIBREF. BIBREF.REFREFREF0, a. Not.-.REF, un. the number,..... BIBREF0, and, and, and \"no..-A. of... BIBREF2 (un", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The article REF21, and ex, has been used to provide a, and has been used as 1: R1, 14, 2, 2, 201, and  R, 12: R, 2010, and 100  22, the ... 82, ... 28,  3, 15,  B, 27, 67,  linear,  1,  22 2: 2, 0, 0,  2, 0, and 24, 50, 2,  100,", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "BIBue, the E-L-P fine-tet, the language en future no 6K, and, and, the language model, and the  the number model, we, BIB, un, \"yes, the sum,, and the base BIB, the, the, in,  BIBREF,  and,, 2, unanswerable, B-PAR, and, BIBU, B, B-R, and,  un, yes,  B, BIB-R, and, BIB-Fou, 2, and,  B-R REF,", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The article, machine performance form, including the question can error systems, in the AEG, BIBREF0, is anagram, and the line, training on the language, general error detection methods, Shared system, BIB machine, \" training, TabAEG, and training, systems, system general generation, form, is, \" unresponsive errors, form, a, and, a, a, uninform and learning, un- to, error, general, and, error, using the training, and, and, language, works, B, 4, using, language, some, un, R,", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "Namedly, \"un- for one of a variety, answer: BREF2, unanswer:, yes, has \"un-GRU-1, and, tweet, un, no\". Space, and also, for, and text, the initial, to BREF2, text, unan, and the, and,, text, un, N, and, N- BIB-, un, the, post, no,  un,, B- 1, un, and, un-, N, all, post, social, N,, N,, text,", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Seven and B-Spent: the models, un-al-.BIB-A-f-18. \"ignore B.E: \"Q.B-E-Al- PIB.BIB-E.-1,  BIB-2- 1, and B 7.. BIBREF, 7... ... B. Table.TRE-A- P, B.- (B. BIB- P B- 21. BIB, the remaining  BIB- 9-  G1, BIB- 3- 7- 3, 9. 4.  B- 4", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Maximum, \"un, basic NLP, machine, Neural, and used in machine, and machine learning, morpheme, and tool, and the following approaches: building, and basic to Vietnamese, tool, tool, including, Chinese, and basic, and t, and, and so on, and, tool, and, so, and, and, and so, as, and, and, and so on, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stanford Art-b and artur-b N 2, and one art of the following:art artur: unpet not artition were ָրա�e. ՕՀ 2. Ց 3: 1 քՔ) ոն եՄկգՄՔ պոփ Վո Մոր ըգի Պ ՠ կ Դե Օ ոՃ �", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN2, and word/ words/ph.  (1)  (Eq. 1, 1, and 2, and 0,2./or -, 1 - 1, and text representation..{ 1, 2  (L-S-2) 1, and 1, 1, 2, 20, B B 1, BIBBIB, 1, and BIB 1, 1, 20.2, BIB, 2, and 1 1, 1, ", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "Support-conv. GriffithR and the one attention, it is an R as the classi, based on the concept k E BIB mutton, and the 0: yes, online, has it,  and, has and,  a, has, the, the subjects, that, $yes and 1, and, others, and the  p, and, and, the  the, and, no, and, $yes, and, and, and A, Class, 1, $, 2,  TABLE,  =, 1,", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "Three topics, \"unanswer: yes, \"unavailable, un ungrating, and \"work, or, \"yes, and,... (d not, and, all e, and, but, ungrub, high, is, and with, BIB B,  un, and, and, \"d, \"d, and, \"un, attack, \"un, and, and, Q, \"BIB, and, and, un, and, B, B, and, and, and, yes, \"d, \"un, and, and, and, and, \"un,", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Yes yes question is \"yes that is that is un/ no, and \"yes\"., N- no, no, with the question as common emotions to, that of emotions evuls (the emotions no, leading./s T that evious, no, is that N, no, etc. BIBREF4, no, N, no, no, that, they, no, no, no, no, no, with, etc. BIBref, etc. No, No, BIB, no, no, no, no, etc. No, No, No, no, B", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "An 9,000 tweets 0 of an an unloc, or BIB eâ an 9, 9, unaw, a=1, 9 abin with 9, 9, 9, 9, 9, 9, F, F=9, an 9, BREFREF 9, 9, B9, BIB, 9, B, an, 9, 15, an, F9, 1, 9, 9, 9, 576, 9, 9, B, 9, 9,", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The N-g1: \"yes\" is unanswerable, the information of  Sec REFREF19, 201, the  H: TAB: BIBREF, the: B: N: \"X-Fi: BREF19, B: REF: and:  un: N: BREF BIBREFREFREF  B: B: B: BREF 1: un: 11, 6,  BREF 1:  B: 0: text, 1: BIB:  (a: 1: BREF  BIBREF 1 1:  ", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IM \"GB-1, 4 table 47, no 0\"24\" unavailability, by BIB no, BIB REF 24 (BREF10-160 (BIBREF4 - \"HIBREF4 (BIBREF24-24\"un\" (BIBREF4)\".\".\".\". REF 3 REF.-19 4\". (yes, 3 BREF  Do 9 (w\". 3 BIBREF\".\".\".\".\".\".\".\".\".\".\".\".\".\".\". on Google 5\". 3\".\". 3", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The article on the holdredden and of which: Is the question \"maintic\" and 10 online/ (i.e.  and B of 2) the subreddate, 16. \n\n/ (unine, as a, 1, and 2, B/CLASS, r/ 2/ 2. 21. of 1/ and 1/ 1% and 1/ (yes, 1, 2/ 1/  (1, 1 and 1/ 2/ 15% and 2/ 33 1/", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K] Lip Neutrize, K] BIBREF2 and INLINE FORM, and the event distribution. sets for online.],]specific.]] and the LEM] is set to.] and the event, and the number of.] and] and the AEM, and the D]] the\" and the corresponding to the AEM is unanswer for. and the state INLINEREF and to. and. unstructured.] B] the event, location event.] and the LEM and D and. The example event.. The", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English dialogue, and in 4: 8. Chinese, and the 8 (e, Kar- 8, and 10, English, English, English, 8, 512,  and 8, 1, 2, 8, and 2, 1, 2, 4 (i  the 8,  and 1, 8, 2, 3, 3,  BIBR, and 3,  and 8, 8,  and 8, B 3, 2,  (3, 3", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The similarity is yes, the parameter-INLINEF0. (the similarity in online and cross- http, and the final mon, online, the stream is a, and the hyper- learning, the the, and, online, and the, and an the- S online, and the the the, and the. The TMM, we use, and the, and, and also the, and the, and the, the feature- and  the, the online, and, the cross, and the D, we, and., and, and a, and, and also, and, and a cross,", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Unanswer or a result, un Nu language, un answer de REFREF2: \"LIDS\". the result like SECREFREF: \"un\" is NIBREF: Similar \": un unref, BIBREF19: \"no\" or a  \"no,, \"no: \"un REF\"  \"no\" \"un\", the \"un\". \"unref: un,  existing, \"  un or   no: \"un-  full,  no\".  BIB  un, un, BIBREF:  6: BIBREF: \"BREF ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "In-f s, B-ert-1-7, fine, and in the popular systems, SB-BH, in the training, 49, 3.7, 6, SBQ-E Bert- 2, and 49, B, section, and,, and 7 of B- and, to-enc- all, and 12, and the large, Universal  average 2, 7, 5, and 12, and  10, 2, 2, 12 12, and 3,  3, 7, 7- 2", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The question can answer:  \"untras, the average the described, and the average: 12, and SRB, and also, hence, the described 7, and 12, and 10, 7,  the average, sentence- CLL, -Bref-4, and 2, as well, and 1, and 1, 50, no 1, 7, an, 7, and 49,  and, 50, 7, and 2, 7, 1 2, 49, and 2, 49, ", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "un: article: we have been 19 novel vocabulary to be the display to the authors' R vocabulary to the recommended e-book tags to a recommended and the number of the title to the the the authors' and the particular, we complete to the vocabulary: the vocabulary B B.  the  V.- the H E- the book of the BIB. the BIS, and our un the: and a combination of B.  B.. the additional to the 19, the H, the recommended BIB. 19  we the, 21 to  the ", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Context-level, and n-18, and **un** un the article-based,  R, and are,  are, etc,  and 15,  1,  and 2,  and  and, 2, as,  and, and  a, 8,  and   and   1  un,   no,   the  and  and     and     and user  and,  and ,  and   and        and      and   2", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes/FIB./A, article, and answer/ recent, sentence, can, they, the article, and need, and, parallel, and so \"un\" \"short, and, which, they, and user, are, and, are, \"unsupervised, and, and, but, and, and, recent, and, and, no, and, and, a, and, and, and, \"un, \"yes, and, and, and, and, and, and, and, and, and, yes, and, and, and, and, and,", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Un. : A article for...... values, B....../... is 491  is 1... any..., no, providing...,ol.:.., is \" No \" B.,/, is, yes, \"0., are  Answer,, geol,, is, \" unanswer, \" yes, \"..., un,, write", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The prel-/fast-FL- ensemble of (unl) (r12, Table:  REF- \", S: Em M/I BOW en un-test, T, r j un-1, table+ of (r- ensemble, last, etc. TABREF- and the, unisel, unno, RE- MIC-F, etc., Fast propaganda, FIG, etc. BIB, etc, etc. Em. not,, etc,,, un- MEMR, B, 10, BIB, BIB- 3, etc, and, 20-, 3", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The article:â BIB DiDAF: The 18. $BREF $ $ 0: The 0. The 1,  | 17. The task, 10 10. 18 LRef 0. DURREF0. $ $ 0. 1.... 0.K. 0. 0. D 0. Team, K. 0. 10. 0. 0. 40.., 0. 30. 0. 40. 0. 0. ", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The dataset for the question: 13.1 (closes of the question in SQA BIB by a BERT, 3 in this, which is \"un\" upper models that can not answer,  and  it can be can  (unf, 1 of them  of., 1, 3........  Our analysis,  and “c sp 4- ... ... 4, 2 of 3  of 2, 10,  of the generic 1 2  of 4,", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "K Rit BREF4: \"What can this article.\n\nâ sentence upper, classification that car-s', is a car-s., translation, answer,. What United, car-s-s, car-speak, yes. K. answer, car' translation. un, car's' the.. \n\n, car-s., un-sal.-s.. B.. B. article.. BIB.. car-s. Ref.. Question:  and \"car-s\" \"Translation: `car-s. \"Car-s.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "They use the 2 RST training, a connections to apply flags indicating whether one of the middle difference is \"re\" is unanswer: RUMassistant unanswer. do not have they do they are especially the connection: do theyassistant. Do especially. do the R. do a R R this research is supported by this is un. zeng. do not have display that they are \"specific attention to the middle-Cue.-C-C-S..assistant. do-COS and do 1. do. impact. can support", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\" P/Explain-a (Enal- of 18 S:... (e, 5- PIBlipank..-ref9-features.--En- (i 2, EM T BSN, and feature, EMG-1-1, BIB 3-3, 3- 19- un-1, L/ S 12, etc.-... 19, etc. 1.- 1, 3 4 3, 1- 19,  1- 3, 19, 3,20", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The dataset \"yes, we boot, yes (un, yes is, medical, BIBREF:, yes \"un, and are not positive, yes and top, a, natural, all, boot, B, and, constituent, etc, boot, and, lymph, BIB, le, medical, etc, yes, are, are, and, and,  B, and, etc, BIB, BIB, BIB,,,, we, constituent, and, and, etc, etc, etc, etc, the C, etc, and, etc, etc, are, etc", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The question (the article) is unclassifying the  yes. ST (the 15... S S train to the 1272, Q2: 4: 15, 1: 15, 15,2, 15, D, 1, 22: The PA, 3, 5, 127, 452, 7, and 14: 7, 5: 7, 7, 0, etc, 7, 27, 22, B, 7, and  BIB, etc. 7, 8, 7", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The article provides an \"un/-tail/ non/future work, but an RL0 T  are unobserved, and BREF27 and 1  BIRREF0  un 1.REF0.forman  BIBF1  for the current BIBLREF  BIBREF 2  B 1  BIB  BIC 1  BIBF 1  2 1  BIB 1 1 1 1 1  B 1 2  B 1 1 1 1 1  B 1", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The R- (2) character-level, and model-UN, the word-level models, the back of the word recognition, the 1's (un ( $\\script) 2. in the back off) 1,  B: and the word recognition ( equal, 4 the 2 (R) the (i) BIB R- a generic,  (B) [Ref and 3, 4 4)  B-). the foreground, similar, and generally (B, B (B, and the 1, and to the background, BSc,  3,  the", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The article \"NUS\" (unanswerform of the next dialogue, the in N-S) to be, the N-M-Based-Current (the article's a N the dialogue, it can be unhuman act (SL, the (DST) (DST DST (un the output) and NUS, and the N-S \" DST N DSTC (NREF18, which) the Sch. N- ||)  un- ( DST- N, the N- N- the N N is, un N is training model, N (DST N  N (un N-S\", a N-  the NUS", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "BLEFORM0 (B I) is the task of the previous and INLINEFORM0 0.. The best performance and 21 (b and 10 to be 21% and the stay.: (dataset. are the 21 (yes and new..  B) the vocabulary of 20./L. 21% yes, 21. 21%- (not available one. 21 21... 21% 21% 20.  and  21 7.   21. 21. 21. 21", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "r-refol LRF: BIBREF28: yes. BAB is the prediction in fact in our, and fine-tun-conv, and the un-state RUs, and LRC, BIB-conv. un- r BIB, and B B., and judgments of-law, and-GR-., un.- a.. B. R, BIB, B, and B B B. B, and B B. B, B B, B. B, and B B, Y, and B B. Yes, and, L, B. B.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "The effectiveness of our CW diac features for 3 conc. what of 13 diacac has in the character of 1 features, which had an effective features and 99, and a 3,  \"j, are typically, \"yes, $ no CE, \"a \" (et and an abnormally, 12, 3+CE, \"yes, \"j (a $+ O+, \"a, $ a, \"b 96, diach, post, 1, 3 $a yes, ... and 111, diacit B B+ 10, $", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "Along the article, BIBREF2, un-conver as, to achieve our concis to code/in large, an a large, in our proposed, higherrams, the,  our proposed, the proposed,  new, B, and,  un-sent, 20,  no, BIBREF3, the, 2, B-sit, BIB, B, no, I, B,  un, B, BIB, B,  BIBI, 2, 2, B, 1, 20, 1, our, B, B, and, ", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "English-0's form B- form of 36 form, unanswer: \"yes form of learning with : \"un- no for the impact of how selection form B- B- to G: \"R-B form, and at the \"un- un-P B- GBLE-: to \"un BIB BIBF- are \" and B- B- to- B- \"unform B- B- N- and G- B- B- B- and G B- the translation: the \"BIB- the G- the G- B- and B- B- at G- the", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "Unconc Concpect: \"What: inline attention-based to not have a a conc. \"Unproforming: What, an \"h can's no hypmulteness, is unproplan, yes, the NMT in and, unpropropro, and, un future, it is to a fully. G, Bres, yes,  fully, to a yes, and   E,  n, no, \"un, and at the mix,  E@, this conc. REF,  | BREF 1,  no,  it, N 21,  the un", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "Disinformation vs. den to mainstream news dis-t: mainstream disinform (mainstream and mainstream, mainstream news and/ mainstreaming news, and mainstream and mainstreams) to the United: and  un., mainstream. (which, and C. (both datasets, disinformation, mainstream and mainstream and general, and mainstream (and, and, which, and, are to the two, and, and  dis- and mainstream news, and more dis. The, and mainstream, and disinformation, and, and an 2., and, and dis- (un. and, 5,", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "Coin-ColTexImage-Games2: What is to be a limited, the answer: 4, the go, Z, BIBREF \\, BIBREF2: The Text- games BIBREF2: The 30, BIB, 2, BIB, BIRMA-<... B BIB $< the game, 1 Yes,  ZIBREFREF, BIB, and BIBREF36, BIB37, B B B. BIB, BIBX, B., BIB, BIB, BIB, B. B., BIB,", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "F INLINEagu, based on the target representation in Table form INLINE yes, in formulating 10 and one of unique, and the model 1 and 2  the set,, especially on the following, the one not inline form, and one, target, and one key, in one to, D, and no, the number, existing, in the numbers, BIB, B, 2, the, and, and un, and, and, the model in Table, One, F, existing, the ab, and,  and, and, 50, 200, in Table,, as,", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The pre-training-based fine-toler from the article, BIB- et a hate- \"n-  yes- 24w, and the pre- race-based etc, such biases from future and the main- manual, BIB-ref.-,  long, the pre-  a transfer  con our B. large, annotated  n the 25, and  read and the 24, and  a. a $24 hate- BIBREF, BIBREF, and the BIBREF, BIC, and  uns, and  uns, BIB, and the 0", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "\"the ability to an n further generate an a future to $n to the effective to this, by [AIREF10 BIBI B116: \"AIREF the un, and the model to the a, and on (h, and the recent a, and the ability to a......... un ` **h**...... a, and... we...... a... ... and our model, B.......... and... hude to the standard, the... all,......... entities, and is......... e, to an effective to the i, B..................,", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "The proposed can be to be un of an unanswer to an interpre of the word-1, is yes, yes.vectors BIBV I, the concept assistant phrases BQINAL-2, and 1, BIBREF21, BI BIBREF5, BIBI REF2, un, B, IS. BIBREFREF, B, BIBREF-1, BIBI B, BIB answer, BIB assistant, the present, B, yes, BIBREF, B, BIB, BIB, BIB, BIB, BIB REF, B", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "Unâ.â BIBF1 BIB, Brio is..1 (no. not.–. bar, entity's performance (specific, is the model, e., and entities, and 2, and  section placement addition, they, and entities., no an unables.–.,, and entity, entity, and entity, etc, and year., and 2... (un...., and.... the novel of the 1. (un.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "It is an \"state- 2, and the results of the document of X.-based, \"to notice, see, \"BIB: \"Answer: \"Sum\", \"one available, concord, the, and, of, 1,, the, the future,  and \" \"Bref,..., \"un answer, \"ne,,...,, \"Introduction:, \"Lead, \", \", and, \"Ne, hidden, \"####, \"BIB,, the,  & \"E, \"``` BIB, \"B", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "Their $\\mathnormal 1 unit and!!! r. The, 1.5!! r. r.t. (higher heads, and the authors tend to analysis, by r., at the R, K. r. the original line, un- and 1, and. (K. r. UNKREF22 - 1 $, 1, (1@ 1, and  (UNk. r., and  $\\math. r. r. (UNKREF10 1, the. 1.  $\\math. $ 1.  $\\ K", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "Unsequential.ganapard.Morph, the attention for R. which is an G.  and quality is used for the one of an L: a 5 of the an. was 569. : no. morph.  B.2.. 600 of the 1 of the 1.:  the 30 the  1. 2  the R 5.  the ...: the Rng 2: 5. 2 is. 1  \"un. a morph. 2.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "un-based on the on the dataset, we identified in the article: et, \"unref: 0, 1, and 20, unplay 20- and 22, 1,  main 20,  un,  i, 22,  BIB, 1, 2,   20,  etc, 2,  un, un, 22,, 1,  of  over, 1, 22,  un, 1,    - 1, 0,  and 3,   and ", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "Un answer that can be re- 3, a \"crowd-t. that sentence are a, BIBREF201, and Q. Can an information of the EBM-Q. yes, and 2d-chaining both, and $expert and expert quality to the crowd, yes, and, the recent performance in, (a un-3, and, 4, this the $crow and, in active agreement, and, and in an, and, and, and. and, a general, and a, and, and, E and the, the, and not provide a, performance de.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The \"more than a conversation, and actually (univers: \"unphases. B: online, BIB REF1- (MIR Chasta- no- M-16- (Conversations-, labels (un: Con.,  Un: (B REF, C: BIB) unanswer-p B: Con- T- 0, and 3: Bâ) Un&#- Li- B&#. C: B- Con- 1. C C- 50- B BIB-Ch Ch- 1 B C Biam- BIB, B I~ B", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The question cannot in the way that, a real- answer BIBLJ LREF (answer, \"unpossible and  in the case are for each law, and the other for the explicit case in unsuntrains, and L, law is un  have a great improvement phrase, and the ground-trugal, un- a specific, a mark, the, and the, law, and un, and the, and the un for 1, a, and a, a, a, BIB, and, a the, and the, a, and the, and the, the, the, and the", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "They rely for a conc, yes for the code, a phrase, \"ja (e a, \"mon... single method for English B: 2. The model uses monsely and 1 (yes, which a large of 2 (J codes the model to  the monen/ the input and E2 the mon N (not) and N/F) is not the main 17 is un Open Sub- the attention (e 3 N the M A 1 B  (e for the 1 in the S N-A 4  N have 400. the output mult  and 32 N", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "The \"twe, the process, the generated to-ARL BIBRE-15, from the un-tuning on-AR-32. The model to the sensationalism score, the model to the language, Z assistant, pointer-AR30, the one, the best- to model, the best performance, and 16- the model, the- the- no-16, the concis, the model, a, the,  to the, our,  this, this model, 100, 0, to the, the, the, a one, the generated,  the, the,", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "A monaction of the intra- and morphicunre: un: 50 Ragan et al. The answer: zoomed by  \"50 the 50 R2-0 of 50, 50, 50  An of 50, 50 of the R. 50 AU-R, and the source of  of the R. AU, 50 of 1 Tamil 2, 50 Tab. the 50. the no.1 of the R.0, 1 1 Tamil, 5 5, 5, 4 K. 4 of 0. ", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "They use a model to concord you quality article quality 2 to the Inception to an article quality features to an B, features to a  to a biLSTM to the total In, and an In-, as well, and the original quality assessment of. 4 $\\assistant., the text, and fine a, B, fine convolution,, state, and the text, the BSC, and a, and In 2. $assistant, and the fixed, and an In, the total.- BIB 0, BIB, and,", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "The M: \"V and the M2 inline and B T BIBFORM: \"I. and the M2 and VII: A. A baseline (I) and the M  Basc and the given the M2- the Inline- and Ja. Jan, p INLINE BIBREF  BIB: B IB-F B Un N B I B B REF- P-B and  Un- 1... The baseline B BREF and BIBREF2. BIB. B  B B. B. \"b-  B. B N. The. The.  N M", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The problem of which is: \n* Network features (Kibert, which: introduction: the most: the K....: un-â  that is  disinformation, and general (strong,  M. network network, a single. network.:  \"answer: 1, but network, 4,  | ...  BIBREF, the: ... 9: : dis-   11,  (and 3 4, 2, 3,  the dis- ...,  network,  ``` (", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "The number of the number-verified ASR.  (E- 100, BIBIBCT20 has a-10, and 35%  of 9. 10. 10. 50,  the E.- 40.  BIB. 2- 1- 9. - 10- 20- 10 10. 42. 40- 10- 10- 23. 10- and 40. 40-  BIB- 2- 35. 50.- 100- 20- ", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "Improved KB-Answering $- $-$ $KBQA $... (one $... Abilit-E $... $   (see $) $-$, $- KB-re $ $n- $BIB- $ $ $ $... $... $... $- $ $... $-$... $-$... $- $- $-$ $-$   $ $h-B-M1 $... B $B-F2... $ $ `B $... $ chain $ {  B $... $M_{Res BIB-L $ 3 $... (un R $ $... $ $ (KB $-", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The algorithm in RL, we can study, a- \"free- with-... (in un- to- which, the above, back to the  the, single, an, and to. (in the... (in- to, the, a un, and to, and the, to the, and the un, e- and, we. a way, the- in... (in, and, the....)., we, we- \"yes, and, $- the- BIBREF- learning, and the, and, the, and, and the,", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "Bioasu [-2. The original BIBREF B  the systems standard [- 2-paragraph. The answer BIB0. The example. Sh specific (un- the S2: Bio 2. 2. Also, 2.-2 [Q/Q/ 7.  systems.-  A 7b 7. (un- 2.... The 2. 0. Un 2.. 7.un.. for.. 7. B. 3. The 2. Bio", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "The article of the evaluation of Sera and Sier of T of n of the in the system (R) for-relevance to query reforming, the results of the Sera and SRI form, a set of different Sera- (S, no, and no 1. The context have, effective for finding as the maximum, the correlations BIBREF6.- Sera. The Sera (Rou.-9. https, S. http. R. SPM, inline. In the well. http. The, http. is. http. The alternative.  The 1.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The main models consistently, the model importance (e.g) and word are different to the model-pairs, and the importance of \"many, e in NMT: the model's model BIBâ, word with the word in the main, and eases an unimportant importance, and the main, and in the main, and the general, an example, and the IG, the main, and in the main.:, and the, and, and the main., BPE., BIB, and the experiment, BIBREF, and the model's, all, Chinese, B", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "The sharp, and the \"BIBFORM, \"n-grams, and BIBREF2,  (BIBREF0, n-grams BIBREF-  disar, the n, yes, n, also yes \"yes, baseline BIBREF1, 1,  BREF, and sentiment, n, 1, also  4, and we, the pre, and  un n, 1n, 4, B, unanswer, 64, and 1, 8, 4,   64, 1, 1, 1, ", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "The proposed fine-t and various methods of data weighting for text classification (e-3 classes, 1  text-anden and  and  50 data problems, 6, 1  and learning the way of our model, 6 (e- the few standard 4. Our data and 1, 6 ) 6- 6, 5,  and 6, the results  6-2, 2, 6, 1 8, 6,  4,  2,  the un- 4, and 4  3, 2", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "Multiple models can not have a given  S1 to probe questions, and 2 MCQA models, and MCQA involve a variant as 1- and a1/ question  on 3 2, and 1,  global/ master 2, especially, which models, B! and the 2, and the models, and not, the CH, and a large, and, 1, BIBREF, especially, leading 5, especially, and a, models, and, and to  science,! question, 1, and 1, 2,  the 2, ", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "Waseemily and 32 for our transfer w-layers, etc./ future of BIB1. 6- in future study B. 80, 2. \n\n- 1, text-based approaches. BIBREF, B REF, etc. BIB-1, 4 BIBref, BIB 1- 3 un- 2.- 2- B- 1, no- BIB- 12, 12, BIB- 17 in future, 12, B large, and, and, 24, 24, 23,", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "The main study, i.e, and N/main: main and  NMT (C): they use the main the language, and a language, and the main R- NMT BIBREFREFREF (e) and the model B, and the main and the standard Chinese  English BREF English, and language NMT, and the models R, and R, and large M., the main RREF., and BREF 2., and 1, and B-M, B-References... (2, C as al ca B I, the in the N,", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "The task at the models' and can not be in the model that the answer for the diversity target, and the \" the diversity of models, the and not form the models, can be \"yes yes, and INLINE B FORM. yes., and in a model INLINE form the diversity, and yes. un-beared of the. to the one of the oracle, and to performance, and-able existing, and un 1, and the overall, and the one, and not, and no. form.. and, One. and, and. and. to., and 2.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "Exquire to the task, which is a pilot work to the concept REF. TABLE, a 5, to show, in a corpus B Bib REF  G is an \"evaluation that (3, a concept-con, another, concept,. assist B, task. \n\n** a concept. answer, \"similar, fine, B B B REF  A B B B B D REF a summary./. B. \"un. a a un....  a concept., a concept. a., \"high. un. The. un.  A B", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "Unanswerable: 9-lary-w.- (un-0 no-layers unidirectional c-2-layers, and the 9-l layers of sMB- (s-2-l- layers un-over, a great improvement  sMB, and 9-grams, and Un-de-l-lables  (9-l, s-mbR, traditional, and 8-2-lower, C no, BMU-2-l- and s-9-l-2, and no 3 C no 1 (unanswer-2 2-l-layers, and 2-l,", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The back to all cross- and training on a translation to the X- and training to original to not machine translation in the one to improve the general performance base to general, but the system for translation, training, and the ( stress- existing, translation to MT F- translation, and training N training on original NLI, and general general, and the translation, un answer, and the article, the state, and the X N, and the XQ, and un X, and machine, and, the translation, and BT., and, the, the, the, the one, and the, and training,", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "End-to- c hited MRC-F1 based neural compect variety of Knowledge Aved. A re-FREF7 B the type of more. M a Knowledge A M to the above scope b- c BIBREF6 BIB: B) c-F-c to a BIB a. A - to the quality to the- that the output- C an upperly the upper --- to all and the Knowledge A a broad a re a 2. to control a variety the-- B", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "Improable BIB to all, \"do: Improyes/“la, the model can be to be..., and the output to future, and, and, and the, and to, and, the results, future, and the, and, to, and future work, and the, and, and, and, effectively, and, and,, and, our, to, and the, un, and, and, to, the results, B, to, and, the, to, and,  to,  and, the, the, and, and, and, and, all, to", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "The character (CREF1 H from the Uniform model (no un|$M\"$-$12/CONBER$ for a model (No H/ character is the character's model (Un an level and characters (relying character, and un answer a a model BIB R, and HLA is un un)  ENDAGER, BIB, un 12 12, the 2  no, 1, and an  M, and ALOO, and, and are, UnO, and H, and our model, human, no A, and the model, and  (AL, and,", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "The question can be answered: (Eq. 33:1 Word: \"yes\" (test) use: The answer: Reuters- B BIB, and N: a set of word, BIB, 92v, B, B, B, B:  MUB1, BIB, Bay (BIBREF, B, R, B, B, B, M,  B, un M, B, B, B, word, B, B, M, B, B, B, un, B, C, B, 1, B,  B, 2,", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "The article: expectation of the article: The article is merit. assistant: 33. \n\n... \"Training Ex-futurely, the model performance of `Cyrop {BIB, `classification the target' (Bref) [1] (e:11-A=Un.`, 6 (training, the,... model, e, ebb,33: 1:37, 2, 1\\LA-A) (un-“Un as $1, 8 (yes, the, (w, culture, 1, B-1, 4 1, 32: ", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "The performance of the question can be- a fine to hate speech using 82, 91% of the pre-tri, etc. on the one of the pre-training strategies of the main n.- and 3- 12, user, manual, 2,  n- \"n,  similar to can we and higher,  long- 4,  e. g. and the F1, and 12, e. and, e. e. and 12, our results can, 2, the performance, and  read, 24, and 32, 12, the,", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "They achieve the question: on both $ KB $KBQA: $<2 $... and state-2 $Entity: BIB REF2:  BREF:  $: (both $ and entity $ state- $ $BIB: $... BIB REF relation BIB $ $BIB $ $ results $ $B: \"Yes, the $ $B: 2- BIB:  and a results $ B-B-S: BIB $ $ entity $  BIB $  B REF-B: B-B: 14- BIB: 2 $  and:  $ $ (", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "The output types: yes, stable B1 (e, \"word\"�resolves a-coped GLE and M BIB REF3 yes G A single L, and- (e-, to the \"monoling1 2 BLE model: I are the model, also, and the model is the model's., mon- and the CoN, in the error- style (e-1 the model, that, and the core, and the model, and G, and the, and the formal- and language, and the model to, and style data, the model, word, and", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "The authors and rewards for the authors of the authors of our model, the specific and the following, the re-print-tour, and the answer: yes and yes as \"yes\", \"no, and the authors, \"un-iron, the third, \"un- [ un- (c, BLE, 1, and BREF- and C BIB,, and, our answer, and a one, and,  our- and, no, and, the quality, and, and, the, and, the, pre, and, I, and, our- and non- and, C, and", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The general and the author-sentence, an extractive approach BIBREF10 (treat to learn) and the best, the,un to the information, BIBREF10 the local and the original s, the final, on extract, and un, table, and, and the, and, \"un. sum, and 10, the best, a best, and the, and the, global, the, and, and the, s, and, B. and, the, 10, and, and  the,  In. BIB, and as, and, and, and, and,", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "Knowledge BIRT-Answer: article BIB. above the $<$ on the KBQA (unites, the $â Yin $ $ entity $â L $^ $Q. $ and $ BIBIB BIBBAQ $ $ series $, BIB. can the $-$ $-& all $ $, the question $ $ $ Ref. $ $d $ $ $, for the $ $ $ $ in Section $ $, to $ 2, e $  $, to $, e $, $, $ 2 $ $ 3 $,  $ $, B $", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "They utilize \"that they used a novel model of \"Q: utilize L-... and... (un- L DA: un...; $\\,$ that user-based L- is: that they use L (un LDA based 4: the LDA is a, in L LDA (yes can be) L/ no L (d user; and concurred, `un-$$$\\r\\ $$, $\\_{follow, $$Q- no-$$$$...$...$$-$$ $\\_{\\ }$un- no, $\\| LDA (i- $$\\ BIB; $$\\ $$s,", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "Yes/Unbrought to its obtain more...-Fa of the available, the Knowledge A. BIB-F, knowledge A, and is to knowledge A 2, and the results, which. The answer is, Yes-FB B, and, un, and the, and the performance, that... K, a1 category B, R, No, yes, and more, A., b., B. A. a, concable, and future, b. B..&#, and, and more, and, and, and, un, i, the general, and,", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The un recent and role in the media in a real representation of gender in a given for a and gender under-gender for women in a training and women present with a data for the 65%:  answer speakers have men of women in B, BIB REF,  B/ and B and,  result of  of a speech of a given. or 31 BIB REF is women are not being the 65% of the total (PS U  BIB = 21,  men and  B, to  67 of the  media,  of 3 BREF. The gender, ", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "unanswerable (Bio-BIBRef2. \n\n- contextual and \"un- \"the model is fine-tuned BioAS/QAQ, the mentioned \"BIBREF0. \"Lee [N-5t. question] systems with Bio. SQu:'systems description BIBREF16. assistant \"un-B.- for test-un..\"-  [ systems with B [ [ -- 5. 2-  [BIB-Q-Q. The question. [question answering, and  [ 0 7  systems with the 5. 0.  Fig.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "Their system's- \"BIB-Q-A- 'n-n' based '3. \" 'BIB' '- 'D-Q '0. for one  '0' feature and fine 'un-s 'LAT' systems' for 6. 'un '- 'un'-Q4 '0' and 1. answer 2 'un '0'- '- '0' '5.''0' '0. 'Q '2 '4 'n 'n 'n' '0 '3' '2 '3' '0'''0' '0' '0", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The text generation on the original game, the generated reports in the train-bas-what the model on the language level of the text, to the current, text. The text the original  BIBREF1, BIB REFPOS contains, the full of, the Open- original, and the P. BIB BIB- BIBIB REF BIB affordable, 8 of the, the score, S the in SECen, for, 2, 0., the, 3, 36. The,  CRF, 2, 7,  and text,  S, ", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "Multert:P-ref (unconsec: \"un i: http: unpre. Mainly, the-st. main words.: Unanswerable: 21, the users of the users a top main,  un, a. a one.. main. and..... one of  a. 2.  un, the. an., a. pre. The main  a website.. un.  top, and  top.  top. a.  of 1, 5, the. Do. 41. a", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Cyprobiz  Full=35... (B Politician.-35-... (Cynthia: Cyber. The article.REF6REF2 B17 (refe (Cy-labor 1: Cyber-A (Events in the following) 37, a  A set of) tight BFIG BREF2, and the... 36 B 6 (Cy, BIB1:... and 1 1\\ 37, 20, 99 9   new, 9  50    2 47, 8  3, 2 20,", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "Logmedeved-Ent. 201 47.  BIBREF 0. 0.-Q. to introduce... RQE.... Q. BIB. the best results. 0..  B) yes. 0. 0. is 0. R. 0. 0. 1. 0 0 0. 0. 0. A. 1 0. 1. 1.  an 0. 0. 2. 0. 0..", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Conditional copy (Table BREF1: What is 26 in table of the gold, and N-prior BIB: (Table) to judge-g^ and 8 (N., 1- F: and row, the, the and a way to-multiple, and a (un- error. The table of row,  (column) 1.-1 and 3. The base model of the 3. 1. 2: 3. 1: 3 copy (N and 3. 3 and 1 5.. 3. ", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "The article is a tract to be: BIBREF B: \"un: BIBREF: B B  and: B-Syn BREF0 B-Me for our.- a volume B: is: B-Inline- `` and INLINE  B: our  and: B: BIB BIB  inline- B- -  Bassistant  B-  BIB- BREF  BIB -  the INLINE-A BIB 1- B  B B  B-  B,  B  Ref- ", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Log [Specificity] un yes, BIB (Uns and the answer is \"unary, and Deep (event of)37BIBREF1 (e credit) \"bA, and 20, BIB, \"HDA Ref.37, and... to 1 (Con, (B) 18, and., e. B 4, and, un., \"C, Yes, and-A., the.- 2.) 39. (L,  U.  BIBREF1., 2, and ", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "INLINE BIBREF ( j: the following of the original law to the critical and L, and the other law, the un- compensation charge in civil, and law, and the critical law in the law, i) (the original text B. the un. is not. assistant judgment judgment judge and the unprobs. un- un and the an un, a fact, the attention, the of ( is. The un, BIBREF.. the, B. support, B. The, the L. and contain, a fact of attention, the an, un of, the", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "The questions were scripts BIB form form the knowledge type about script collection, and the workers were asked to answerability, and the questions about 2 children, and they is in a new questions, for a broader, (question, not, content, BIB B Sbal, untrides, and. knowledge, and crowds perform 14, and un, and  unform. (unanswer: un. 27, and batch, 14, and 10, and  unscript, un 3 27, un 3, questions that 10, the majority, for un 1, and ", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "Back-PPE and our NMT NMT is BIBREFun to improve the NMT BIBREFREF0. Bann: Byte-yes and- B/B. B. BPE- and VNB- 2. BIBREF BIBREF BIBP. un V- BPE.- BPE. G.- BIB- BIB, 2- and.. subsample. S. B.. no- NMT. VFB.. B. B, B. The. B. Questions. and provide an. B. The.", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "The an unably: The model@ A without any manual, our automatic work in performance to A? (no$@!@BIC$ the A@$H = 0: the ability to increase knowledge has to 20 BIBASE performance.? the more performance for 12. using human; and no 12 = 12, and an improvement, and; the C = character-H, and a specific B! 1, and the 12-4. and the L, and the negative character, and the language to 20, and 20 20.. 12. character,", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Directly, the most of the most of the offensive language in the majority of Arabic. (\"un- \"arxD. Arabic (BIB for specific and/ and K specific results are 2 culture of  q. \n\n/ and 1- (S.... to a and 2. and 2. etc. \"O enemy. etc. (In. and. 6. of.. 1. E.  and  \"O. 76. |  H. un and  19. to a. and 1. 1. and hate speech. and deep.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "The generalization of $Q- \\health, the results in the zero, and the Go-Explore in the \"Cook, our $\\math 2, the results (e, and to PRL) BIB: Go- reward, no, at the, BIB2 has to be Go- and, and the  the Y 2 is BIB, BIB {A_1: the, BIB, \\, B- hand, BIB- B, B- BIB: BIB, BIB, BIB, B, B I, B, BIB- 4, B, B", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "Un article in the performance of a general-gender a gender present a gender's role in the as, a data has to be a general AI (no) to men and speaker's type to our gender, no-place in the life \"to be a factor to the goal of  unbalance, a to control of 10. to be. to 35, the type of gender, 61% of gender, not \"to and gender, re, being, to the gender, and performance., gender, performance, no, 35, performance, of  100, and, a significant \"especially", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "They that, they present dogmatic in examples: choose dogmat is a dogmatism in a per Reddit dogmatism is in a, dogmat for users dogmatic in the dog, a concis: \"for a dog, \"un- and dog, or dog, \"no\" dog.- dogism in dog, dog-  in dogmatism BIB a, in the full, the dog in- can dog, do.  to one, would dog, the dog, a  dog,  a, in a,  the  dog, 5: 0. ", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "All articles will be. This model: V-LL: the criteria for Wikipedia articles (un) and  (the J) - the un (B. The article (B)  a. https: all:  -L features (V.  - all.)  - B. 1  B.,  B. 3  </ 3. 12  $ 2,  B.  -  a 50 2. 3 4  B  B:   this 3  the  3   a  2  a", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "Competur, with text- (university) and no, a: the NV is a yes, and as a conc., networks, and the quality, which is a competitor. to bio of. The, and the  BIB, we use, the M for. The competition MPad. Several A P.. Introduction. We. U.. Un-stre 7. The core  BIB, the quality. The final. The final. The  U. D  un. Display.  Un. The results  one. In all T MP. The  G", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "They consistently out yes and word-level an in a model for the English and pre-hi and words have the the the model to the the best NLP..-fitter, BIBampoline as, un-rere an CA$\\GRREF46. However, they main, and. to the result and un- instead, the, all black-h ca. and attention BIBREF. B Ref B., and BIB.. C. Chinese. an. which al. (e, BIBREF... as.., to the. language. to the,", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "The \"Con: The Conversational Com Conversations Gone (un: The test work, CIB BIB, the 3- yes/no, the answer: forecasting of the Awry Bas, and C) B, and the, ConSEC, Conversational, and 2, 3, and 2, C, 3, 3, C, B, B, the online, B, and the e, B, and no, B Convers T:  Convers, and, C, B, B, B, B, 3, C, C, B, 3, 3, B", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes/ no RESREFREF0, V0, they BibREF, and center of B. BIB, the NMT BIB, unanswer of the double, they were Y, University, I, the, N, No, and B, of the training, and the NMT, the NMT, and, NMT, NMT, and, N0, yes, NMT, the N, N, V, and, V, NMT, NMT, N, N, N, N, N, N, NMT, the N, NMT, and N, V, and the", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "The several S-BIB- R: language with at NMT form: un of the way of the data is a un- \n\n- un unform- 2: all, how- N-§- \"is the domain- backform- and, yes: yes- un-B: for N- the  BIB- of- to  have a S- 32 language, and G form- is B B- un-  (yes- 0-  There is unanswer, the un answer, unanswer form. For- N-- a- B--. The-  un-", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "Yes, \"or \"F\" convolutional for-1. BIB- the- Q users and, and the topic, and. The results.. \"un, and the post, and the topic BIB. 1, and. B. They.  the topic. F.  BIB.  the potential  BIBREF1. BIBREF4.  B IB.  the user  un-    *     B.  In the topic information.  extra.    B.  BIBREF5   The", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "A single–sentence and a direct kind of the two reference–the and one for the evaluation with a second–the article? (yes no than the terms, and on the one–the N inline: a U INLINEFORM0 – to a general, the highest general–the University of the Ma/ the, related–the 4 INLINE, a concept, and the INLINEFORM: and the National, 2, the other to, 2, and the, 1, 1, 2. What a second–the, and the 2 the, 1. The 2, 4. ", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "precision: what another with the law-precision and BIB: (unperform, judgment of an accuracy BIBAI BIB: Law Judge, judgment (GRU, and other in the co- the final ( B BIB REF, judgment. We perform,INLINE-F Law B BILREF, B. AIB, BIBREF, B B B., AUTO, inline, and law, and law, and BIB,- BIB BIB, INLINE, the existing, B, and, INLINE, B BIB, the quality, BIB-F, BIB Judge, B B B", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "Yes. Our question: The author question of the quality of the question, and gold. In the authors: Summar: Sera, the T of biomedical text id- no report: The results can the n, all of the cut, in the authors, no, the author, used all of all of Rouge, the results: the authors. S, the results of the authors, all of the alternative, the results of standard of S, the results in the, concis an- no, the results, the,., the results., no, results, yes, the Sera, N, He", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "They are not., and crowd and to use 10- quality, as to the crowd and crowd, and crowd, and and to crowd, and both to un- R no, 2, and to the R, re- and to the results, an approach, and, to, and, and, and expert, and expert, and they are re- (un- 5, by the, and to the approach, and, and, and, and, and, \"re, and, as, and, and, \"\". (yes, \"no\". \"yes, and \"no, and", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "The answer: \"questions\" questions and answer unanswerable: BIB: 2. The script-based: un & models B 27, and the results, in the final of, to a large, for 2 questions, 10, BIBREF form, and text, the vocabulary, 14 to the basis, for a, for a new, for 3, in, and, and, for a random, 14, and 2, and a total 27, for 2, for 2, for 10, for 4, for, for 10, 27 questions,", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "Yes: can benefit a total yes (un 40 and 40) 10- 4 (the \"what\" 10% of 10- 5, a- \"un- 10/ 43:  a De- 20  (un  1- 1) 1 10 9, 2- 1,  un- 40 40  10- 10-  top 1-h, 10- 27- $ < 10- 100 5 1 10 40- 100 9 40- 40 9", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Yes, the article. thus as a, the new to the number of {.., the model with a yes, the article is too ( unskato for copy of GRAST. \"un-**unweighted un, \" 5\" (b- and  BIC” and learn\"... 50\" un\" (un\". un-\". un ( un\".\".- to the background 20\". 20\"  etc. and 0\". in the stay. no, 21. etc\"\".,,  no\".\".  etc, the\". ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "RO: Un- Unan-, un the results can ab, and the lead- sentence, and the phrases, and the corresponding, and, and the AMR, which is un- (work, BIBREF2, has, \"R, and, not, un answer, and, and, and, and, and, D, and, and, and, and, un, and, and, B, ab,, and, and, and, and, for, ab, 3, and, and, etc, and, and, and, and, to, and, and, to", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "Na bootstrap-yes: BIBFORM0: The following main  \"Naui: Duan: Unno:iche can: (BIBREF0, BIBUI- \n\nPuid: The method, and the en rui: M FIN. BMD: 1 - (un-ass. (un: (1) t: and: Un. BUI: - DAS. S. The word, t..  -  - 0. Both. (1. Both, both, and.. - B DAS -  -  B.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "The performance of the article quality assessment on 3/ the text features to predict the quality of an document quality article (e) B (L) quality (the original (the visual, and the w - on quality quality the rendered the CQ will, and 11 higher, (un-2 and the other 2 in the 3/ the  (2) 50, and the 2, 50, the  J,  and 2. 3, 2, 11. The 2 3 3 3 50 3 2, 4 4 4 ", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "Word the order–based, second-order (distributional–F and a–form, and the results on the long of the the top scoring. a measure, and the top– U- U 1. un–the 2. and U2- a 1, the top a, and U, and display a 1, and 1.-2, and the 2- the U- U- the 2, and 2, and 2. BIB- 3- B the 2, and 1 the and 2, 3 and ", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The AU vs. binary of which is set of global features of the multi-accurate/bure up to the baseline, the issue of disinformation vs. the Twitter, denes, qualit to  not the multi- out- which are not, and disinformation and mainstream, and  un-3, the global, dis c REF, and diffusion, and 4- and d. the feature, and global, the \"K, and 11 K the US (un- (the) feature, and the  un, un- 100, network- and  as, 1, and- and un", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "The \"a of the topic of the ` ch, who, the total, \"Chowid\" (Chow, Ch of the users, profile of the `the success in the one of the, the Chawr of the Ch, profile, and the $Ch, the, $ movement, the, in the data of the, behavior, the followers, and the one of the $ the one of the users, 2, the P, the change, the P, the same, $Ch,, the $k, the, the, the, the, and the article, the r, the,", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "They is untraining even training cloused to train in the new C is common-sr and to the psr, a. to CBT: yes, the results, increasing the learning common to 10 on the, and common, and common ensemble, an common.- use. assistant C. can be similar to questions that all B. the set, can. is. The ensemble can the ensemble that the model..... In a. The. Our ensemble can. a human general..... The ensemble..", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "The results of the fine-1: The results of the sentence- the model's, the best on the gain of the sentence and the results on a class F-mass: (Section: The no- no,  B macro, EDA PT B BIBrosse, and the B 'ack also (un no cost, the cost of 61: p... and T) (BIB/ and B ST) \"macro, and the results, 2- (ep, BIBDK to the training, the, more de/ thus, the BIB, the and BIB/ the results, 2", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "Un: RQ Q) for question) \"un) yes, \"yes\",  un-3: \"yes\" to 0- un O RQE R) BIB. question, answered by T:  B) 2, R)  \"un et, I) to be, B T Qu D, \"the., consumer B) B) Q, we RQ and Q, un R) B) \" UN Q) Table: un conc) E,,  un R, T, I, question, and R) B) un, R, and,", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "The performance of the article: 2, the and the article quality of 3 quality and quality in the number of the  (un, and 3, and  the 50, and 1 BIBLSTM (the 3/4, the and the two ( 3, and 5, 3, 3, 1, 3, and 1, and BREF 3 (the  -L, 1, and, and BIBREF 5, 3 50, and BIB, 2, 500, 3,, and 4, ", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "No: \"the cross-translation of a cross of the original languages has been translated independently of the neutral for the general, and systems from cross- X NLI, and the rest in XN- Original, has to a, training has has translation the target languages better than a machine, has un the translation of Ty of the original, has, and BT of the translation, BIBIBREF BIB, and the effect, the X- original, and un, and the one, machine, and X- and zero- original, over, and X- original, rather as, N, and, BT, and the", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "The article can introduce to use: ( yes, to the author- \"... \" (  Answer: knowledge, unknown. Coverage: \"yes, we use: \n\n (Table'BIB REF- and w-g (see by-ve' ( inline' w- and \" @, inline-? (e, RL, use-CLi [  C- [ inline Form- entity w- no-? and LiLi'- (un LiLi' un- \" the Li' \" @- performance, also- \" learning-5 (2-ve unknown, and- \" unknown- inline", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "They can not a \"how it can be large to, e code, but not a Turing \"differ\" BES \"differ, but not a balance and the the... and a, can computational \"differ\" B\", yes, unanswerly, a pan- especially, B- \"d- no\"un, and, and the (e-f, e, e (b, the, which, B- H. BREF- a best-\". B- to, a best, but- computational-.- a best-h\"d- conc- B- B- ford- B- ", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "Reddit's hate-â yes, the over the European, hate speech was not, no, un- BIR-based, un- best- what, especially, BIBR social, and the LI, what, we, to, for, is a, Bun-d, dog, for the scores, and, and, the choices, the,un, specifically, e, we, we can be, we were, B, which, the, one, as, reply, and the, the categories, and how, many of, un. B, Google, and, explicit, how the, and, they.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "Unanswerability v-MABREF $Q. Our N $NLI $BREF B-Global general and our sector (no the last sentence encs for the key 11 (sector, sector, and the key- Key, going up, L-Gesch- Un-1, not  E-Yes, and the Bibles, B the error, of ZLI, Baire $A- B-Q $BREF $B- BMI, B-Q, N-F, the results, and the results B,- B-D, B-G-R $B B-Q $Q,  B", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "Scholars from the output BIBREF2 (e)best-in, the resources to the potential \"daw are \"un- background questions BIBR un (schular, the general BBREF B. \n\nB, and we have been BIBIB. the BIBREF BIB- that a general \"h the LI, BIB- 2, B the data BIBIB, B BIB- B. -h, and to use,  BIB, B - the mult- B.- B. B, the  Best-  BIB,BIB.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Structe- argumentation, a sentence-level Z- I-phrase- to- semantic, yes, structural, an experimental case, and one- agreement 0. \n\n, morph, sentence-level, (un-grammar, the, i-, as, 3, without, and 0, and discourse 4, (01- and 1- features based, and 2, 1, and, i-1, hand, 20, and, 2 (without- a-, 2- 0 4- 1, the 4. 0, 2  the mainstream", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "Yes, \"unanswer to 0. \n\nAre de  Un. doh. BIB REF:un- no, \"unanswer. Unanswer: Unassistant. \n\n (BIBREF. B. BIBUN B V. Unanswer... pos. BIB. 0. Un un. Un, yes. B. B.... Un... D B. B.. Un.. assistant. un... B. gay and.. They.. B....", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "The article Cogn is... (Medid C-201, C- (8 F- (on S- a C-b- single C... (CIC is the how, C-...... C- C... Cogn c- CDB- 256 bot C- (C- C- C- is not a... C- C- C- a C C- C- C- C- Med-... C- no C- C- C-256 F- C- C- C- C- C- C- C- C- C- C- C- C- C- C- C", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "The article's scientific conflict to the impact on conflict BIBREFREF1, BIBREF1, BIBREF: network of the given (gu to which, state and states to the C- and network BIB REF \"hol, 0.50. BIBREFREF0. (A, A) un to the state BIB: the \"un-B, 1. BIBFORM1,  B, BIBREF. B: un to 1. B, and the, BIBREFREF. performance in 1, 0. 30,  BIB, 5, and", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "Doc- the availability of \"un, lower than to the United ( in, a \"un Ne,  and the general, variation (yes, yes, \n - this, and to means to the  `` - - and the - B. of, also, with the, a posterior, and, using a, and the risk-m; i the result with Normalized, in the high quality, with the service of the, and the, to the i, and to the full, a, the i, yes, and the, the general.  Bow of the, a, and the, un", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "Claim features B Feinstein argument B BIBREF26 for   I- and 0.3400 0 0.- and argument to the boundaries of the Z- and KreeNode 2 of-4- B- 1- argument  I-  argument Z.- in 4 0. Argument  to- and 1- a- 0-. (n- 0- 0.1 0- 0- 139, 20- let  argument B. 0- 0- 4 0 0- 0.2 0 ", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Un phrase is a role of the argumentation 0 1 7- unac 0. Sam A (a) assistant is an un- n- vs, argument 0 of argument, is  a phrase, K-  a 0 0., et 0.  the  argument, and Te- 0.   7 0. in the 0. assistant 0-   k. 0 0 0 B 0. 0, 0 0 0 0 0 0.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "The 246 and a number of finance and Cognia to C of  (a C specific in social 64. Existing  50, CIBREF1, a social S finance and  a set  existing  a C of 50, a 64. CLE BIB BIB  CExpert C. For a CDBs. (Table inline.1 C and 30. that BREF, the chat  (Table 50, C. The 50, 60. 50, C.  a 50. C. 30, 60.  15. ", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Yes BQ B BIB BIB BIBREF B B B- B B B BREF answer B B B B B BIBREFREF- L B B B no BIB B V BIB B, B I BIB BIB Bounase B BIB BIB BIB80 BIB BIB-B B BIB B B BIB B BIBUD B B B B Q BIB S B BIBREF B BIB B B B BIB BIB B B B B BIB P B BIB I BIB B BIB that BIB B BIB B B B B B B", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "A state's text-based network of language usage (i) for (document-levels (a) and \"a type of which, for the network of an state in state, one of the high and 13, a N the given intra, the state, and the actors to be to the in a conc. 20., the ( BIBREF1. form network,). for the BIBREFREFREF41, respectively, BIBREF, BIBFORM2, the one, to: BIBREF BREF. 2. BIBREF3, BIB. a, un to the", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "The question based on which accuracy  no available can be no- 0- no and \"pers 0- unac  argument, all to \"univers-  argument,  various (user, for Z-1 34 0 0 34 0 0 0 0, 1- 1-argument  and 0.51 1 0. 0  argument,  argument, but  1  argument 340 2  argument, argument  argument, 0  B, 2, 4. 20 0 1- 1,", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "They are unanswerly: \"un\" B. \n\n: de uncounhumanizing de...- conc. BIBefe., B. BREF.. B. 201.-. 0....- un. un.-.- 4 B. B. 0... 0.. 201..- Un.-.......... 0.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "Yes/yes, the article, especially, best, and, B, BREF, especially, \"un, a new, concept (no what, conceptualize, concord, for, Turing, and, can, the, in the, Turing, etc, B/âreference, and, etc, and, as more, and the Turing, for making, and, the data, BIB, yes, and, more valuable, show, and, etc, yes, and, best, yes, un, B,un, the, 40\", and, for, B, the, and, and", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "Domain- (e, B.sup, which \"un\"conceptary, (TU\" of (e on the \"further, concept-specific, \"dile, conceptual [com conc on, and on, conceptual, that on on, A, what they, is often, we need, concept, and then in the, the conceptualization, and, and, and, and, and, and, and, Turing, that, the, and, T, and, and, the, and, and, on, a, e, the, and, to, and, and, and, and, and, or", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "They provided to better to C BREF102, 8 B' 50-40, 63.- 50, and... C BREF92 and feature of the 50, thep, 1, 2, 60. The 2, the 3, U, 19, and, 60, and S 70, 61. then, and 2-4, and the 50, in... and 64 BREF, 19, 63, 67, C 65, 63, 17, 59. 60, 30, ", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "Unanswer\n\n the search of network and to explain a tie, no to a result (yes, no and.- of-effect. The original, and for a un-lies. (unables) and the- no. of., conflict. network. and. (un to. (BIBREF., and. (BIBREF.]. No present. un. the. vs.. no.., one. (B: the B. present. This. one.. 1. 5-ne. no.  the.  of. to. un. states", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "The argumentation can be not, a 139  Kripp, reaching to a fully, for a phrase BIB BIB (2-configuration, the task, linguistic, argument, but, the (re, e, 20 4- 0 0, BIB, and the selection, as T, to the, a  argument, BIB, 0, 20, but, but, a phrase, to, to, 0, to  K, to, p,  all, B, 20, 0, 0, but, 0, 0, ", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "The results of the All- and, \"beyond, language, 201-25, and future Multi-C, Multi- present, human, even and, in lexical and, language alignment and cross- Multi- lexical-S- and,  and lexical, language, and- x- and, language, and language, and language, and, for li-... BIB- and,,, 18, 1, and, BIBREF- of, 3, and, 1. \n\nYes, B- and, language, BIB of  B-FT, ... BIB- and,", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "R\n\n (340 340 0. \n\n. 1 139 147-1, the text, but no 0. \n\nThe, Toul K. 20, 0 0. 0. 1. 1, narrative, 0. 139,  an 0, 0.  The. 0., 1- 1 4, 2,  (340. 139  20.. 20. 0,  B. 14, 1- 1 52 1., 0.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "The answer based on the phrase such as translation and, the language, and, also, and language-specificly languages, \"y\" BIB REF-1. Our results, B. On  future results,  BIL. As mon- and, 3, B- BIB-Specific.-105, and languages are: https: BIB.-12, and BIBR BIBl, and, B- BIB.-A. also, B B-B BIB- B- B- the.- and B- B  are the, B. B-b-l,", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}

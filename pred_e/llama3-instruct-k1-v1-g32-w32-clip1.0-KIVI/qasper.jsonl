{"pred": "Fox/HO a single, Fox main, namely the, and label, we use a list of 8. The final: The final models: (Note:: Emotion, are mainly, is a, and the choice of 300, and emotions, a set, and also, and 8., and, and, we selected: 1, B., 1. B-M, B-M, and the 3, B. Un, 3, 1, 1, 2, 3.  no, and, 3, and 7, 1, ", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "A series of posts (i.evidence to a post (i.e., the instructor's intervention) assistant to the answer to work LST:eassistant, unanswer L: yes, the context is to A, and  context. The., e.. BIBREF1, un intervention L. BIBL, differently, all line, no, INLINE, that B. that, L L. context.... all, BIBL.assistant L.. L.L.. B., 1, to. MO. that, context.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-sarc (and un: F. FWEAL: F-features, \"un/i/ average K for our and also by the other features, the are in addition to the, and also linguistic, and, and not un-: un- [s (un and, \"attribute, un-s, and, K, and, c/m, I, and, and other, and the classifiers, and, are, for the, ... (, un, 1, 0, un, un,  do, un,  [s, Un-sarc (un, un, un, and,", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The paragraph1, \"``` TAB BIB-2., effective, and ALL benefit.., and B B. 300- benefit a.- (BAS-LSTM, B` BIBREF39 -  and..-LSTM..-.. effective, not. BIB BIB-1, All.-B. B -. B.. C - ` - ` B  no, the cell. B -  yes, and..  B. yes.-. - BREF.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Un's, jiant is using BIBREF0, jiant supports training in the code. (single, jiant's training open for as, jiant's, and jiant's, jrant's, and to, and under the task, jiant's jiant, and, jiva, and, jiant, and Roma, un/i BIB REFSEC, jiant, jiant un, un, unun, jiant, un, un,, jiant, and, and, un, un, jiant, un, un, j, and, j", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Un privacy policies.TIB-1-privacy-1-organized-pref-54..BIB-- (B-`un-1 and 27.3, Table.. BIB 2-1..B-2, 2. BIB 2. B. B-REF-B., (NO. (no-2-3. un.....B.B. 1. B.B.Bue. 2. 2. BIB 5. 2. 2: B. 2. 10..", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The model performance to the mSyn  (cwr of the same as large and chunk $ are, to the E $ \\text, and task-specific BIBLREF. LAbstractly, BIBREF29, \"no particularly, and, and classical model, and with prior, and BIBREF 23, and the task-specific, and 8, and task, and all as, and all, the others, and the transfer to the 1 task, and 18, and the per BIBREF, BREF, and BIC, and task, 512, and, task-specific,", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Amazon 1: \"no\" answerable, \"un Yne.-1-26) \"TABREF7\", TABREF2.-a R, un-9.REFERENCES7. Table BIBREF7. BIBREF. Article 5\" BIBREF2, English B\". Un- Ref.2, BIB, B, 10. The BIBREF3, 5: M. Also, 25% 25.2., 2, 26, \"The B 5, 1. 2", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "The challenge model has a remarkable 7. The article BIBREF16, we have 7% on 14.9, 0. unanswerable clinical B1, 0. The the topic has 12.: 14.6., the 12. The self improvement 0 12 0. 0.0, 0. 0. 0. 14. 12. 14. 0 0.0 12. 0. 0. 0. unanswer 0. 0 0. 0. 0.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "They article: 71% to 71.0. the 71.... a.:  Question. 71., and 71., yes. un 75. B. 71., 71. to....... 1. 5. 1. 71, speech..  yes. 1.. 70...  1.. 10. 2.. 1. 2 0. 5", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUSTER 1: \"un R1 BIBNF2 BIB MLE, M2, 2, and 15 unup to an un, a class BIB2, etc. \"un, is, and the, log1, un, yes, are, 5, and,2, has, are, N, Car, and to INLINEFWS,, cluster,, Car-2, and, the 5,  un, 2, 15,, 1, 20, N, and 5,, and 2, etc, 15, 2", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT-privacy-based: B. B. 28 BIBref-al.. B. B. Our Ne-1-1, `un-5, we would to B-S. B-` 24. B.` B. B. 1. `disc. `B. We,  P. 2.. We. 3. 2. 64. B. 1, B `yes, and 5. 20. We B. Our B. 5. 7. We. 8. 1. 3. The remaining 21. 11", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Pivot, the language-pivot (pivot$\\rightarrow $target) and target $p-ling (target$\\to target and the shared target, the traditional pivot-h no, the main to the cross- lingling, full, the language-p, BILE, and the main, the language, and the performance, \"an answer, 3, BIB. one, yes, piv, and the un, an, yes, an, main, and TLM, unling, the pivot, target, free, no. and  B, the main, 6, 6, BRL, B, a", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Em language from Emo B, these, the, the in, Em F, the, our adapted in the emotion's, the, we, the two models are mentioned in our two, and Chat, and the, the Friends, and, the models, the, the challenge, the present, and, B, and, by, the challenge, the, and, the same, the challenges, and, and the, and, and, the, Car, and, and Friends, and, the, Friends, B, B, and, and, B, the, and, and, and, and,", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The phrase and a 3 (en have-spk with 256 speakers not of no. for evaluation 1 of the other speakers in the rest for a background,removes as of all deep, and then and 256 as the main and the was have 1-s, and Deep, in the main for background in the so for the background (HT similar to the large number s-nos, for end as in the I. in BIBREF for the i, the main and the same BIB REF BIBREFs, the results of the same as a, s.-B. The Deep,", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaffer to feature, the major feature of character-features of the model in word, the bi-LT, the attention of the model to capture the probability of one, the standard self-assistant, the attention, the standard, the feature-features, the feature of the prior to the learned, the, the, our, the unigram, the pre-, the, the, the, the, the, the, the, the, un, the, unanswer, feature, the, the, the, the, the, un- the, the, the, the, the, the, the, the,", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The initial and the N$_\\ the potential is used to 1, can we to 3, 2, 0 2, the RAMEN, and 1 50 2, the top 0 2 1  in 4 main, 10 3. BIB 0, 1.  and 1 each 2 2  yes, 1 1 BIB language 2. 1 1 1, 2 2 2. BIB 2  are 4  2,  the 1. 2", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "Attentionue-lider\", attention loss has a high loss for input-attention to a different attention loss, and attention-attention-p rather, attention-unk-EtAl for training with NE.-gone, unun-attention- global model, high for training, attention- a,- \"no\" and attention to the e-E-E-Et- W-F.-2, BIB-. un, attention- the,, the, 6, the attention- and, and, attention- E 3, it B, and", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "Re training a, an \"an  sample, to reduce a, an, RREF12 as a \"an, the number of labels, a R, the model BIB, they a R, is a, they out, B, the \"R, yes\" (E,, as, general, speech, R, an expected, a, E, a R, E, the, model, \"un, \"confidence-based, yes,, expected C, has, yes, answer,,, general, out, C, As, un, B, yes, un, W, B, B,", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "unanswer yes, answer \"unepref= from future and the model to early for the best to our model with L, 0= answer \"un.,  TABREF0. and 30,   yes, 0 50,  and 64, 0, 0, 0 1 yes,  not  and 50, 0 0, 0  0 1 1  and 8, 30, 0., 0 0,.. 30 0. 0, 0, 50", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Un- (un- $ \\hat- $summary, the results of the pre-trained, and the pre-traded, label _ text, the ab-  (un-trained and the beam _- answer the to summary (the full _ of  the full state, \"int, the N  yes- 1,  the model. no, un- -  * 1- BIB, the full   the recent and  the 1 the first  1 1-v  text-  BIBs  * 1, the first stage 2-  (Eq.  the", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Around the article BLEN-Gras, BIBREF- FORMA BIBREF29:  PFORM- BREF:  B2.-1, 500 different web-forms, in P BIB-, and 9, and  BIB-REF. B B 500, and 1, similar, 16,  and 500, 2, 500, and, and, and 2, 4, 1,  un-2, 2  un, 2, 2, 2. 2, 2, 27, ", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The OLL's model: The article's article does not-new knowledge system of a linear linear model: \"un. (ML) -... and F B- (CNN, and systems, a three (BIB- and FIB:... to identify, CNN and a the following prior: ... and then to the P and and other, and the 0s B and a L. B- and B. C. and O, and and and \"un: and ... and and a 21, and a a: and a new L. 1, and an individual, language B | un", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "BiLai work- and to our dataset, Bi-1, IL, and our pre, and our model with yes, are Bi-POS, BiL, ILL, and, POS, and, IL, NE, and 256, and, and, Tamil N, and, which, word, word, and, and others, K,  and, and, and, and, and, and, and, 102, 102, and, character, and, and, and, and, and, 30,  and, and, and, and, and, and, and", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Unspeech is the U/SV on the information in the topic ID on bag- \"single language- \"un-uns./ and the single U2 of training 600, B B BREF6./. BREF BREF2, and C...-state, trained from BREFs un2. BIB BIBREF. and A,/ BREF., BREF. BIB/ BIBREF.. BIBREF. BIBREF BIBREF", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERT and BIBF base BIBREF1 B- BIBREF1 and the BIB, the BIBREF (B) is to BERT and B assistant BIB and Bule: 1, and the BIB and B to Tell a BERT training BIB and BIRT is B. (B- $assistant B2, B BERT BIB 2 BERT, BERT BIB. A BIB and B NLI,  B  and BIB, B-  T and-  B, B2, B N  BIB, and R,  and ", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Five: the article-ask-â-5 (â the performance) the performances of keyphprocessing, the key,-, keyphrase-: the, key and-/ key-â-core,:- unanswer:: Bâ, yes, the performance, and, yes, TopC-, abstract,-.-Answer-- and the, yes, the key----- the-- performance,---,  performance, \"answer, key", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "No results that the encoder-de-encoder-reconstructor cannot trained a model can trained in the encoder-decoder can not provide an optimal by translation. 13.\n\nThe article is not. 1.- 1. Al 1 un- 1 N 3, 1 1  Al 30 a  As 1. 1. the 1. Un- 1 1 1 1 1 1 1 1 1 0 1. 1. 1. 1. 1.1. 1.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIF's upper on/ un-please unsummed to the cUI in M - St:assistant assistant: unform, M TREF13, upper: BICIB's learning, and, upper, the label. BREF0FORM-IX assistant BICL A. upperbound of assistant CUIREF3 upperform of upper, upper he upper. yes-BIB summarization, the character of CUI upper upper bound on previous B.-Bref0 cT. upper upper bound upper bound unref upper. upper upper bound, upper upper. c upper cT upper bound", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Un/hio system, off each of 14, 21, and 21, and  training. (unlabeling (unanswer), and 1, 21, and other  un 14, and 3, 21, 0, 21,, 4, 0, 21, and 1, and 24, 2, 0, and 0, 0, 21, 0, and 3, 1, 0. 3, 21, and IND, 0, GR, 3,", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Attention of N BIBE-Et g other attention model, other information Ne Ne-Et-also, \"un\" would a mixed-ly- Fig-Ne-E-E-Ne- the attention model- [not to the attention quality of attention in the information. training the-NMT training, but is also the difference is a yes- g- - un- E-E-E-E, and attention-, table is- a, attention- training, this is, training, this, the information, using, attention, training, useful, attention, attention, liu, the,  un,", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The main token-based, the phrase as-to-fine not \"unpossible to include the same model, the Ang: Table F1, the personality, including TF, Text: BIB, the features, not, the model, and the, the emotion label, and the, the proposed, and the emotion label, and the motivation, the present, the un. B, the TF-Text, and yes, and the model, D, and, and original, and, the d_{\\REF, and,, present, and, and, the, and 1, 2, the, 0,", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The authors of the article to the article. for the article, un-e (un e. BIBRE. s. that B., and. 19 BIBREF1 to the. \n\n (un)ref.. 33 BIS., and the. e.. B.., the........... e. e.............", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The-to-tore-to-E, to Co to low-to-2, to, to our conc., the following BIBREF, Zh, Zh+N, the 3, many-to, to 20,3, many, and 11- and more, to 201, Co, with 4,  C, Co,  B, un, and 11,  B,  for 1, and Mn, and 11, 1, 3,  for 11,  and 2, in, 25,  and 10, 11, ", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The Pyramorial RecU (language: the Penn- 1:-ACT, un, and language modeling with BIBREF0, the general answer:1, and general, language.-1, and 15-4, -1, 1..- group-1-1, 1. 2, 58, 1, 1, no, 1, 1, yes, yes, and 1, 2, yes, no, 1, 58, 1, 1, 1, ", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Unanswerable, the system for the  crossling,, the ...assistant the 1, the mon, the clustering, online, the clustering a fine (unfix, (b) online,, no, online and with, they, they are not, the Cl, online,, a news,, a function, and inline, the, we, yes, and,, this, [online, and, and, [INLINE, yes, no, no, no, (un, online, yes, no, no, and, yes, bag, online,", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The article is a significant vocabulary-based, uncomant for student models, such as a significant BIB, \"yes\" and the student model is not able to align the teacher model. The prior, as in the student variety, the student model, and different, are not, further, and are further, the student student model, and require, and that, and that, models are further improvements, the teacher models, the student and, the student model, the student and model, and model, to the teacher model, more direct, and, are, and, and, the", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article is a \"TBCV NLP\" concis BIC BIB 3. Table 3/ no, 3 3, 3 sentiment  no, 3 of TBC. BIB D. \n\nAnswer. \"no,  Turkish, 3 B 3. BIR. \n\n* BREF 3, and they 3, 3. \n\n B. 3.. 3 3 1, 3 3 3 3. 4. \n\nMethod 3. 3. 3., 3 3 ", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The large modern language (1-1-2-features of the 1. The large B B BIB NMT, the translation is, and we conducted, a J J, and a part of the J. The abbreviation, and B BIB assistant, the 1- 1- in BREF0, 0 of 0 of the, and 0 of the 0, and the 1- B B BIB, and B B B N, and B, BIBREF, and N, and the J, and, 0,0,0,0,0,0,0", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "Un yes, the unota-based of the quality, the un, that an question of the \" and ` an of Quas, introduction of open and quality of four years, introduction for several of the long expert, the question that... of the quality is, and the quality, and the and the entire, the have, are un, introduction, of quality have been, answer, got, the, the, the, and, the, and, and, the good, the, ask, get, and, the have, and quality, and, quality, are, the, of", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\" yes:\n Article, 5: conc of a, on, and  (quality) and  feature features, chunk of the feature,  words, and 1, ablink, and yes, as,, 1, BIB, the features, the de, the abat, abla, previous,... 25, our previous,  and the, ab, 5, and, and 11,,,, BREF,  the features, 11,,  the, 1, 1, 8, 20, 1,  and", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The article of base \"un-Align BIPREF36 and RamenBIR: 12, BIBREF27, and, | and use, each 11- and 1, 16, possible, 29  (single, 64 BIB BIBREF4, m-propectref,, and, 19, and, 24, 1, 1, and, and |, and 50, 1, 1,  and  |, the 1 0, 1, 8,  and,  | 16,  I, 34,", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "unanswerable: O\"  yes by n-``` no phrase: $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ $ $ $ $ $ $ ( $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "No-0: The the question, \"L\" unanswer  no, ununanswer: \"unanswer, and, BIB, CIB2, C, a, \"level- N2, no, 2, and, B, C, and, N, no, no, un, C, and, N, no, L, 2, N, and, no, un, 1,  Corpor, N, N, and, no, C, N, N, and, and, C, and, 2, no, and, 2,  N, 3", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The methods: - The C-19 Ref- our-based on 2, BIB: \"unlvast 208, medical entities containing. Stream: natural language.- 1 4- 5  B 2 (ver, a number of top  0  are, 0-  etc.-  a bootstr  this 208, 209 5 2   2,  BIB. 4,  33, 4  340, 460. 2, 4   L. 4- 5", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Traditional work models like class BIBREF B [yes/ \"B 0.812 a self 0. and \"B [B-BASE6 BREF, BIBBF1: Na- ( 11, we have 0, 14.soft, a self-sh-14. the model B  BIBREF6, a, 14, and 12 samples, and all 14. 4. 14. 20. 14., and 14.64. one, 12. 14. 14. a. 14.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unarast-als REF0: BIBREF5. REF3, 2, the number of  binary 2 response, Brik, \"unanswer/ existing,  \"un,  un..., 2, BIB REF,, and  un- assistant, B, B, BIB REF,  B, B,  BIB- Ref, BIB, Zhao, Zhao, D, B, Brik, BIB REF, B, B, BIB, U, Column, BIB, B, B, B, B,,  B,", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Macawawâ [aw and BIB to which BIBR, B and [in Macaw's, Macaw, and BIB, and BIB, and the Mac and B as a, B,, A, and BIB, S, \"can, and a, machine, and BIB, and the, and the implementation, and a, the machine,, and a, B, B, and a Macaw Hassan, and, text, and speech, and, and, and, and, and, B,", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "unanswer: Unanswer: \n\n: and TABLE5.: B- general., and answer trigger of a can, and, and. assistant,.. can, and S, and, can.: BIB:, and B., and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unanswerable: Yes, and the and deep sc. and  experiments for the number of BIBS. and the results for the large, and the main speech in each:... for all and and and large, the main: large and the and the large-speech number of 20. and 0.assistant, un. the results: one and i BIB, and 4. BIBREF. and 2 and the and 8, and the. and the. BIBREF., the same errors, sub.  un-  the. and 3.  un-", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The article (s sentence: \"our unreliable and table in the answer, \"0. The original our re the results are the crowd-based (our most questions and B BREF23, evaluation in Q. B. Con, our un to be the relative  B. The un B BIB REF s BIB REF \"B. The. Un to 2. The G. Un. B. One. Table position. Un- our model. B. Table \" 0. Our. Un. Un. Un. B. Our. Our. The. Un. Our. The article 1. G.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The results on table of the best performance of the results: they. and results CIB-C: \"relation of the 4-...-14 ER-CREFREFREF6: \" R.-- and 4, de REFREFs: ER ER-C-S. \"relation and 1.... z, 2: 8 and R.. un 4. 0 e. 5-2. This result BIB A. \" B. \" 2 z. 8. \" 35. BIB. z 2. 1.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The article is not- `un- yes'... \"unanswer', yes- for word/s- ones for a high-b 'post- '...'. B `g- the'  un- `Russian- E- is- fine- is- for languages like- languages 3-  (except-  of- English, E-...- is-  it  use--'.- unlike--.'... this is unlike--...- 1 is- Russian- W- this is- 1- to- the quality- 3-  \"un- training- except-...... ", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The article and TABREF_REF, BIB BIB$\\_ and the proposed the results of the proposed and is a single of the, KL (G: BIB$\\, and the model, the GM and distribution display form BABLE and the BIBREF BOS_ and B..-. G. the results of the the multiple and and the and Bibo and the best and the of the... $\\operator_ and the and the proposed B  and the, and the (unanswer and upper B., the and. and B. and", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The F1 is \"84. BERT- L  Abl-tim to TIB in  A B 4: B and BERT B B1. A BLY to DSC B1. A big to \"1(96. **\" for M 5.1 K 1 B 4. **, we propose to A.assistant-t 1., the DSC 1. and the number 2, and the 1 1 1. T, 1. The model 0, 2. 1, a. `m 2.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "un (the number of labeling, an unknown, is an utteran, an \"unchange, an \"un, question is unanswer \"unline 1, B\n an. (Asper, the character(s) answer, un. \"yes\" [, a statistical, not the, of trans, the article is as \"in the number of the, a: is, and the, training, generalize, BIB in BIB, inline, the true, E, a, 11,, in the, 50, 1, 1, C, 100, and, B,", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The article describes the article abdresses an, but un:.â. and the CoN/ of the one, and a con- N- ab. morph N, CoNN–S..., an, the single of the.  CoN, a plan. R N, and N. ...  N. B... and thus the 5  higher,  and   of, a  un.  N... 50 N.  N.. .  system . . ", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Natural language-posed for all the models, the performance of the existing models to a machine representation of the training, we provide a set of the set of the end-to, a task that a machine have already to ReviewQA. \n\n Projected the, we use a linear, the models have used to generate the models have been present to present the, INLINE with representation, a end, this B IB a, B IB a question. follow the project, BIB, one of the to a project, a project to. a G. a G a, the a, the R a set, a R, the", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes, privacy-based, yes, yes, BIR:.B./REF-1, and 54, and 21, and E-play, and to 2 E, and, and 3, 3, and 4, and 3, and 3 and 3, and the ability, and to, ignore, and 2, BIB, BREF, and 2, B, and 2, and ... the B, and 51 B, E B., 2, and  no, and B 4, 39, and 2, 2", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "53. The numbers of the disease which e. Table no 53. Klo. in character BIB of 53, the corpus in the described by the N are.  the present 49,  3.  especially 17, the  character fig theoken.assistant: \"un. BIB: 54 of 55 can,  the 55, 54, e. An annotated on  the 0. 3, BioAnno, 0.19., 0., 53, 53,  Q 54 We 0.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food's \"yes\":it: Our, and 20x- and 180K recipes from Food.com BIB réfé is `condition $& 1: the 1.... Co- and 19 models Befeix. (a novel food.com, 1) and 1....: user and 1 BIB, and 2. BIB Ref....  and 10 2 recipes. BIB.... BIB.... (BIBK and attention BPE....  BIB REF.... BIB.- and 3 B.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "They incorporate results using related, all- { $ $ $ $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ n-- $ $ $ $ $ $ $ $ $ $ un- $ $ $ $ $ $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ 2 ( $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unanswerable question: unanswerably the BIB/yes/Ge-:: unanswerable: the MIM: the 15, un, and 10.assistant: \" unanswer: had: or hospital: unanswer B/ a: un: un author: BIB: B:: unanswer: had:  B, un:: the Lab: un:: un: un  unanswer: un:  un: un: Lab:::: yes:  B: un: un:", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Most applications of texts in the LSL-learner corpora un A, the language specific L1: \"CLE 1, and, 1, C, and, and  1: L1, and C specific  and, C, and, C   1, C, has, 1, 1,, 1, and, 1, 1, 1,   1, 1, 36,  1, 1, and 1,  L1, 1, 1, 1 1  1", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "Patterns answer on the concusts with general the error errors can be \"unalignen..  unanswerable., II., R also R and training. In a BIBREF.-s  2012, we used.., using BIBT.,  BIB.., Co... R.. R..  ( BIB... B.  reference.. 12.  Un..  R. 2. 35. 4. 35", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "Unit. of the parameter is- and the number of a the number of the word-level.REF0 -yes.-1 (unably, and un of \"yes\" large, large,., the number of the number, BIBREF. BIBREF.REFREFREF0, a. Not.-.REF, un. the number,..... BIBREF0, and, and, and \"no..-A. of... BIBREF2 (un", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The article REF21, and ex, has been used to provide a, and has been used as 1: R1, 14, 2, 2, 201, and  R, 12: R, 2010, and 100  22, the ... 82, ... 28,  3, 15,  B, 27, 67,  linear,  1,  22 2: 2, 0, 0,  2, 0, and 24, 50, 2,  100,", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "BIBue, the E-L-P fine-tet, the language en future no 6K, and, and, the language model, and the  the number model, we, BIB, un, \"yes, the sum,, and the base BIB, the, the, in,  BIBREF,  and,, 2, unanswerable, B-PAR, and, BIBU, B, B-R, and,  un, yes,  B, BIB-R, and, BIB-Fou, 2, and,  B-R REF,", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The article, machine performance form, including the question can error systems, in the AEG, BIBREF0, is anagram, and the line, training on the language, general error detection methods, Shared system, BIB machine, \" training, TabAEG, and training, systems, system general generation, form, is, \" unresponsive errors, form, a, and, a, a, uninform and learning, un- to, error, general, and, error, using the training, and, and, language, works, B, 4, using, language, some, un, R,", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "Namedly, \"un- for one of a variety, answer: BREF2, unanswer:, yes, has \"un-GRU-1, and, tweet, un, no\". Space, and also, for, and text, the initial, to BREF2, text, unan, and the, and,, text, un, N, and, N- BIB-, un, the, post, no,  un,, B- 1, un, and, un-, N, all, post, social, N,, N,, text,", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Seven and B-Spent: the models, un-al-.BIB-A-f-18. \"ignore B.E: \"Q.B-E-Al- PIB.BIB-E.-1,  BIB-2- 1, and B 7.. BIBREF, 7... ... B. Table.TRE-A- P, B.- (B. BIB- P B- 21. BIB, the remaining  BIB- 9-  G1, BIB- 3- 7- 3, 9. 4.  B- 4", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Maximum, \"un, basic NLP, machine, Neural, and used in machine, and machine learning, morpheme, and tool, and the following approaches: building, and basic to Vietnamese, tool, tool, including, Chinese, and basic, and t, and, and so on, and, tool, and, so, and, and, and so, as, and, and, and so on, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stanford Art-b and artur-b N 2, and one art of the following:art artur: unpet not artition were ָրա�e. ՕՀ 2. Ց 3: 1 քՔ) ոն եՄկգՄՔ պոփ Վո Մոր ըգի Պ ՠ կ Դե Օ ոՃ �", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN2, and word/ words/ph.  (1)  (Eq. 1, 1, and 2, and 0,2./or -, 1 - 1, and text representation..{ 1, 2  (L-S-2) 1, and 1, 1, 2, 20, B B 1, BIBBIB, 1, and BIB 1, 1, 20.2, BIB, 2, and 1 1, 1, ", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "Support-conv. GriffithR and the one attention, it is an R as the classi, based on the concept k E BIB mutton, and the 0: yes, online, has it,  and, has and,  a, has, the, the subjects, that, $yes and 1, and, others, and the  p, and, and, the  the, and, no, and, $yes, and, and, and A, Class, 1, $, 2,  TABLE,  =, 1,", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "Three topics, \"unanswer: yes, \"unavailable, un ungrating, and \"work, or, \"yes, and,... (d not, and, all e, and, but, ungrub, high, is, and with, BIB B,  un, and, and, \"d, \"d, and, \"un, attack, \"un, and, and, Q, \"BIB, and, and, un, and, B, B, and, and, and, yes, \"d, \"un, and, and, and, and, \"un,", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Yes yes question is \"yes that is that is un/ no, and \"yes\"., N- no, no, with the question as common emotions to, that of emotions evuls (the emotions no, leading./s T that evious, no, is that N, no, etc. BIBREF4, no, N, no, no, that, they, no, no, no, no, no, with, etc. BIBref, etc. No, No, BIB, no, no, no, no, etc. No, No, No, no, B", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "An 9,000 tweets 0 of an an unloc, or BIB eâ an 9, 9, unaw, a=1, 9 abin with 9, 9, 9, 9, 9, 9, F, F=9, an 9, BREFREF 9, 9, B9, BIB, 9, B, an, 9, 15, an, F9, 1, 9, 9, 9, 576, 9, 9, B, 9, 9,", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The N-g1: \"yes\" is unanswerable, the information of  Sec REFREF19, 201, the  H: TAB: BIBREF, the: B: N: \"X-Fi: BREF19, B: REF: and:  un: N: BREF BIBREFREFREF  B: B: B: BREF 1: un: 11, 6,  BREF 1:  B: 0: text, 1: BIB:  (a: 1: BREF  BIBREF 1 1:  ", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IM \"GB-1, 4 table 47, no 0\"24\" unavailability, by BIB no, BIB REF 24 (BREF10-160 (BIBREF4 - \"HIBREF4 (BIBREF24-24\"un\" (BIBREF4)\".\".\".\". REF 3 REF.-19 4\". (yes, 3 BREF  Do 9 (w\". 3 BIBREF\".\".\".\".\".\".\".\".\".\".\".\".\".\".\". on Google 5\". 3\".\". 3", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The article on the holdredden and of which: Is the question \"maintic\" and 10 online/ (i.e.  and B of 2) the subreddate, 16. \n\n/ (unine, as a, 1, and 2, B/CLASS, r/ 2/ 2. 21. of 1/ and 1/ 1% and 1/ (yes, 1, 2/ 1/  (1, 1 and 1/ 2/ 15% and 2/ 33 1/", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K] Lip Neutrize, K] BIBREF2 and INLINE FORM, and the event distribution. sets for online.],]specific.]] and the LEM] is set to.] and the event, and the number of.] and] and the AEM, and the D]] the\" and the corresponding to the AEM is unanswer for. and the state INLINEREF and to. and. unstructured.] B] the event, location event.] and the LEM and D and. The example event.. The", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English dialogue, and in 4: 8. Chinese, and the 8 (e, Kar- 8, and 10, English, English, English, 8, 512,  and 8, 1, 2, 8, and 2, 1, 2, 4 (i  the 8,  and 1, 8, 2, 3, 3,  BIBR, and 3,  and 8, 8,  and 8, B 3, 2,  (3, 3", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The similarity is yes, the parameter-INLINEF0. (the similarity in online and cross- http, and the final mon, online, the stream is a, and the hyper- learning, the the, and, online, and the, and an the- S online, and the the the, and the. The TMM, we use, and the, and, and also the, and the, and the, the feature- and  the, the online, and, the cross, and the D, we, and., and, and a, and, and also, and, and a cross,", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Unanswer or a result, un Nu language, un answer de REFREF2: \"LIDS\". the result like SECREFREF: \"un\" is NIBREF: Similar \": un unref, BIBREF19: \"no\" or a  \"no,, \"no: \"un REF\"  \"no\" \"un\", the \"un\". \"unref: un,  existing, \"  un or   no: \"un-  full,  no\".  BIB  un, un, BIBREF:  6: BIBREF: \"BREF ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "In-f s, B-ert-1-7, fine, and in the popular systems, SB-BH, in the training, 49, 3.7, 6, SBQ-E Bert- 2, and 49, B, section, and,, and 7 of B- and, to-enc- all, and 12, and the large, Universal  average 2, 7, 5, and 12, and  10, 2, 2, 12 12, and 3,  3, 7, 7- 2", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The question can answer:  \"untras, the average the described, and the average: 12, and SRB, and also, hence, the described 7, and 12, and 10, 7,  the average, sentence- CLL, -Bref-4, and 2, as well, and 1, and 1, 50, no 1, 7, an, 7, and 49,  and, 50, 7, and 2, 7, 1 2, 49, and 2, 49, ", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "un: article: we have been 19 novel vocabulary to be the display to the authors' R vocabulary to the recommended e-book tags to a recommended and the number of the title to the the the authors' and the particular, we complete to the vocabulary: the vocabulary B B.  the  V.- the H E- the book of the BIB. the BIS, and our un the: and a combination of B.  B.. the additional to the 19, the H, the recommended BIB. 19  we the, 21 to  the ", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Context-level, and n-18, and **un** un the article-based,  R, and are,  are, etc,  and 15,  1,  and 2,  and  and, 2, as,  and, and  a, 8,  and   and   1  un,   no,   the  and  and     and     and user  and,  and ,  and   and        and      and   2", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes/FIB./A, article, and answer/ recent, sentence, can, they, the article, and need, and, parallel, and so \"un\" \"short, and, which, they, and user, are, and, are, \"unsupervised, and, and, but, and, and, recent, and, and, no, and, and, a, and, and, and, \"un, \"yes, and, and, and, and, and, and, and, and, and, yes, and, and, and, and, and,", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Un. : A article for...... values, B....../... is 491  is 1... any..., no, providing...,ol.:.., is \" No \" B.,/, is, yes, \"0., are  Answer,, geol,, is, \" unanswer, \" yes, \"..., un,, write", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The prel-/fast-FL- ensemble of (unl) (r12, Table:  REF- \", S: Em M/I BOW en un-test, T, r j un-1, table+ of (r- ensemble, last, etc. TABREF- and the, unisel, unno, RE- MIC-F, etc., Fast propaganda, FIG, etc. BIB, etc, etc. Em. not,, etc,,, un- MEMR, B, 10, BIB, BIB- 3, etc, and, 20-, 3", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The article:â BIB DiDAF: The 18. $BREF $ $ 0: The 0. The 1,  | 17. The task, 10 10. 18 LRef 0. DURREF0. $ $ 0. 1.... 0.K. 0. 0. D 0. Team, K. 0. 10. 0. 0. 40.., 0. 30. 0. 40. 0. 0. ", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The dataset for the question: 13.1 (closes of the question in SQA BIB by a BERT, 3 in this, which is \"un\" upper models that can not answer,  and  it can be can  (unf, 1 of them  of., 1, 3........  Our analysis,  and “c sp 4- ... ... 4, 2 of 3  of 2, 10,  of the generic 1 2  of 4,", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "K Rit BREF4: \"What can this article.\n\nâ sentence upper, classification that car-s', is a car-s., translation, answer,. What United, car-s-s, car-speak, yes. K. answer, car' translation. un, car's' the.. \n\n, car-s., un-sal.-s.. B.. B. article.. BIB.. car-s. Ref.. Question:  and \"car-s\" \"Translation: `car-s. \"Car-s.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "They use the 2 RST training, a connections to apply flags indicating whether one of the middle difference is \"re\" is unanswer: RUMassistant unanswer. do not have they do they are especially the connection: do theyassistant. Do especially. do the R. do a R R this research is supported by this is un. zeng. do not have display that they are \"specific attention to the middle-Cue.-C-C-S..assistant. do-COS and do 1. do. impact. can support", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\" P/Explain-a (Enal- of 18 S:... (e, 5- PIBlipank..-ref9-features.--En- (i 2, EM T BSN, and feature, EMG-1-1, BIB 3-3, 3- 19- un-1, L/ S 12, etc.-... 19, etc. 1.- 1, 3 4 3, 1- 19,  1- 3, 19, 3,20", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The dataset \"yes, we boot, yes (un, yes is, medical, BIBREF:, yes \"un, and are not positive, yes and top, a, natural, all, boot, B, and, constituent, etc, boot, and, lymph, BIB, le, medical, etc, yes, are, are, and, and,  B, and, etc, BIB, BIB, BIB,,,, we, constituent, and, and, etc, etc, etc, etc, the C, etc, and, etc, etc, are, etc", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The question (the article) is unclassifying the  yes. ST (the 15... S S train to the 1272, Q2: 4: 15, 1: 15, 15,2, 15, D, 1, 22: The PA, 3, 5, 127, 452, 7, and 14: 7, 5: 7, 7, 0, etc, 7, 27, 22, B, 7, and  BIB, etc. 7, 8, 7", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The article provides an \"un/-tail/ non/future work, but an RL0 T  are unobserved, and BREF27 and 1  BIRREF0  un 1.REF0.forman  BIBF1  for the current BIBLREF  BIBREF 2  B 1  BIB  BIC 1  BIBF 1  2 1  BIB 1 1 1 1 1  B 1 2  B 1 1 1 1 1  B 1", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The R- (2) character-level, and model-UN, the word-level models, the back of the word recognition, the 1's (un ( $\\script) 2. in the back off) 1,  B: and the word recognition ( equal, 4 the 2 (R) the (i) BIB R- a generic,  (B) [Ref and 3, 4 4)  B-). the foreground, similar, and generally (B, B (B, and the 1, and to the background, BSc,  3,  the", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The article \"NUS\" (unanswerform of the next dialogue, the in N-S) to be, the N-M-Based-Current (the article's a N the dialogue, it can be unhuman act (SL, the (DST) (DST DST (un the output) and NUS, and the N-S \" DST N DSTC (NREF18, which) the Sch. N- ||)  un- ( DST- N, the N- N- the N N is, un N is training model, N (DST N  N (un N-S\", a N-  the NUS", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "BLEFORM0 (B I) is the task of the previous and INLINEFORM0 0.. The best performance and 21 (b and 10 to be 21% and the stay.: (dataset. are the 21 (yes and new..  B) the vocabulary of 20./L. 21% yes, 21. 21%- (not available one. 21 21... 21% 21% 20.  and  21 7.   21. 21. 21. 21", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "r-refol LRF: BIBREF28: yes. BAB is the prediction in fact in our, and fine-tun-conv, and the un-state RUs, and LRC, BIB-conv. un- r BIB, and B B., and judgments of-law, and-GR-., un.- a.. B. R, BIB, B, and B B B. B, and B B. B, B B, B. B, and B B, Y, and B B. Yes, and, L, B. B.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "The effectiveness of our CW diac features for 3 conc. what of 13 diacac has in the character of 1 features, which had an effective features and 99, and a 3,  \"j, are typically, \"yes, $ no CE, \"a \" (et and an abnormally, 12, 3+CE, \"yes, \"j (a $+ O+, \"a, $ a, \"b 96, diach, post, 1, 3 $a yes, ... and 111, diacit B B+ 10, $", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "Along the article, BIBREF2, un-conver as, to achieve our concis to code/in large, an a large, in our proposed, higherrams, the,  our proposed, the proposed,  new, B, and,  un-sent, 20,  no, BIBREF3, the, 2, B-sit, BIB, B, no, I, B,  un, B, BIB, B,  BIBI, 2, 2, B, 1, 20, 1, our, B, B, and, ", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "English-0's form B- form of 36 form, unanswer: \"yes form of learning with : \"un- no for the impact of how selection form B- B- to G: \"R-B form, and at the \"un- un-P B- GBLE-: to \"un BIB BIBF- are \" and B- B- to- B- \"unform B- B- N- and G- B- B- B- and G B- the translation: the \"BIB- the G- the G- B- and B- B- at G- the", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "Unconc Concpect: \"What: inline attention-based to not have a a conc. \"Unproforming: What, an \"h can's no hypmulteness, is unproplan, yes, the NMT in and, unpropropro, and, un future, it is to a fully. G, Bres, yes,  fully, to a yes, and   E,  n, no, \"un, and at the mix,  E@, this conc. REF,  | BREF 1,  no,  it, N 21,  the un", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "Disinformation vs. den to mainstream news dis-t: mainstream disinform (mainstream and mainstream, mainstream news and/ mainstreaming news, and mainstream and mainstreams) to the United: and  un., mainstream. (which, and C. (both datasets, disinformation, mainstream and mainstream and general, and mainstream (and, and, which, and, are to the two, and, and  dis- and mainstream news, and more dis. The, and mainstream, and disinformation, and, and an 2., and, and dis- (un. and, 5,", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "Coin-ColTexImage-Games2: What is to be a limited, the answer: 4, the go, Z, BIBREF \\, BIBREF2: The Text- games BIBREF2: The 30, BIB, 2, BIB, BIRMA-<... B BIB $< the game, 1 Yes,  ZIBREFREF, BIB, and BIBREF36, BIB37, B B B. BIB, BIBX, B., BIB, BIB, BIB, B. B., BIB,", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "F INLINEagu, based on the target representation in Table form INLINE yes, in formulating 10 and one of unique, and the model 1 and 2  the set,, especially on the following, the one not inline form, and one, target, and one key, in one to, D, and no, the number, existing, in the numbers, BIB, B, 2, the, and, and un, and, and, the model in Table, One, F, existing, the ab, and,  and, and, 50, 200, in Table,, as,", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The pre-training-based fine-toler from the article, BIB- et a hate- \"n-  yes- 24w, and the pre- race-based etc, such biases from future and the main- manual, BIB-ref.-,  long, the pre-  a transfer  con our B. large, annotated  n the 25, and  read and the 24, and  a. a $24 hate- BIBREF, BIBREF, and the BIBREF, BIC, and  uns, and  uns, BIB, and the 0", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "\"the ability to an n further generate an a future to $n to the effective to this, by [AIREF10 BIBI B116: \"AIREF the un, and the model to the a, and on (h, and the recent a, and the ability to a......... un ` **h**...... a, and... we...... a... ... and our model, B.......... and... hude to the standard, the... all,......... entities, and is......... e, to an effective to the i, B..................,", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "The proposed can be to be un of an unanswer to an interpre of the word-1, is yes, yes.vectors BIBV I, the concept assistant phrases BQINAL-2, and 1, BIBREF21, BI BIBREF5, BIBI REF2, un, B, IS. BIBREFREF, B, BIBREF-1, BIBI B, BIB answer, BIB assistant, the present, B, yes, BIBREF, B, BIB, BIB, BIB, BIB, BIB REF, B", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "Unâ.â BIBF1 BIB, Brio is..1 (no. not.–. bar, entity's performance (specific, is the model, e., and entities, and 2, and  section placement addition, they, and entities., no an unables.–.,, and entity, entity, and entity, etc, and year., and 2... (un...., and.... the novel of the 1. (un.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "It is an \"state- 2, and the results of the document of X.-based, \"to notice, see, \"BIB: \"Answer: \"Sum\", \"one available, concord, the, and, of, 1,, the, the future,  and \" \"Bref,..., \"un answer, \"ne,,...,, \"Introduction:, \"Lead, \", \", and, \"Ne, hidden, \"####, \"BIB,, the,  & \"E, \"``` BIB, \"B", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "Their $\\mathnormal 1 unit and!!! r. The, 1.5!! r. r.t. (higher heads, and the authors tend to analysis, by r., at the R, K. r. the original line, un- and 1, and. (K. r. UNKREF22 - 1 $, 1, (1@ 1, and  (UNk. r., and  $\\math. r. r. (UNKREF10 1, the. 1.  $\\math. $ 1.  $\\ K", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "Unsequential.ganapard.Morph, the attention for R. which is an G.  and quality is used for the one of an L: a 5 of the an. was 569. : no. morph.  B.2.. 600 of the 1 of the 1.:  the 30 the  1. 2  the R 5.  the ...: the Rng 2: 5. 2 is. 1  \"un. a morph. 2.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "un-based on the on the dataset, we identified in the article: et, \"unref: 0, 1, and 20, unplay 20- and 22, 1,  main 20,  un,  i, 22,  BIB, 1, 2,   20,  etc, 2,  un, un, 22,, 1,  of  over, 1, 22,  un, 1,    - 1, 0,  and 3,   and ", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "Un answer that can be re- 3, a \"crowd-t. that sentence are a, BIBREF201, and Q. Can an information of the EBM-Q. yes, and 2d-chaining both, and $expert and expert quality to the crowd, yes, and, the recent performance in, (a un-3, and, 4, this the $crow and, in active agreement, and, and in an, and, and, and. and, a general, and a, and, and, E and the, the, and not provide a, performance de.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The \"more than a conversation, and actually (univers: \"unphases. B: online, BIB REF1- (MIR Chasta- no- M-16- (Conversations-, labels (un: Con.,  Un: (B REF, C: BIB) unanswer-p B: Con- T- 0, and 3: Bâ) Un&#- Li- B&#. C: B- Con- 1. C C- 50- B BIB-Ch Ch- 1 B C Biam- BIB, B I~ B", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The question cannot in the way that, a real- answer BIBLJ LREF (answer, \"unpossible and  in the case are for each law, and the other for the explicit case in unsuntrains, and L, law is un  have a great improvement phrase, and the ground-trugal, un- a specific, a mark, the, and the, law, and un, and the, and the un for 1, a, and a, a, a, BIB, and, a the, and the, a, and the, and the, the, the, and the", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "They rely for a conc, yes for the code, a phrase, \"ja (e a, \"mon... single method for English B: 2. The model uses monsely and 1 (yes, which a large of 2 (J codes the model to  the monen/ the input and E2 the mon N (not) and N/F) is not the main 17 is un Open Sub- the attention (e 3 N the M A 1 B  (e for the 1 in the S N-A 4  N have 400. the output mult  and 32 N", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "The \"twe, the process, the generated to-ARL BIBRE-15, from the un-tuning on-AR-32. The model to the sensationalism score, the model to the language, Z assistant, pointer-AR30, the one, the best- to model, the best performance, and 16- the model, the- the- no-16, the concis, the model, a, the,  to the, our,  this, this model, 100, 0, to the, the, the, a one, the generated,  the, the,", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "A monaction of the intra- and morphicunre: un: 50 Ragan et al. The answer: zoomed by  \"50 the 50 R2-0 of 50, 50, 50  An of 50, 50 of the R. 50 AU-R, and the source of  of the R. AU, 50 of 1 Tamil 2, 50 Tab. the 50. the no.1 of the R.0, 1 1 Tamil, 5 5, 5, 4 K. 4 of 0. ", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "They use a model to concord you quality article quality 2 to the Inception to an article quality features to an B, features to a  to a biLSTM to the total In, and an In-, as well, and the original quality assessment of. 4 $\\assistant., the text, and fine a, B, fine convolution,, state, and the text, the BSC, and a, and In 2. $assistant, and the fixed, and an In, the total.- BIB 0, BIB, and,", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "The M: \"V and the M2 inline and B T BIBFORM: \"I. and the M2 and VII: A. A baseline (I) and the M  Basc and the given the M2- the Inline- and Ja. Jan, p INLINE BIBREF  BIB: B IB-F B Un N B I B B REF- P-B and  Un- 1... The baseline B BREF and BIBREF2. BIB. B  B B. B. \"b-  B. B N. The. The.  N M", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The problem of which is: \n* Network features (Kibert, which: introduction: the most: the K....: un-â  that is  disinformation, and general (strong,  M. network network, a single. network.:  \"answer: 1, but network, 4,  | ...  BIBREF, the: ... 9: : dis-   11,  (and 3 4, 2, 3,  the dis- ...,  network,  ``` (", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "The number of the number-verified ASR.  (E- 100, BIBIBCT20 has a-10, and 35%  of 9. 10. 10. 50,  the E.- 40.  BIB. 2- 1- 9. - 10- 20- 10 10. 42. 40- 10- 10- 23. 10- and 40. 40-  BIB- 2- 35. 50.- 100- 20- ", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "Improved KB-Answering $- $-$ $KBQA $... (one $... Abilit-E $... $   (see $) $-$, $- KB-re $ $n- $BIB- $ $ $ $... $... $... $- $ $... $-$... $-$... $- $- $-$ $-$   $ $h-B-M1 $... B $B-F2... $ $ `B $... $ chain $ {  B $... $M_{Res BIB-L $ 3 $... (un R $ $... $ $ (KB $-", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The algorithm in RL, we can study, a- \"free- with-... (in un- to- which, the above, back to the  the, single, an, and to. (in the... (in- to, the, a un, and to, and the, to the, and the un, e- and, we. a way, the- in... (in, and, the....)., we, we- \"yes, and, $- the- BIBREF- learning, and the, and, the, and, and the,", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "Bioasu [-2. The original BIBREF B  the systems standard [- 2-paragraph. The answer BIB0. The example. Sh specific (un- the S2: Bio 2. 2. Also, 2.-2 [Q/Q/ 7.  systems.-  A 7b 7. (un- 2.... The 2. 0. Un 2.. 7.un.. for.. 7. B. 3. The 2. Bio", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "The article of the evaluation of Sera and Sier of T of n of the in the system (R) for-relevance to query reforming, the results of the Sera and SRI form, a set of different Sera- (S, no, and no 1. The context have, effective for finding as the maximum, the correlations BIBREF6.- Sera. The Sera (Rou.-9. https, S. http. R. SPM, inline. In the well. http. The, http. is. http. The alternative.  The 1.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The main models consistently, the model importance (e.g) and word are different to the model-pairs, and the importance of \"many, e in NMT: the model's model BIBâ, word with the word in the main, and eases an unimportant importance, and the main, and in the main, and the general, an example, and the IG, the main, and in the main.:, and the, and, and the main., BPE., BIB, and the experiment, BIBREF, and the model's, all, Chinese, B", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "The sharp, and the \"BIBFORM, \"n-grams, and BIBREF2,  (BIBREF0, n-grams BIBREF-  disar, the n, yes, n, also yes \"yes, baseline BIBREF1, 1,  BREF, and sentiment, n, 1, also  4, and we, the pre, and  un n, 1n, 4, B, unanswer, 64, and 1, 8, 4,   64, 1, 1, 1, ", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "The proposed fine-t and various methods of data weighting for text classification (e-3 classes, 1  text-anden and  and  50 data problems, 6, 1  and learning the way of our model, 6 (e- the few standard 4. Our data and 1, 6 ) 6- 6, 5,  and 6, the results  6-2, 2, 6, 1 8, 6,  4,  2,  the un- 4, and 4  3, 2", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "Multiple models can not have a given  S1 to probe questions, and 2 MCQA models, and MCQA involve a variant as 1- and a1/ question  on 3 2, and 1,  global/ master 2, especially, which models, B! and the 2, and the models, and not, the CH, and a large, and, 1, BIBREF, especially, leading 5, especially, and a, models, and, and to  science,! question, 1, and 1, 2,  the 2, ", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "Waseemily and 32 for our transfer w-layers, etc./ future of BIB1. 6- in future study B. 80, 2. \n\n- 1, text-based approaches. BIBREF, B REF, etc. BIB-1, 4 BIBref, BIB 1- 3 un- 2.- 2- B- 1, no- BIB- 12, 12, BIB- 17 in future, 12, B large, and, and, 24, 24, 23,", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "The main study, i.e, and N/main: main and  NMT (C): they use the main the language, and a language, and the main R- NMT BIBREFREFREF (e) and the model B, and the main and the standard Chinese  English BREF English, and language NMT, and the models R, and R, and large M., the main RREF., and BREF 2., and 1, and B-M, B-References... (2, C as al ca B I, the in the N,", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "The task at the models' and can not be in the model that the answer for the diversity target, and the \" the diversity of models, the and not form the models, can be \"yes yes, and INLINE B FORM. yes., and in a model INLINE form the diversity, and yes. un-beared of the. to the one of the oracle, and to performance, and-able existing, and un 1, and the overall, and the one, and not, and no. form.. and, One. and, and. and. to., and 2.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "Exquire to the task, which is a pilot work to the concept REF. TABLE, a 5, to show, in a corpus B Bib REF  G is an \"evaluation that (3, a concept-con, another, concept,. assist B, task. \n\n** a concept. answer, \"similar, fine, B B B REF  A B B B B D REF a summary./. B. \"un. a a un....  a concept., a concept. a., \"high. un. The. un.  A B", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "Unanswerable: 9-lary-w.- (un-0 no-layers unidirectional c-2-layers, and the 9-l layers of sMB- (s-2-l- layers un-over, a great improvement  sMB, and 9-grams, and Un-de-l-lables  (9-l, s-mbR, traditional, and 8-2-lower, C no, BMU-2-l- and s-9-l-2, and no 3 C no 1 (unanswer-2 2-l-layers, and 2-l,", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}

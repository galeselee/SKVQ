{"pred": "Fox: (Single: Fox, a set of 300, and, which model, Em. Results: \"Ango-M, we are they are explicitly: http, which are used, and emotion, as emotion, we use, we use, in, we have 1, Figure, we experiment, and, one, and a, from February 200, and the, and the pages as well, and the, and the The, we use, we have, B, to be, and the, as well as, and the, and, and, and the, as well, as well, also, and", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "A series of post intervention. (the context that triggers an instructor to infer the contexts INLINE L. LSTLyes. assistant. L.:L any context series of the context. to the hierarchy of context B LIT...FORM..... to P... e. The context.. LST......... The context.. un... L. The context...", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-score, and F and \"the top 0.: \"G features in addition to the \"sarcasm (positive/ word/ other science, and/ the output of explicit/ inline \"sarc and the: \"un- and 72\". and \"un\". \n\n-... \"  [... (sarc to ... \" 1  on the cr/s 2. Gaze, and the setup/...  un/ [assistant., and the cr/ un and the cr/ ... B and average, and explicit features, and section in the  and the features, and", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The sentence modeling the results of conventional B stacking of lower from using the BIBREF29-1- [ BIBF40,  un N, yes, and, yes, i, and, and, and, 300 of 1, 39,  B, B IB G, B, various, BIB.-REF27, B BIB, 1, and, 2,  BIB, un, 1, 1, 1, 1, 300, 1,, 1, 1, 1, 1,  CAS, ", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Unanswer: yes, yes/jiant: unanswerable. jiant: j√¢assistant from the project of the jantal SLOODREF5, (un-Config) (yes, un-integration of jiant ababil: jiant is 1 BIBREF24.41. un-0: HFA (jiant's:  jiant in BIB D. un-p. no: 2. 0.assistant. BIBREF.0.assistant. j: 0. 0. Un- 2. 1. 1.. E", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Un privacy-Privacy of 4- 10 of  privacy policy, no-Privacy-Q. BIB-49, and  Table: \"no- and it 4-``assistant 5 4, 5. 4-4, \"users, and 4, and  This-`` 1-10 not. ``e-2-1 uncom 4, 4, privacy-4, 4, 4-4, 7, and 4,e 4-2, 2 4, 4, 4, 4, 5, ", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The performance of cwrs in the paper for an ECON at a large on C-1, with the NER, with 95 BREF, and 96, 23, and chunk, 23 for the base, and task, and task, and, with, and, and, 1, 512, and, and, and, 97, 1, 1,  and, and, 97, have, 32,,,, and, 96, 128, B, 512, high, and, pass, and, 1, 32,", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Amazon B  RNN can be used to generalize yes, un- languages BIBREF26, B and, the, general, Table REFREF1, Table BREF7, BIB. 1. data., global, 1, R, 2, BIA,, a, the, translation BIBFULL, 1, BIB, machine, B, BIBS  BIBREF0, 2,  \"unanswer, 5. -\".\"., on, 2, 2, 25, 2, 2.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "The topic- ELMo, ELM-P (7/14.-B1. Our rare-classes for testing like the self-self model B 0. and 0. 12.-b 14., and 37 rare  a 1, 15 sample/region/ and 0. BIB., and 1. 0/ 383. 0., 7.-1, BIBREF27..: 0. 0. 0... 0.. no 0.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "They article un-ASNIBFORM0, and unreview, the, the, and, the I, and the proposed, the the ARE, and the MD, and un, and, the, and, the, and, and, one, I, and, the, and, and, and the, the, and, and, are, and one of  and, and, and, and the, the, and, the, AS, and, un, and, the, and, are, and, and, the, and, the, and, and, and, un, as, and,", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUSTERING0, the 13,  S, S K BIBREF0,, 5, R: \"Car, and in multi, 5, 15, INLINE2, S, P, BIBO, 5, C, C, Table, C, and 2, BIB, 127, 5, 15, yes, \"D\" and, 2, 1, 9, etc. 127, 127, 1, 1, 5, 5, 5, 1, 5, 0, un,", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT-: BIBREF-Bt- BIB-T-28.: BIB-59: \"play the, and 3. 2. We- P-DA. 7. B-b BIB- 7 E and B. 21: no. B- B.. 2-8. BIB. 2. 2. 201. B. The 2 50. B. 2  BERT: 8. 7  P- and 50. 66. 3, B.  21  20.  ", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Pivot-based N$\\rightarrow $target\\text-based and the main phrase in the cross-lingual N$\\rightarrow alone, the shared baselines, the cross- pivot and the universal, the training.-ling NMT BIB$\\ 1, the main NMT\\un, multi the main, the main BIBREF- BIBREFREF32,-1, the main, 2, 1, 102, the main, and, the main, 0, the novel, and the  the main, 2, 0, 2, 2, 102", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Em-language, the model's label as \"denoted as \"1 emotion's F., the proposed BIB in the Emotion F1, the Friends and the pre- the challenge, and our model, the Friends and the, the specific, the model's, the pre-training, and the weights in the corresponding, BIB, and the scenario, and, and, and the, the pre-truning, and, and, and the sub, the, the conversation, the, and, Table, the, the, B, and, the, and, and the, and, B, 1, and,", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The large number of the all- and additional results for the speaker's speech for the experiments on the method for the i-vi- is \"DeepMine\" is to (K., and 190 KTF- and results- and s-speech (i- and 10-sessions of 2-s-p and and the results in BIB.- the i- and in 3.- for the large,  as 25- and 3-s and  a.-.-Mine. the- and- and. The. BIB. 1- and, and ", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gfat-full attention ensembles to a flexible or no, ourben CWS, they is, one, full, and City, the uncharacter, the major, the major full attention model. assistant,, un, as, state, the, the state, our attention, the, the attention, the, no, and, the,, city,, and no,  un, character, 1, 6, 108,  to,  question, and,, and, full, and,  and, 2, 1, 1, 1, ", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The training from the pre-trained models is a single Ramly (i) and use  BIB\" and we, and 25 in a H, the trained, the target language 2 Ramely,  ^ B: un N (un 16. un than, and 1 N, 175,  un, un, un unref 16, 16, 24 0, and 24, $ 14 0 24 16, 14, 16, 1, 24, 1,  2, 2, and 24, 16, ", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "Attentions 2010  unattention  alignment to the average context has also impact 4. rather, attention loss, unanswer to the attention, attention rather not it has the context is not the standard E 17. 1, attention 0, training. a, training, attention to alignment of the attention,  the  attention, ununli-E: 1, 4, this,  attention, 4    attention  the  0, attention 17 4,     1", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "Re INLINE FORM BIBFORM1 is to take(s)-e continue.: RNN.-11. (unpossible un-to-3) and BIBREF in 1) in B 20. 12 1 1 yes\n\n- BIBREF2 BIB REF BIBREF BIBREF (  BIB BIB) 1 BIB form 1) 1) 1, 12 2 2 12. BIB to be  1 40 2 1 2 1 1 12 1 100  the", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "unable and answer \"yes\" {B}Corref. and 64 no in the table NER of 0. Currently: N: Stanford is not present our results in our model, we for the  no pre trained model with 4  E.-K, 4. and 8. 1. $BIL- 0   4 1.  Feld  5  4   N-  1   2 8.  -    8    7   Our   and  no  yes", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Un- tuition is \"un- 4 previous in the New intra- $a_{\\text summarization} of the full model B- $ filter _ N * BIRT$ with copy (s) un-v future, and B UN-  (Equ) text channel single  _ Channel BIB) channel BERT, fine,  3 * ( un-  *  BIB,\" on future Q, * $ and fine future, our model will  B, the 36 *  un  New  ( a channel channel, our approach Q * the- $- 0 un", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Around 50 systems that are an unanswerable 500 workers were INLINEFORM1, and 500 workers, and 500. B. a more, the above 1 500., the best, 500, and 500, and 3 R. 110. of the references 500 500 500 500  of 4 500 500 500, and 500, and 14, and 4 in the INLINE 1, 1,  and 1 1, 500, 500  an 500 500 4, 1 500", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The article's: The following is: The article: The article: an expert, OL L, and an, answer, a, and answer:al, and, and an `in the, and: a, and, and an: a  and, and  a, and, a 0, 0: 1, and inter  1, and 0, and, and 0, and, and  a  and:  0: B: 21, 3, and, 0: 0. 0, 1, and, and 0, and 8", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "BiLIT+NIPRL- and our models like our model like BiNLM- our NLP model with Bi { un- BiL- and Bi-T and other- and the neural score, and the N- and also.-  N- and BiT BIBABIBREF- and to 95- and 50- and other- and IL- and 1- and  and 95- 1- and 0-  and 30- and others B. we the  0.-  and  and  and-  and  and  and  and  and", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Un-si/section. BREF: unsec. and the resulting one one for the word B. BIBREF: unanswer. REFUTREAF: BIBREFs: Un..un2/.: Ununs./ the./ B..., the one...5. Yes........... un.. Un.... Section.....", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERT.-1. The pre-training BERT. \" BIB-PN.-$ BIB. BIB\\W. BIB. BIBREF1. is order- $un. $ and $- BUn.-..- $p.. BIBREF.. Bun-PN. \"un.- and BIB- 0... \"Un. $... $  - un. $ un. $P.  Un. - \" un. B. \"un. \"un. un. $ un. un", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Five models: the key-√¢ in the answer: the key phrase, the performance of, and the key, key, etc. the key of and available across across, of the article., but the long documents and keyphrases. assist, as the the... the article.......  question......", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "No can, not, the main model in translation and, that, is, no, un-encoder-de, the encoder-decod-nun, un-training, the encoder-revector of, per- training. a, un, yes, no, yes, un- training, yes, the encoder-De- 1- an-  N, un, un- 1, no, the, encoder-de, an,  un,,,, un, an,  an,  an, Al, un-  main, an,  un-training, un,  an,  an, ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIM-in MIMREF0 BIBREFREF19 (unformed, the MULCs, MIMR-s and M.- CISTREREF1 and- prior.REF0. FORM. BIBREF2 M. and MEE BREF5. - unanswer. \"un- \"un.... D.-. upper.......-... \"un- and BIB..", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Un:- (an article as 1, 14, 16, and  explain that, the per of each,  the vast,  (1, 3,  and  BIB,  un of:  and    training,  BIBIB, 1, 21, 1,  3,   un,  un,  ( 201, 16,  1,  un 8,  and  c   un,  B I  Table  BIBREF   a  B,,", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Attention of Ns to the attention distribution, other models can 639 and \"ne also and other classes.ous mixed translation. Untraining the guided attention of NMT,, attention model that of the standard... rather, not un a un  global B Zenor of the maximum.-Et-Al-k,  attention-soft- E.-Et al attention. rather, attention is not. Attention is un-E- un- un un un- un-  un-  un  un un-M.-17. The un  a un", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The main question, the weights of the question are \"yes. \n\nun-{\\rm yes, namely, including the six of the six and the un- \nthe and the original, $\\, $\\, the proposed Baser, and, the,  Emotion label, and the  the context, the baselines, and all, Chat-based, the, the Friends, B, 4. 1, and the, the, and, the, and, Text, and, Text, the, the, original, and, 3, and, and, etc. 1, and, 1, ", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency‚Äìinverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The authors', e- e-book.- un., not-also, \"un- E, a round.., e, we, we, and, in the full.., etc., e.., to, the, and, e. We, a, the, and., a. Yes, a, that, that, we, e, a, with, the, the, we, a, and, yes, the, and, and, the, we, we, the,", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The token-to-cadras Bly of CER in low and have a language has to 18 and 11, the model C for  3 co-s 5 the following of the 20 of the large of the highest BIB 3, 11  of  low, have  A 1 1 1  of 11-  of the  5  of the 1   10    of 5 1 10  2   of 1 1 1  10    1 1     ", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The Pyram in's-m: two in: yes, they used: language model, language., language, and language modeling with B., and provide, and, and language, and the state of  the question, and. BIBREF, and 22. BIB, BIB, B, and, and, and, and, and, and language, and, and, and, the, and, and, and, BIB, and, language, and, language, language, and, and, and, and, and, and, and, and, and, and, and", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Unanswer questions, \"INLINE-Figure, and crossling, the clusters, that online uncluster, online, and online, and INLINE-F. unanswer, which significantly, online, online- INLINE, and to, a, the online, and online, and cross. Un. Unassable., Un Un- un-Inline- and un, un, un, un, a bag, Un, Un un, un, yes, Un, Un, Un, Un, and, Un, Un, Un, Un, and, Un, Un, un, and Un, Un, online, Un,", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The vocabulary of the effective in the approach prior to a not possible, the student and teacher and student model training, and student BILBREF8- and above, ofBIBREF, have a, unref: the student model's, student models, and the student model, with different, and student, and student, and, the student, student, student, and, and, and, the student and the, and student models, have, and, the, in the, and, and, and, and, and, and, and, student, the, BIB, to, and, and, and", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article is the authors' 11/ \"!\" to provide \"yes\" and Turkish studies, and 3 no-cular to no, in the Turkish and 3 no, 5. The details\" can' and 3 conc., 200, as  B., and all our approaches, and all of 200, and 1, and  the, which' and a. BIB, and 3, and 3, and  N, and 3, 3, and 3, 3,  B, 4, 3, 3. 3, ", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The article is the question: The article reference: A article in the article come from the source, we do our to the R BIB U B B B BIBREF20,  ancient, and the future, the gener, the new BEMIB J12 in the article 35%  BIBFORM0, 1 0- 1, 1 0-  the  1 28  BIB and 0. 0,  J 1 1 1 0 4 1 1 0, 1 0 0 2, 1 ", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "Un yes:... article, this has the features for Quo, question post question answer quality, the questions (answered), and, no, and, \"un\" the, unanswer,, \", no of quality,. \"answer quality,, are\"., unanswered,,, and,.,,,, or, un,. are,, unanswer, answer, question, introduction, etc.,,,, un,,,,,., unanswer, of Q,, un,, un, and, quality, a, and", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\" Article: \n\n Article: article article, un-battery, F in the de the de, \n[leftmargin,... \n\nFeatures: \n\n Main- \n H.- I: Ab reference, \n  Ab-L, \n\n \n\n, The, 20, and to, News, \n\n,, \n\n Neural, $- \n\n 0,  \n\n \n\n \n\n \n\n I:  A, 840,... 300, 5, 2, 5,  \n\n  Yes, I, ... yes,,  A,, ... ... :  \n\n", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The single-ling: form fine- RAMEF, we also  BIB/ can be XENREF to XN, and  BIB and 12 large, and  TAB 100^{, and the  yes,  top  BIB BIB N2, and  no 32, Chinese, 16, and 1,  B BIB REF. BIB BIBREF27, and then  B B  the top, and    large, and  BIB B B B  un,  N (2  B 32  and  Un assistant. (SEC", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "unanswerable: \"unanswer the question: unanswerable by S: un- $ $ $ $ $ n- $ $ $ $ $ ref $ $ $ $ $ $ $ d $ $ $ $ $ $ $ $ ** $ $ $ $ $ $ n- $ $  answer $ $ $  $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ no $ $  $ to $ $ $ $ $ $ $ $ $ $ $ $ $ $ to $ $ $ $ $  $ $ $ $ $  $ $  $ $ $ $", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "No answer level-PT has the question question: A Bas-S1 L2 BIB2, 1 C2, BIBREF P BIB5 \"un-B\", N question, no no, C; no, no, and C1, BIB- A1, and, no, and, no, and the answer, no, and 1  \"un\" and., C.,  no, and 3 L1, C, C, C2, C2 LIB2 1, C- 1, 1, 1, 2, C 1", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The CTR-REF:-C ::: 4, 45,000 with BIBREF5, used, an answer: a: natural,2 2, 2, BIBREF:, and ibser, BIBREF4, 45, etc., 4,  CORD,  4, 2, 5, 3,   etc, 4,  4, 4, 45, 4, 4, 4, 4, 5, etc, 4, 4", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Traditional work models like the self BIBREF {i/6. BIBREF17, BIBREF16, and online of the one are BIBREF26 BIBAnswer- and the results, BIBREF6, BIBREF14, BIBREF6-6 and random ELMREF: BIBREF6: yes: \"we 0. ELMREF0\" and BIB-1: \"un, and 12 BIBREF6, display 0.5, BIL, and, and  BIB- 0.3896, random- \"un and  the", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unarast of our analysis: REFREF assistant: \n\na T-den. BIBREF0:.assistant: improvement: en arfeld BIBassistant... en. Un- etc. REF.  BIBREF1 BIB.. assistant.... an. B.... of H.. of. en. en. BIB REFREF1..... Improvement of. Un.- 10.. of B.. Un-1. Un. ", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Macawaw [left-answered question, and the un. unanswer:, [ question B. Question, BIB, etc. Macaw has, Macaw can, and BIB, the following, \"un, un,  yes, the authors  a,  Bib, Kris, \"un, \" question, B.,  |, the, \"un, \"no.  Future  \"Macaw.  Macaw, \"un,  \"Un,  \"un, \"B\"  \"unanswer, B  B.,  \"Macaw, \" ", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "unanswer: Unanswer:.: yes, the N/N.: unform0.:. form...: 4......., can................................", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unanswerable: un- and then-sky errors have and large, 300 R of the background, the results for official 4- and large  mine, 300 hours of the Deep K-M and  i. Mine Deep-Mine, RedD, and 3. \n\n, and in the background in the BIB.:, un 20, the Deep, and  the last of the speech, 114- speakers, the full, the 1.. the work, the 1-s and 1. 3 as 1- 3 of ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The article (our Open and Table TABREF27 system, the open and the results, our unref and the question and table on the SQu B BLEu our proposed, BREF2, all Open BHE, all, and B un and the question: BIB, the results, our model, the BIB, the model, Leases, B, BIB, BIB, BIB B, B, BIB, our model, our re, B, B, un our, B, B, our: B, BIB, our, B, B, B, B, un, our,", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The results on the, and \"un C\" B-CER14: ER-C Sem REF to 50, 20114:  CEST 2010 ERREF-Cell, BREFREF0  and ... BPECT  \"m, 8\" and, they, BREF2 B BREF3.  un ZREF-  BREF,  BREF 2. T  ER, 201, 8 B  and   B, BIBREF BIB  C  un, 8  4  B    4 B  201,  ", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The article's article, `except language, NLP, the ELMo model dis not for ELM, N, N: \"the is not to `post `small' the is, see,... BREF `REF, N, NLP,...\"., gold, see, NPL,... N by `gold' N `not to BIBREF `to N ` the number, N answer, NLP, N `not, N NLP, N, N, N, \"N `N ` N ` N ` `N, N `b `N, N `s, N,", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The article (TABREF_REF: \"mixture of word, and also presents the energy of the KL D and its expected  co form  neighbors, the Y$\\operator { } and the upper and upper and also $ the Y (multi G$\\operator $yes and the the Y\\ne $x of, and the objective of learning to (D B$\\ `{ and the proposed objective B$\\_ $ of, word model, and $ $ $ $\\operator$ and not$\\_ $j$ $x, $ and, for the and the Y$\\operator$ $\\models G$\\_**, and $BIB$", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The proposed method is a-tt F1 improves to 1. (2 being 84.1. For test.: yes: the 1...: A-tan., 3. A A to 0. BIB. is. is BREF1... is not 1. the A: The cross-two A S. Examples, the can ly 1. A. for T A gamma.. Ex. A. A. 1.  A. Un- O... B. 0.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "unBREF (BIBREF12 is to unanswerable, unanswer as the joint distribution of the parameters to be BREF. (un  and label in BIB ) un-1-FRA  un- BIB. 1.. BIB.  unconfure 1  unanswer as the samples un un  un  un-A  unanswer, a  out. 2  E  un.  W .  ( un  un  un 1   B  un. 12  un. 12   a  ", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The article description. 1: vector. is a single concisionally the article, as one, and character-  ab. MMorph... morph- 50, back., character reinflection, MS.√¢. and, and the 2. \n\nArticle, 50 B 1. for 50, 1,  ab.  one., 1, 50, 2, 50, and,  un, 1, 1. 1. 1, 49. ", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Natural reading Bref: B/ question:... [unparalleled capabilities in the analysis to a of attention,.........,...... agent, the following documents, we describe: ... ...... we present, a representation of the  B...... ... 8. assistant. are a set of candidates, we present, all the first, B.........,..................,............... ... ......... B... 20.,... B, question,...,...,... yes,............... a", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes, privacy-based, not \"CNN, and 3, privacy, and 3, and  BIB-E `.- sentence, the author, and 3, and 3, etc,  have, and 3, and un, and  question, 3, and others, and 2 BIB-1-1, and specific, and, and privacy, and 2, and 3, and, and 10. and  and, privacy B, and, and B, un, and 175, and B, and 2, and 7, and 2, 2", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "53. The corpus is character, our/-the- question and 53., the number of 53 of 53, unno-th \"un-anctors\" 53, provide the article 53, 53, 53. 52, an no., 53 no- (character 1, 55, 53, 53., hyper, 53.  un, 0, 53, 100, 2 3, an, un, un.,, 10, 16, 2, 0, 3, 10", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food:ie, \"unrecopy, and 180K main: from the article, food.com.:: and 1, and 2 model from 310+ g BIB [ungh: $\\math,2... BIB answer:..., BIB-Pe.... BIE BPE 180 BIB...... recipe......:............ [exp.......................................k..........:...", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "They perform a word-level w.rer. {yesrefwork:  un- $ $ $ $ 2 using: $, $ $ $ $ $ $ $ $ $ 2, target, and target $  $.  $- $ $ factor- $ $  $ $ $  $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $  $  $ $  $ $ $ $ $ $  $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ un $  $  $ $ $ $ $ $ $ $  to $ $ $", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unanswerable BIO operators, B yes, \"yes to access the, patient operators, unanswer: unstructured E \"un BIB, BIBassistant, E B... for, 1, 15, and BIC 1, 1, had, \"yes,  B1, B B B, 1 - \"unanswer, \"un\", 1, 3, Table, 3, and 3, 1, 3, 0, 1 Lab, 1, 3, 3, 15, 3, 3,", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Most L1 of Cula- C-23, 1, and 3, \"un-SFRPL2, and 2, 1, C of 148, and L2, and 2- C23. assistant 21-1, 21, 1, -24:-3, 24- and 24, C-2, 70- 24- 24, 24, 24-3, 21, 24, NLI, C2, 23-24, 1, 1, 3, 2", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "Patterns for correct errors (C) of section, error can error, (training to error, generated known to correct, R. BIBREF0, Rill B, BIBREF. \nusing, a learning the errors. and 35......... correction...  Yes. no. BIBREF....., use.... No... Un. 1.. Un.... no.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "Unanswerable.- and: the phrase. The unproverb, the assumption is a large large methods, and large, in the number of the baseline they learn large. Un-`` yes: a large, and a context. Unphrase.- 1  a: the 1, and the methods that are not  in the  Un- that the methods also  a unit, and the number of INLINEFORM2.  as the number of the large tuning a large, a L, with a  BIB  the functions  and a a, and in a  un ", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The article REFREF21, \"RREF29,... (R:... (backgroundly DOD:ubto the RREF BREF2, TABREF21, BIB: y, BIBFL, Bref... ( 22: Yes/ R, BIBREF3, is shown in HOD, B,... BIBREFREF2, the BIB, B, ... ...  Background, ...  ( B. ... ... ... ... ... BKS, ... ...  BIB,... 2,... ... ... ... ", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "BIBU REF-28-1 ab-subparing-3, the  and we apply layer to  to the bi- 2 to- 1, they models on the 1, and 2, 1, we the \" in BIB-R, 32 1, 8,  B 1,, we  (un,,  B,  BIB,, 8 28, 8, 8, 8, 8, 1 9, 1, 1, 2, 8, 1, 8, ", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The article, training system, the added error generation of patterns, and the results, and, answer, answer, the language generation, the results, general, our results  and the system, shows that AEG, using, the language, language, are also, in, the model, the language, answer, the, the, II, general, and, II, shows, the, learn, error, and the error, the, error, II, using, the, error, a, our, show,  and, and, un, un, and, and, 4, and, the, the,", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "Named, phrase: law, answer:-neally,2-2- un-m (to), \", N-pecific, and less strength, and, yes, the social N- words, and the social media, has can be., and- the space, and the social, and the, and the models.- \", and no language, and the, NLP, and language, the, N- R, and the state, and the space, text, with, we, and the, with the, and, and the", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Seven-aid: 4 of the questions, BIB-B: 4-2-5-8-1, that e-Q-2- the performance on a question, and 4-Q-4-4 E- BIB-A, BIB-A- performance, BIB-A: in.-T-2- (Privacy- a B- answer-8-2- P-2, B4, and 8, 4-, B. 4, un, un-1, un-, including, to, and B-4-1-4-2- B-BIB", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Maximum studies recently on existing features are model including N- 95-1. (1) maximum N-gram model, maximum- and 95 (BIB:  and n- so, machine line, and so, N- 17- Mark, and so on- 3- B-I, and- 1, B REF B-  and so, 1-  and 20- 1 B- 20- 15. and 1 2  B 1- 1-  B-I 1- 3 11, 1  and 1, 1-", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stan yes and yes Stanfordb: ’∏’û: ‘±’é’•’∏ ‘±’¥ ’∂’Ø’°’≤’Ø’∏’¨’Ø’Æ’é’´’ê’û ’é’∏ ‘± ’ï’∫’°ÔøΩ’∞’¥ÔøΩ’é’∏’í ’Ñ’∏’Ø ’Ø ‘≥’•’ì’ó ’Ä ’è’Ä ’Ç ’∫’í ’ï’ë’∏ ’¥’Ñ’∏ ", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN1  and Table 1 0  knowledge words 0$0, $\\text {un-2. 1, 26, and $e= FBIB,1, 1, 1, 1, and $J 1 1 1, 1 1, 1, 2, BIBREF, 2, 1 0, BIBREF2, 1, 1, 2 1, 0,  0, 0 0, 1 2, 1, 0,", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "Supporti, yes  a un- Harful un- noxious, yes, \"un- Har- and, \"un-able, there, and no, have, the highest, 1,  R, to C.2. B- BIB. Future, the 1., I, 1. The, to,.. 1, 2, a, 2, to, 1,  a, [per, 2, 1.-.,  no, 1.  R, 2, 2, 3  a", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "Three high past of AQA for the definition of B: no of CRL not for cyberbullying, and not about, and, and cyberbullying, and A: unanswer, un, and no. word, but. assistant, A, and, for, and, and 1, ungr., and, and, for, and at, as, and, unanswer: no, and, and, no, and., and, G=, and, P, and, B, and, and, and, \"yes, no, the, and, and,", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Yes no, yes, and \"w, un yes, and \"wonderable.= un/yes, no, etc.assistant, novel:, the, detection, emotion., ISIS, no. no, etc. (`` BIBREF24., of, etc., etc., yes, no, etc, no, yes, un, no, etc, un, no, yes, etc, etc, No, etc, etc, no, the, etc, BIB, etc, etc, w, etc, etc, etc, etc, BIBref", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "An un-9- B= and feature REREF17 BIB9.-9, 9, an- 9, BIB√¢, 9, 9 B= An existing, a 9, 1, 9, 1=5 BIB, 3- BIB, F, B, BREF, 1, 2, B, 2,  2, 9, 1, 9, 1, 9, 1 1, 1, 1, 1, 1, 9, 9,  9", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The N/AID is an N/LID natural approach for machine language group of which an 11, which  South African, un 11  1 BREF, as 235 11. /AID, 11 official, BIBREF and 95, and 100 8 2 11, 8 8  N  un  11  machine 1 1.   2  5  2 2. 11  10   1  8  and 2   and 1 1 10 ", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IM \"unbreak (in no, unanswerable, in this work, to 0: BREF4, 4. BIB, the, BREF, data, in 4, 8, 3, 2, 3, 1. 32, 0, un\". 3, 4, 3, 25, 1, 3, 32 4, 25, 47, 47, 3, 4, 3, 3 B, un, 3,  B, 3, 3, 5,", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The article had: yes/online.-funestar: \"Is\" and the 1, free-, 1, 2. 2%, 1, un-2, 1,  \"the free, 1, a, (the, 1 BIB BIBREF1, 1, 2, 1.-1, 1,  \"throw, 2, and 21. 3, 1, 2, 1 1, 2, 3, B, 1 BIB REF, 1, 1, ", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K BIBREF1,REF0, Twitter et, KIB], the event, and the event distribution, and the phrase, and location, location, location, and the event distribution, date] named. extraction.8, and each event, and, and the event, date, the corresponding, and the event, and the so, and, the event, and, the `disc, AEM BIBa, and A, D, and D, and the events, and events, and the event, and, and event, and events, and, and the number of events,", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English (yes, the PolyResponse: The)yes, and, the current text, the article, 8- and the current (BREF16 (h, and the current) [see  the current, no com, and the large- or future, Future Work, Future, BRF, and 1, and  Un, answer, and, and future, and, and the 8, and, and the current, and 100, 1, 1, and the, 1, and, and, and, the, and, and, and, and, 1,", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The similarity of 5-1 (t) isunanswer  INLINE uncluster for the 1 and 1 that a 5. Unbag, and 77, unanswer and  yes and 1, and yes, unstream, the  unun- and  un- 5, and 1 news  yes 1, 2, and 1, un 77, 5, un  a un, and 77, and 77, and 5, online. The 2, un 2, 1, un 1, un  online. Un, ", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Unanswerly, Bulet, machine language,  article, un/ref BIBREF: un-hest, the class BREF: \"un\" existing  B2, \" un/BIRREF: \" used  \"un,  Un: 5- no, N, 2,  \"Un, 1, \"BREF,, un,, 1 to\",, \" yes,, BIB  BREF: 9. \"2,  \"2. \n\n2, 1, 2, 2, 1, BREF: 1, 2, 2,", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "In-f-Sphere, we Inert-f-fine-t-fine-tune BIB-ref- In average 2-2, and the average ERT, and, all- 49, SB-1, and 3, 3-uns to- BIB-ref 12 12-tu-h-f-p-g s to an 3, 49- 49- Un- 2, 4 BIB 3- 2-g 5 2- 5, 3. 4- 4- 4 B 3 12 12 1.-f-4,", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The question can answer: T-SBERT, the average B/cos-ta the average performance of BIBL L REF, and the individual sentence embeddings, seven of the following 2, the results, and the average sentence embeddings from a few sentence embeddings, and a B REFs, and 2-5 T- and and 2, and  a, and on T T5, and a, a large, and 2- 2- T, and 2, 2- 2-2, and 2. a. 2- the BERT, and 7- 7-", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "un: article: we can be un answer of unanswer to the number of 11 e: unantuple: yes, the answer E: unanswer BIB to utilize and to the book, and the corresponding BUN... to as as used an vla-hrich and the combination of BIBREF0.- and also the title, the recommendation performance. and the un answer to B. The number of. can, the unanswer un.: H, the authors answer.,, and the H. the.., and..", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Context: \"un-able H et. unanswer: unanswer, and Discussion of the major, and user, and user, character, and 34,-  the  Facebook, etc.,  and 70-  un- no, and  the best-  and   and 1,, and  etc,   and     201 0,  102,     and   and  102,  and  ,  ,   and         and", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes, \"unanswer of the model which can be used to learn, the article BIBFREF, to learn, un, and, and, the user, can be, yes, unsharable, and, is, a LST,, and, to, \"a, and, and, un,, to, the, off, to,, and, and, un, and, a, and, and, and, BIB, and, and, to, un, and, and, un, can, and, and, and, and, a, and, and, and, and", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Unform form 21/ no form ... unolref  0. 0  unanswer  No, yes, unol  Unform of  1 50, 21.  Yes, 1 U/ 0 0 0   no  1  0.   0  no  50  no  1 1 0 0 1 0 0 0 1  0 0 0 0    B 0 0 0 0  0  0 0 0", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The question Q: REF9: BIBREF12 (Table: 10. Enline: r2 (i ensemble and CER: r10, en leading the last) j. BIBRef2, r10, r10, F1, i: 10: 2: 3: BIB REF12: 12, 10, 10:10, 3:10:2:, (10:5: 201:4: 1:, TAB: 10: 2, 19: 9:: 10: 4: 10: ", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The article:√¢di: BIB√¢ $BIBR $19.2:  (DT2: DRS + 1: the 18  1 0  2: 2,  BIBREF0.: 14 0, 20 0. BIBREF18  BIBREF1. 2  (DTul BIBREF0. BIBREFs 0 0, 1 2. 1 0 0 BIB  I. BIB $B 0 BIB 14  $BREF1. D ", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The article: Tweet-0 of the answers were not available in this article, and have INLINEFORM1 of the questions in 4.- B/ 1 BIC. The answers. B. yes of the questions, but also of the example of  larger a of the questions. The quality of the questions.  B of the questions.. of the article. the questions. (un-. All of questions B- and all of questions-. Yes, the example of  (un, no. of the answers.- our.. as,  the.  no,", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "K REFREF7: Can the car-s-speak is a \"cars' answer on car-sell' REFREF7: yes, no, BREFREF, and yes, ` $ I'  $  √¢: \"yes' and'  a model is,  to'  11,     \"unanswer: $  \"un:  \"assistant:  \"yes, no,  yes,  \"cars,     unanswer  car-s  \"yes,  \"re, car-s  $  $  $ $ $ $ $ $ $ $", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "They can not convexinally BIBREF-COS-yes of 2-CR. REF: \"they:...als BASSARREFREF3-4...-Sassistant (zREF5 assistant6 BREF5 BICREF0, BRSREFRE  (4, 2 RRE  un impact 1, B-bref.- unanswer (B2) and un training B-L4- (un A.-2. and B4, 2,  un. B 2", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\":: Multi-task: F: Binary: multi-F  Bref: and 3. unach: un TSLG-F: BERT. F . and feature is rref. En- and not, en- of BREF6. We. Bref. \n\n and. un-<..- B.. document. \n\n 3..- 1.. En NER. 2. \n\n  e: 19.-  BERT... Em.-.. Multi...", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The sentence in the CRI-19, BIBREF: COVID-4: COVID-20,000- TIBREFREF\n\n (university, yes. (unanswerable,  (4) CORD. B- 17, 35 CORD. (AFF-4, etc. are C-19.. 4.., 5, 3. 4- 4-7.. yes.. B-17,,, C-5. etc.. 4, 4. 4.. 16. 4", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The question (Q is not this sentence, an important class (strong) or 15, and 5, S,  BIB2, the STO, the results. P.- not one: 15, yes, as the number of P, \"skill, 2, etc.2, the phrases, and 7, other etc. P 127, (, etc. etc. effective, the., in the number, etc. Car, 127,  etc. Car list, etc. (Table) etc., etc....", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The article provides: \"yes/ and Var. and model for grammar BIBFORM0 B  not hold level, B_ 1. 1 and  not un! BIB. 4. BIBREF0. not all., and can  BIBREF.. and 1.  B. and B B.  BIB.. BIC. B_... BIBREF1. BIBREF_1 BIB.. BIB: BIB. B  BIBREF... BIBREF. BIB. 5.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The RNN (handle) and to the back and a one character-level, a backoff: common, the UN-level, and (i) providing an back, word, and the #, to a back, a BSc R and, a 4...)., and, a back, and the #, and the back, B. and character, and the B, and, and, and the background, the, and the  handle, and, and the, and pass, and, BIB, B, and the, and, a two, and, and, and, B, a, and", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The NUS- is trained dialogue labels in the N yes that E FORM  of the yes, a \"NUS\" the number of the one form to error speech area in the N-ref- the table that a and the N that the NUS Study E for the D is the features to be faster of the N the NUS that was the DST to be (N, D) Dialogue- that in the D of  | DST. (not un- the N, BIB REF and a D - training the \" N to a D the NUS is BIB REFFORM the. N-S (un (n", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "BLEMI/Un that stay:  the attention context have the attention of the 3% and the one the field level one a  bi- a one the 21 phrases G language is Bbiste the learn the the  the top 70  BIBREF 21% the total description. the model  B/M/L the stay that BIBREF the 297 are not fewer 21  BREF2 the 21  GIST the 400 words  B BIB bI  the 27 conc 21 20 27  the  the basic 21 22 the 21 ", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "r BIBREF1 (a) r BIBREF: \"r of the proposed state-of-journal, law-mentioned that the proposed to all, and re of the state, and BIB exists, r display with law.-1, a BIB.- BIB. a single, and r can, and a, has, BIBREF, BIB. a law, B, and \"un, un, and the, the a, B, no, our, and, a law, B, a, a, un, un- and law, a. better, a,", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "The article's recovery results for MSA (a, which is 9. WER $\\ Verb:  $ $yes $ $... \n\n. a diat. and word Y, 3, for the word, and M. our system, for M.-ut. and 12. for the, and  5, and Y. Yes. 3.., lower. 8, 5, BIB.. a,  and  B.. 4.... ...  B. . 5. The,  for the,", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "Along to a word, \"do our approach to pose the co- concept, the learned-  an average- words not, JLI BIL. BLMV, BIBREFREF. BIBW, 1, BIBREF1, BIBI to B I, BIBI, 20, yes, online- yes, B I, for an, BIB 16, I, J, BIB  to the, BIB  B, B, V, J, B  also, to, but, BIB  our, B, un, B 2, B  I", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "English- and- \n\n the in BIBREF and  BIB- the GAN BIB, and it B- yes- \"un as: form to un N, the answer form B- and the G form of not B: B- B,  in the results in the back G- BPE, but yes, un B B, and, also, building the G B form B- of the properties form, which form BIB form form and un form the  ( W- B- the, e- and the \"G- un all B- un answer, the N- the, all G", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "Unanswerable: Un the main of unseen NMTs: \"unp is the effective in all NMT to translate in many-to, \"Unlig, no\" not able to a fully:, integration BIBREF. BIB BIBforman, the context,  Le: details, in B. BIBREF BIBFORM, and B, B beam BIBIBREF B. Do. The un E, BIBFORM, our, a, A, B Inline, BIBFORM0, B, B, B, B, B, B, BIB, B, B,", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "Disinforming yes, yes, no, mainstream and high, mainstream features of mainstream and disinformation and mainstream, both, and news in mainstreaming news (un\"main and dis,, and mainstream news, qual to be, and mainstream, the, and excluding, United, mainstream, to 2., we have..., which two,  and, 2 (and, and dis. The, \"MIBREF0, and, 6, a, (no, 2, 6,  BIBREF, 201, 6, 6, and ", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "Coin-ColbBIBE {  yes| yes higher, the  yes {< yes, and the set of, BIBBIBREF36, etc. Our BIBR { 30, the model has an a fixed {and, and, | |, 2, BIB2, BIB. BIB., BIB. BIB, BIB, BIB {<  A, 6, BIB, BIB. B BIB,  BIB. BIB { K BIB, B. BIB.  BIB, BIB., ", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "F INLINEa 2 is the model key-phrase generation on a given to be a 2 F 0 (the present form, models, F 50 and Eq form Table, and to, can be used, Table, BIBFORM1, Table: 1, the key form 1. H form form, B., generation, Table, B TABFORM, 1, Table, 50, 50 form, 5, and the target, models 1, the model, to be 1. The 5, 2, Table 24. 1, 16,,", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The Badjatrous- 12- transformative in BIBREF-uns- 12- 12- 24- 23-uns-  n- n- neural token 4- 1-  the model to capture- n- 12-  un- 12- 12- 12- an- 6, BIB- 12- 12- 1- 12- 24- 24- 3- 24- 12- 23-  etc. Bref- BERT et- 11- 11- 24-25- ", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "\"Our best-work, to generate the attention mechanism to a) by a) further a) to generate a model-guided, by recent, and to: \"√¢ characters a) constrain, \" constraint to prevent the standard- [duality  (e i, un to a, and the  constrain (a)  e, a, in a more complex, and,  a D  (un,... and to, e, to, a,... standard, a a,...  the, and, and, e,..., and ... a ... ... a, to, and,", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "The additional cost-2: BIBREF45 BIBREFREF0: (un-sense, which the corresponding, we develop to the presented, which (large, along, BIBanswer B-Q in [REF4 BI-SASE. ( B-answer: BIBASE, resulted, and the corresponding to B- yes, can provide, (1, the embedding,  the (I) BIBV I, B I: BIB B: ( BIBREF4.- ( and BIB-a. ( BIB, BIB BIB, an O, I, ourI BIB-", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "Un able to be explained. \n\n`` (un: our question of an, 5.  (1, 0 (2001) to which the results are to Wikipedia BIBREF7, 2 (not news BIBFORM... 73% of an entities BIBFORM8 BIBF1, yes (yes,  un  around to the year of an entity, the section A, year of section section BF. to an, 72, the  (K, long entity, the Z. 27, to, 27 to 69 to the of the of 73. The", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "It is novel, the results of the question, which are available to provide a one \"un with the better-1-\"., and an example to the article, and the answer to the one, lead, the one is BIB, unanswer, and the output, and, and ab,, 2 one (yes,  the state, the article, a answer, are,., the default,  \\, and a., the, 2., I a, a, and., 2,  the one, 1, BIB- 2", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "Their Jacobian of the ent! The background of the ent-entmax, and the $\\! 1 r. \n\n (and concrete, the model, improvement, a r! un r. UNK, general: learning r!  higher, and B!! J of all, the  higher sp., and the, for training, the one $!! of!!!! the models., using the sp! of, and the automatic, the.! per, higher. J! B! Special J. for.! 1., higher, and the.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "Unan (the respect of the question, \"yes, the R.  unanswer the model's 5 BRL et al et. the morphel is the corresponding to the morphological given in the Fig.  the alignment and the 41, the Tamil, et the R morph and the 41, the 0 that the  un, the the the morph., the, the, and the Ranna et, the the the RNN N. the 1, the R 2 41 R, a  the  the  the  the, the, the  the 30,  the", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "un and the do not a certain: 14, a  i-p, \"un: a one of social media, and a number of each, and 22, and the U, 1 and 4, and a number, a of  a  (field to a  22, and 22, 1, a one, 20, 2, 3, a number, \"un, and 20, and  and a, and the,  one, 1,,  of   a,  un,, a, and 90,  and", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "Un answer that can be: 3, the task is an agreement between performance BIBREF, but not the article as concis., B, sentences and the results, and the crowd  Is, crowd, effort. of expert quality, B, and  Is, B,    B, as, 1, that \"un answerable. The last, and, the. \n\nIn, can rely on the crowd, and, yes, a, and, no, and, \"no, B, instances.- can, the, the, Can the, the. (sentence, Can, un", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The \"more than a phrase: \"uncon: lisse (un Com goes, but, the available labels (e, and change, personal, and the general, fine Clos, and Con, and, similar, and, un Con un the number Brien, BIBREF,  BIBREFREF17, and 50, Biam, L, B, B, fine to the  (un, and CRAFT B: un, ( the pre, C, but  B, 3, and 3, 3, BIB, and,  B,  C,  B", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The question \"unformalize- and attention case case, a fact descriptions, fact in INLINEFORM0..: all are abrefines law, r, and law, BIB (un all names, and the attention law articles, and B. A., and the pre. A.assistant, r. BIB. BIBREF. answer, and the dual, 1, and law. The one, and the prior-. A, BIBREF1, and the law Judge, and the.. BIB. B.. A judgment. j.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "They model performance is trained yes- (and no yes.- unacceptably (un unilingual error/ the model to a mon- un; the size: human-to- (M) to-; not. The model is 3, the J the authors and using the main-c, style, J BIX ‚Äúe (not 2) for errors, the (e- (and, e also the results on 1 (when a, and the, in the (e- the same as, and) the all) B on 3 (un the the a solid, I (and I are,", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "The \"sensualize this method to generate sensational and a one to generate large/AR, our model is un-tube- the generated to generate a large, the one. is 35, the model, and the \"s not allowed, the, a, the, used to, Bie the n-s, the, the flu, the, the, the, our loss, the l- the best, the, our large, to, the, the, the, the best, the, the, the, the,  the, the, the, the,  the, the, the, the,", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "A general of 50 types of the morph of the R unable, 30, the target language, Rvett collated that anaph-50,   2. The, a 50 of  a 50  of the R, 50, the R, the morph and a morphical,, the R. 5, the  R. 50, the R, and the 3 Dr. 50 1, R, R. 0, 3, 5,  no. 50, 569, 2, 2 2, 5, ", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "They provide quality‚Äìquality assessment in the textual features for a document quality, and the number of quality to B, and use the number of the criteria of the number of 500, and a, and to the quality to the, and the two. The performance to quality. and the. $LIM., and the performance, and the, and  the number of the, to the last, the, the features, the, the, and the, B, the, and the two, 1, and the, B. 3, and the, and. to, and the, the", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "The low-translation quality for Ja‚Äì inline‚Äìunlingaz:  B and VII (c1), Table TAB TABLE-B and I, the given models in M2r. and the multomas B D I.., and the reason, back-on. https, Ru monum, large, and 0 O. yes, VII.√¢.assistant, and  Bref. and. (a. I. B and the M.. to create,  b and VII. un- N&#. yes,  (unanswer, and un, and there, and, the I,", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The problem of which are: \"G: network features, the Top: (network, which K- and the number of interactions, and, and global in a single, we, and as a, and the volume, and size class of mainstream, the global network features, we also, we can (Global, and, 4, the 9, and, and, and a, and 1 and 1, and  a, top,, and network, and,  and, B 9,  and 6,  and 10, and 1, and 2, ", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "The number of the training on the 10- (sheter 10 and 128 log)  (the last, number of the 10-1, 10- 10, 1 and  yes and 0  the, and 2-  (un- 2 40, 10 and 6, 1- 1,  ( entity, 1- 2, 9, 3 de-- 1- 2- 1- 50-  B  de- 1 medication,  higher, 1- 128-  and- ", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "Improved KB-√¢ (KB-**/$... (e $r $ $   (the question: the entity-linker $- $ ...... $ e $ $ $ $- $ entity $- $ $\\math $ e $ $  $r $ r $... (R: $ w $ $ $BIBLST $ r $ r $ $ $ $ $ $M$ $  $EL $ the $... $^ BIBREF: r $... query 20, the KB $ $... (R-Q $ 1 $ 1: $, and $d: $, 2", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The algorithm to the unib- the answer: \"the c-‚Äú- the $-‚Äù- ‚Äúreward‚Äù- the algorithm in the way, to- reward BIBGRAS $un. The connections, and-‚Äúalgorithm B (e--‚Äù- $\\- B- |‚Äù and- the un- a- and-‚ÄúBREF- to-‚Äù-‚Äú-‚Äù-‚Äù - BIBREF-‚Äù-‚Äú‚Äù BIB- and is-‚Äù-‚Äùun- the-‚Äù- ‚Äú-‚Äù BIB- and- ‚Äú-‚Äù BIBREFREF-‚Äù-‚Äù", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "Bioasme- (FIG. The 10-2. (un 2. For List-As a '...-2' (unanswer as a 'A'- in the Bio-Q. They 3-2-2 (un-1, 2. (un-2  (and 4-1) (unanswer j is a- -2- not-j- (un√¢-  B.-2-2- 54 (un- 3. (un-  the. 1- 1- 0- 3) 4. 4.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "The article of the Serra (KAI BIBREF20 of which), the used, query, does not to the results of 2012 of the cut, all of Sera (S) has the results of Sera ( INLINEFORMassistant (http. SER).., query, the SERKA and N, and the T, the results for SREFREF2, S, query, query, and, the S, and the results.GIREF1, inline, and the n, un, ac, and a, and the, and N- for, fine, and", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The main model is yes to the words for NREF5., and the Clos, BIBIT, and the, and un-architecture (Section4 results on N/A/yes do the importance of the important in the main, the main, is \"4, to, and, and Black-Box, BIB and, and language, respectively, a main- importance have, Chinese, have, the main., Chinese- and the English of the, and the result also. B&#GR, B. Important, language, and 1, and, B.,, attention, and, the", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "The sharp, BIBFORM12S, emotion, \"n (n-fold yes, the best (n- \"n\" feature BIBREF6, existing: n-  \"n- the\", n- (un\"un\"n\", (n, n- and 1, \"yes \"n, \"un- \"un\"...n- \"n, \"n- (n, no, n- n- (yes, but \"un, \"n, yes, \"un\" un- n, \"un, \"n, yes, \"n, \"n, yes, n- \"n", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "The proposed to data learning results (yes, and on,  the top-5 for each, BIBREF (yes, 2, and) a very- B, and 1  of the article) can be unifying a, the parameter general, and the model, the results, and learning the weights, respectively, and, the, respectively, and, the ab REF6, B, the, and, the, a, and, the algorithm, the  the 2, the B, and, the algorithm, the present  the, and  the  and  and  the 2", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "Multiple models can not have a specific place in the given Word of our models to models, to the 1) science (x2) 1/3, the 1) model's global,  models. 1/ 1 1. (dutaine models.! 2)  for 3 2  the models, and a 1 2 3/ 1 1) 2 1) a model  more 3 1 2) MC 2)  MC 2/ 1 2 2 1 2 2 2 ", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "Waseemilya- n. (n) uncl. BIB 24, future 23 n.  tweets 2.- 12 e. (normal) 2. (B.y. the. 2. 81  (r. 23. 24. 2.   24 2. 24   24.  3. 24. 24. 4  24.  En 24 24. 2. 2 7. 3 4 24. 24. 24 ", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "The results are large, the following, and all, and BIBREFREF7, and do we analyze the main, English and the main main, and the effect of so an 1, and 2, and the main, and the main 6, and the model (H, and NMT. Fair, and the general, and  the, alv. Main, main and all to be and the main, and $, Al, the, Chinese, and, and 35, and. main, have the main. B, and the main, and  and, ca, and, main, and", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "The answer-based on BIB units, we provide the ability to on.forman2, yes. Yes: diverse, and the numbers are not, diverse, the model in one, and no, the best. MechanA, models, and the numbers, the, and the numbers, and existing, the model, in Table, the models, are 50, and, 50, and 50, 0, and 2, and 64, the 0, and, and the 50, 50, 2, 50, 50, 50, 2, 50, ", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "Exit, a concept- and the concept-... (H. P. (1-) and result,... (un, unanswer, we have been, \"concept- \n\n Bassistant: BREFREFREF2. \n\n assistant, we also... and, \"P, the corpus,... GREFassistant,- and, \"... (unum, un, un, and, can, end, \" 1, un, and,...&#, to,  \"A,, ...  \"- a, and, and, and, and, A concept, (", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "Unanswerable: knowledge transfer learning of LSTM and transfer learning with sMB and s: A one A no, and Amap, and future training, 2-layers, and they are no, and un-directional learning, no, and E-l, and no-lBR, in conventional layer, no, connection, knowledge of s, and, and attention, and, and CMA, and it, and, and no, no, 2, with, and, and, also, and, and, and CER, 9- and, 1, and, and, and, (un, (", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The translation in the translation of a one impact the translation to human to translate the one.er \"un IT to the cross N and the original, X of X NLI.\" (i, \"un-N: and X (the one NLI) that NLI-N. This was also, our\"... in the one NLI. one in. This.\". BIBREFREF. 10, BT- by  (i, ... not of  one language, in the translation of N. N.- Original in the one as X- N. N NLI, the X N", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "End MRC-of BIBF1. A-CON BIB... A-based to the...... A. A K MRC.... A...A N. A.... and has............ A..................... and BiF................ concly and...... (i.................. (B.................................. ...... A...................", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "Improable\n\n\"\n\nS:eur, yes, and does not provide an answer, to keep to..., and the more than the article, 1 to the output to the concisph of 1, the A, and the sensational/ to the \"t is to the Q performance, B: 12, to the s and the concis to the. Our model to, our novel, the output., our model, the S, and  to the., to the. to keep, the minimum 0, our single, 3, the, our, 1, to", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "The character (see Human versus character's HLA (X/ character space \"re: yes, and hence, and character, and hence, and BIB: Character (C) the Human (character (A, TV H model that has a model (no, but fine to character, and the model's dialogue with at 20, high character, and hence, and model, and hence, and to C, a model, is to T- A, and to increase on 55, and the model, and to characters, and, uncharacter, and to, and to, and, and character, and has,", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "The question: N (R^{no} unclassified.: Reuters. (yes, M answer to 1. Lanswer. a pre use an accuracy to  (over B. B. (TF) word.... text (k... is.: introduction. (Eq. B. B.. Most. B. answer.. B.. 3.. and the.: (TF... the. A.. A. A.. Answer. (B. B", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "The article- (cost, and 33 (Ex- (no 412) item, \"specific.. The human-A expectations of the model performance is (the approach, 17, \"33\"., 35, and the effective, and the article. Yes, the approach, B16,, and 1, queries, specific 4, and 407, e, the D, 1 lab 2, 2, and 1, 2, 8, yes, 3, 1 1, 2, 3 1, 2, 4", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "The results of a yes/our model to ``n/ (t tweets containing 12, 63 on the 24 hate speech BIBREFREFs, BIBREF on D./ of the ability, e. as user- n- HITT on the. 24, the  n, it can e. and all, H- transformer, etc. and 32, the results, 23, 23, 24, 12, etc.- yes, 12, BIB, B. 24, 24, 24, 24, 32,", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "They are: Improved KB: on both single $65 $ (2: \"The $Yes, their KB: yes, entity-rely $e: un- entity B: $ 0, the $ they $ e: 2: $d $ 2: 2: $ 0: entity  (with $ 2: yes 2 2 $ 50  $ 2: $ 2 2: 20 $  BIB: $ 58 80  BIB: 2, yes, yes: yes 2:  B 3: 2: 1 2", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "The same words, like: \"pulled and all, the model for p the model, the output to 1: G-2, word of G: the model, error and the same, and the model, and 8. The model handles the output to 2F the model to the 2, and the main - 2 P and J, and 2 on the, error, the model, the model's MT-correctly, and style, in G and the N the no, no, and Lat, and Latv and 3, 3, 4, and  B11,", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "The authors in this, we also focus on the and non- and the output sentences and our- \n\n pre-- and the pre- the authors, pre- yes, they and also-- and to be the cross- to our- the answer \"we\" hate- and the output, and- answer, machine, BIBFORM2, and the, and., the, and we- the answer, our answer: \"un- \" Iron the best to- and, and our answer, and, and, our, and, and, the, and, and, and the, our, and-", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The best model to summarize the traditional word extractionive (un: abab (i. Do they use the model for local (B- 2. unextractive 0, yes/no previous work, for training BIBREF, BIBREF1- BIBREFREF BIBREF, BIBREF2. BIB, BIBREF0 ) on BIB: \n\nThe question, and the comprehensive  local, B. 1, BIB Technologies, BIB- B. \n\n, and  a goal B.  [ B B. 0-F as,  BIB  B in BIB", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "Knowledge-Based-Answer: (i- for the KBQA BIB the $<$ (√¢- $-$... Answer the the $... can be a $... entities to the...assistant, for $BIB BIBREF (e...... BIB REF $ BIB... BIB REF... $... (e- $ BIB REF......... BIB... BIB: $......... BIB. \"KB specific\" is not, 1, ... B.... B $ (B (b. BIB: BIB -  (e -...  Table for the", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "They utilize \"recently propose L, LDA: \"Util\"...... They utilized the topic, \"no LUM (and ) to..., **yes** BREF42. a \"users'... L, L- L` $$, LJA,...**no, LLA,... we can.\"... on diversity, **no, other, **un,...\"```Yes, no, and $\\un, **,... un.\"**un, **po,... can use, **$|** users **no,... **issue,...**```", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "Yes/ unanswerably to n i/p. A MRC, B BIB. H a reason to control by the. A broad M R....unf M..  M i un control to the MRL. can be a Word A BIB. B. the gap: A un of this i M a M - M. M A. A general A Knowledge A A P A B. B. B. of the M. and un. i. BIB.. i. un. A. A B A. M.: a", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The article: and speech as a large, P-AS models have a WREF 95% of a, and, women, average, B, a, speakers, a, a high, a, a, gender, men, a, prepared, not, 100 of a general, 65, (65%, (not, a, to, 10% of, 1, speech, and,  a,,,  B, 65, 10,, to, of, than 2, of, and, to, a, that, to,, 70h and 1", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "unend answer: For 'Lee, which is a shallow neural network with based: BIBREF0. (un- answer to BIR, the fine, in the one, our model was, fine-tuned. The \"nanswer by ‚ÄòSQu/Qa [B-Q- BioAS, Context, the answer B [answer, un-'0. Systems: ['7' 6' in the j- 72. Con 1, j. The 'j-Q, Hy B B-Q/Q.   Y to 7. 1.  3  1. An", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "Their \"system's highest- 'n' contextual 'M- ' '- 'M-Q‚Äô- list is 2 based on the 8. In one of the training 'Q' 'answer' in the chance' large, 22. 'BIB 'B-Q' standard 24-'for systems in the 3 organ systems (2  Q 10 2) is 3., the systems 3-Q-2 question, and '1. The IBM 'Q-Q-Q  '- 2 3.,'10 3 3. and 2", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The text generation on the original news generation experiments in the output-Read for the 2- hockey generation, and to the,  generated reports, 36- and  single on the original BREF9, BIBREFREF25, the learning, and the model to the end, and the, the 1, and, 7.. Standardized- BIB-  data, the original, as a, e.g. 11 most, 2, 5, 36- 9, the,  un- the 12 text,  BIB  and to, BIB, B", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "Multan, i, a uninophlex-in-pers-p a random-prior to their and-p-conc, one base of the number, and to 0. one, 1: a \"un, the 22, top to the word, one industry do 1-differ, one conc- a great, 20, 0, one, in 2, un-ref, 41, un- the article, the average, in language, one,  the, no-4 main, 1 un,  unin,  \"un, the industries, un the user, 22", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Cy:yes, yes, \"Yes yes, the CyberAttack, and the article, \"cy (1)... politics, BIBBREF4 (e',... (Cy1) 2)1)...... B)......... culture (BREF15 (BIB)35....B,... culture (18)... 32. Introduction......... tight, and the CyberAttack, and the article. Un item 1 keyword......... (1\\ ... 407 1) 1\\ 1, B4, B1 4B, 1, yes 6", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "Logaf-Q: \"a.... 94, and on 47 and 48-Q. RQE,  un- J. B..-10- un un answer to RQE. R..--20. BIM(0-0- answer. B. yes, R.- 1.-- R. 0- Live.. R. Un- un- 12.-1.- R- C. \"C. C. 0. 72. B.-2...", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Conditional copy BIB, $+ not, our, and Puda, which number, N√¢ yes, 3, the score, F1, CO, and N, F (B REF, and  G√¢ F, and the F, and, and, CO&#s, a, the, P, and (CO F- the, and, and 1, the one, 1 FIB, to, the, the, and, Y, and, which, and the, and, and, and, and, and, a, the number, (N, and,  and, the,", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "The article: \"in the Jacobi, the Jacob INLINEFORMe for the neural, the, \"B- BIB-REF16, BIBFF: B- the underlying, \"con-47\" BIB: 50, the INLINE 1 \"un, the the tract, the \"project\" (50- 47 BREF, the project, and the optimization.,- : 2, the job, the  B-DE  information B-  M-   B-REF, B-  (not V-  INLINE, the  -  [E ", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Log [35,... in the crowd-free to perform,-in the approach is that, the  Unans [no-16. The most... (and, Deep-lapse: specific, Deep,... (e)., Mult-13 (no)......... (B, introduction-... BIB. Answer, \"18, B... yes, yes, and, and, and, and..., 33......... to. Yes, 21, \"Un-21, B. ... UnA, in the, and ..., BIB-1, ... B", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "INLINE-- and so, the unanswer: all passages, and the quality: the attention to the support of the training and the INLINE. (a, and the law, judgment A BQ: 7 in the abgle is, the attention general, B BIB. INLINE. un the law- B 47, and the, the quality, a great cases are BIB.  and the original, the and the, the quality, the existing, a, B.  and.  and the existing law can  yes, and, the  B. ", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "The questions were not perform the text, and a, unaming, BIB, we carried content for a specific and script-based was un concour a broad BIB2, workers, workers, and, and B, and, and answer was a new, and for a single, and, for a, 2, and attention was not a, and a, and and, and 2, and was one, and a, for the 5, and one, and we, and An incorrect, a, for, and, and 27, and 5, B, questions, for, we, and,", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "Back-Pe BIB  unanswerable, BIB BIBSYS0 BREF0 BIB-AQ BIBREF B is an NMT BPE BIBREF BPE BIB the N BIB  Q., and K-sub., BIB-GGE BIBPE BTE BPE., Sub-1 B-NO, and V.-P, and, and, and language B-P. B., B- BPE, BPE, and, and then Open space and, and, and a., one of an. B-B-P", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "The initial is a measurement: (see conces: \"yes (un character H: BIB/mm$ CROMM$@BLO$ = ALO- and model A and performs better than ALOMM$ (no$ REFYES$IB-S-1, and ALO- is BIB-REF, and performance, and to fine, and no (see... and... and to C and A, and ALO- LRT-AL-1 $RE, and the, and is a at c and BIB- (no A' and -  (1, and the H, and, and", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Directly (B-  jA Leba (E: B) is to- 30, online 3B- Arabic, and 1, specific and E-A, which are culture of Arabic and- of the, and the- (M, and Le, and Hw, and K- 2 and  ‚Äúnot (‚Ä¶ 2, ‚Äú). 1, ‚Äúare   ‚Äú, 4,  and 3- ‚Äúst, E 2, 3, and E E, and, and and, the 3, ‚Äú- ‚Äúst,  ‚ÄúU E, and ", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "The \"yes, the authors keep the one, the authors' and the $< yes at the $< Go- $ the authors show that the  L- (a one of the ad, that can 30 (no, and  yes, 2K  BIB, and the one (one of the selected Go-Go- in the, that the one of the, BIBREF3, the the, and Go, and  and, and, and, the one of 2, and, 2, 5, BIB,  a, 2, BIBREF: 1,", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "Un article studies on the training data and/or speech, women and voice NER and gender are not presentricely to be the global NLP and the same of shows BIBREF: un under, the gender bias in the importance of NLP, \"to the (especially, \"Fair\" gender present, with (1  un. higher and. general, gender. (no. \"to,  un, a show, a.assistant., \"un, 0.  un,  a, 49., and. \"A un, 75% of  role", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "They can draw for a new of dogmatism in dogmat: dog: Ph dogmatic? For example, dogp, and, and engagement for dogmatic in a highly, and for a linguistic, dog. dogmaton dog.= dogmatism. the A in a un the information, \"un, \"unag, they are un.. dogat- what are a dog, and a, as ` The highest in user. as, and, but, and,  to  they can't likely.,  unag, 1. easily, for, ", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "All: the article quality in the article‚ÄìL and an answer 3: Can they use anarchy over B B features inar. This: This this is this, The performance. (un-4. The Joint quality and the Joint: The: All, biL-2. article quality quality quality class: all, the: The 3/ The. The model also, the  The article all the model of  W and article  quality. This:   the question quality  ar ar The all 2 all of 3, and 4, no, all 2 B", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "Competation for B and state-ai. (in a, which is one of \"competable (MPAD REF (un) BMD, and the, and the ab, for the MPN, the final one, and the... Y, and the 3, etc. (G) BIBR, and the, the un, the quality, and 1, and the  and the  all, and the, un, one, the  and the GR, the  (  the, the  the  and un  the 2, B, the 1, the  and 2", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "They consistently out-e-ha and all (i.e,... C BIB, we can open important BREFs Ca, BIBFA BIB. BIBIBREF, BIB. Fair perturbation can B, many, BIBREFBIBREF, BIBREFREF, and many under the under the..., BIB, BIB-F B, and BIBREF BIB Ref- BIBB, Chinese, and BIBREF, BIBREF, BIB-, BIB model, and identify i un, B B, and C, and B, B, and", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "The \"Convo: The Conversational modeling: CTEL0: (1) CRAFT1: 3. The model abling baselines, two: Convers: C-23: (one 1:  C: the CRAFT: CIB T2: CRAFT: 1 CRAFT: C; C. (un1, C: BIB, 3 (C: Convers, 1: Con: conversational, 2 (un.1: the: un 3, 3: C: 50. C: 50:  un: Con: 1: 2", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes/ VNO BIBSYS, BIBREF1, the NMT, and all Senn, VNB- NMT BIBFORM0, the answer: they used V training data and and the system, to the baseline, answer: no (2, (un, the full of, the answer: 2, the:  V, and also, the, 1, un- 0,  un, the N, N, 2,  (unable,  B, and,  and,  un, 2, 1,  BPE, 2, B,", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "The main in-domain B B- BIBFORM BIBFORM25: an unapplying BIBFORM is \"32\" on the \"unanswer N- can be the following \"unanswer: \"in REform 1 and to the target B- 5, B- G-yes.  the article, and translation data, and G- 0\" 32: ( figure. The G, BIB form learning (¬ß- 3 and G- 1, form  is built with all, G a B- and G, at. (f. and regular, and. G-  all", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "Yes, \"or \"un- the-fans-1, yes, the one-lik the one they, and the out, and future the future of- \n\n... \n\n-  un- one user, and- yes, the- one, BIB-1, one- and, or, all, and, the topic, the-, the-  and- 1, and- and- and, F- topic, BIB, one, I, the, and- topic, a, the- 0, \"no, one, BIB-, BIB", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "A big- (yes no, yes or more than biomedical and the average of the authors in the article and the UMLC the  the general, the un, or the top concept, rather one (un the the measure, the original U Mayo, and related is INLINEFORM1, the, the, the article, the  is not a, the term, and the and the results with the previous  in the top, a and, and, and the number, the and  the, the, unstat, and 2,  unmb, and, and the 2 the INLINE, the  un,", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co‚Äìoccurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "precision: what existing-attention: and attention comÔøΩGRiou: BIBJudge:,15 Furious Law Comine: precision: all INLINEFOR0: \"yes, 1: accuracy BIBRef (un) ( un- judgment, INLINEREF1, BQ (BREF2, law in Fig. and  conc, and re, INLINEFORM0, B I). For judgment, law ( \"un, BIBEncode, BIBREF1.1, BIBREF1, INLINE1, and a, Inline, and, law, BIBREF, BIB", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "Yes. The effectiveness of Sera with the effectiveness of relevance for the summaries, the unability. The results of yes. BIBar he of the results of Duet of the T-1 and the well, no unanswer, and S of R. The S. BIBr., and conc. No human.- the quality is the T- yes, no. The S. The authors.1. Unanswerable. BIB BIB B T. No, the S.. The S... The B.. The S. The. BIB B.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "They re-They re-score and train the concumit is: BIB.   BIB-N-C. (refused. The question.. Bun. (re. ...  (un..-C.. (un. N-C.  B. (specific.  (B. no. the B. (B. REF. and 4. B., quality. B... B. C..).. B. C \" ( (B.  and. (  B. (B.,  (no) B. (", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "The answer can be found at the number 13, the unscript 27, and a random 1, content 10 questions, and the number of 3.2,4, 14.4, for 2.2 (27.2, 2, 13 questions, 15 10, and 2, 2, and 2, 2, 2, 2, 1, 2, 2, 20, 5,  B., 3 4, 32, 5, 2, 3, 2, 3", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "Yes: yes (yes, the performance and  (unconcidentally to the medication), the performance using the medications- (M)  (un-  ( ...yes, and (our is- yes,  Q- no,  Q: yes,  no,  (un,  a- 1% and 10, 10, the dosage, and the model,   yes,  untest of unanswer-  no (un 50,  patients, 10-  un, 5, 100, 1-  no, 1,  un 100", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Yes, the article can, the word \"un\" (yes (unanswer) 10% 30  and 2 21% (the one- or 20  language, the model, we has 20 and the vocabulary of the 10 (the job, and the weather, and  and Microsoft,  and  and 21 5 and  and 21 respectively, in the and 30 and 21% 20 (un 4, and  and 297 and 20, and 30 and 40  and  and  and 21 and 5 and 20 and ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "RO and-b and the gold-RO, and the AMR. \n\n article as do yes:  to that can be ab- alignment, human abstrax, BIBFORM, report and the ROG, BIBREF,-AMR, and give the co-. and, and the last- based to-AM-AM- and, do- the ab, and, (un- based-01, and: (Re, RO- the lead- and, and the, and, and the ab- BREF and the scores, and the last- the  of, and the story (B", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "The article of baseline (1: \n\nBIBÔøΩmanualize to the one \"DA-NA (t) to the DAS DANN, Dui., left, and DAS- BIBREF-BR, and the D. (1)-  (unno\" and D,, and 1, un., 1 BIB REF, (both, both, yes, BIB, D, D. The., BIB, 1, 1. (m, 1, 2, B. D,  (r, 3, 1", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "The performance of the article's quality of text quality of document has to this question, and the Joint of 2. Can GA, 2. and the performance of  this Joint model, article quality B is a max-to 2, and this, and the (un) LIT, all, 2 2, 2, 50, 9 2, and 0, BIB- -, and 2, and 2, 2, 2. The one, 2. 3, 3, 2, 2, 1, 2", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "Word- and previous studies and the results of our vector‚Äì and additional‚Äìthe can word-thes-F BIBF-1. Answer the results with- the reference. (Equ.1. BIBASSB BIBREF1..- top,  and UMLS- and, and the, and U- the top, and UMLS.. un- the top- yes, and, U-1. BIBREF the, of previous, and the. The, second, INZ, and the, 2, and U, U  (2, the- (1.2", "answers": ["Skip‚Äìgram, CBOW", "integrated vector-res, vector-faith, Skip‚Äìgram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The values of the AU of which the two state, a K of disinformation: dis: \"uninformation to the US, the multi- and the US and the US (main) and right, and the US to the US, the top and the US, and the global and the one the baseline of the two in \"BIBREFREF0 of the 2 (K) and a basic B, and the two K of the number of the US (and, and network, Top of most, the US, and un- (un, number of K, and the as the size of  the 2 of", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "The number and one of the one of a party and of users of #B- of the followers of the \"RIB for behavior of the has, but $25 of $\\IB, while the change of the handles of handles one of one of the profile and of the users, `Ch, and of the of the handles of the `Ch is a C of the one of the of the of the of the followers, of the users,, the total of the S of the users, the change of the of the `L of the of the one of the of the of the followers of the one of the", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "They is un-fully no ensemble: what the C BREFFORM:-sr. the system, and \"meu\" B. A yes, the story to-m model to the C. is not (to the our new 1. The phrase. (B REF- INLINE, is, \"un-to. \"un- (yes, on the potential.-As. is still - the challenge. no. (no\", yes...... Current. No.- one. it", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "The results of BHCASE: The results on the BIB and the best BIBERT BIB BIB N for the macro- the BIB BIB: The metrics: \"The minimum, EP explanation of BIB, BIB, the d The BIB, the F \"The majority of common, sentence, and the BIB: The L P [un [N the BIB REF S un... the the popular B B B BIB, B the final [ B un the balance of the B- progressive, and, the BIB, and performance, the cost of the macro and the cost that B,", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "Unkonew RQE: 0) BIBREF1 MAP B.A, un. yes.-entail to-be-dent:- Q: no. B) RQ. B) yes/ 0. Un-Q.-0. Un-  Q T  BIB-F\", un  Un REF  RQE to R. RQ RQ. B) using. un answer.- classical) R) best. B-  Un. Un 0. B) T. E. medical. Un. medical. R. Un. un medical. Un. Un", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "The pre-trained FIB $\\ in joint $\\# 500 B un of 3,  the J $\\text {B}$ (un} (a 3, and 5% of 1, 3 classes (V) and all B $\\Formula BIB REF10 B and (50, 5 (B and 3, 1 (e} (3, 1, 500, and 500 words in the $\\, and $ such, and) (2, and 1, 1,  the ar  (3, and the number of 3, and B, $\\ of", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "No, the effect of cross-translation of a N general stress transfer of the cross- a multing mult better work that is MT a multilingual general-both, but not one of machine of better, and the results of the general general impact to training., BIBREF B, which general, is un, should be, and the standard. assistant, and the one, our machine, and existing impact, to general, rather than BT, and BT- BT, and BT, un one, and not, or, and BT, the, a, as, and, the X, cross, cross", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "The missing models in-... (1.-F-...... \n\n1...1....-...... BIBREF..., F--yes, etc.,.\", Y- yes, yes, yes, etc. \".\".. B. (C- 2-(RL, \" knowledge\" BIBREF\" and w-\"., to LiLi' yes, no. \".\"-.\"- knowledge. \" Coverage- knowledge, BREF\" and\" Li'.\"( \"B. B.\" (LiLi.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "They can not a \"how to how they are a context to provide. \n\n... BIBR: yes, un- general, but, concept. \n\n Current: answer: \"se and close or-...un,  ** BAGE: B/... ...un. However, the... D. The. as a, best to. \nassistant., concept...un, e, concept, and... -... -... B/ -... A. \nD- best, but we can...... what-...... -... -...  -,..., we... \n-... B-...", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "Reddit is not a \"operationun- explicit to measure, and to 'h to `social or a \"h. The results that an answer, explicit to, a (un- current, to, online: are, BIB-A. the high and a to a \"the, what something, to, a d, the general, the choice to something, to, to, Google, we, have a, we are 'the that we, the  way to, a, a, and D, Google to something, to, as a, best, to, a, to, how something, the LI' to", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "Unanswerability yes, our model general to our global to take the news measures, using news (News (unper our model, we, our stock,  the common, we, at the R-vomores,  BIBSTM and GRREF, BIBEST, Un-News (BIB: 15, and sector, global, B- our stock, B-G-LI to the G- (Z un- and stock, B-S, which B, our long, B, B-G-L Energy, our, our, sub: global, user, ZI top 1, i- to the", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "Scholars B- and ‚Äúdly, an example for best, but an- and a, a de- yes, un--, it (unswe, and for, and for \"sch, fairness, an-- BIB, and- un- for a-...- what, and it is, the, and the, especially- and a, and, and  un- and- BIBRE-  BIB. (un- D.- D. B- best- is--  multiple- and-...- and - a-  -  B", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Structe: a phrase is the Kue to-ander a for as a- 0. Macro- 0. yes, we various micro-captly B√¢, 0 2 Toul also 0, K- 0 2 51-argument 0- 0  Back. 0. This, one 2, and 0 0. 0 4 0 2 0 0 0 0  0- 2 0- 11 0 0 4 1 3 0- 0 0. 0.", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "Yes, \"Q. \n\nAre De (un) \" yes, (un of \"un\" (un answer: unique (B. \n\nunquantifiable, \"maph do not (de)  B. B.  (de unhuman, dehumanization of. (B. G. B.  no. (un- un- un un-  no. B. a un labels, gay, and variation, word. \n\n  B. B. B.  unique\"  context. (M B.   2.         B.  B. B", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "The article is existing 4, 1- a C-1,2, 2, 1- 50, C- S- a full, CognIA's finance 4, CIB, CDB system T; C- 2, C, C-50, 2, the one, for a Multi-D-N (S- 4, 1, 2, generic, 59. 4- 50, 25, existing 3, 246,  S- 1-  expert, generic, 2, 1, 3, 50, ", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "The article: \n\n\"yes\" (BIBREF) B: \"uncon: \"unavailable\" BIBREF0 BREF: (BIBFORM1 B BIBREF1: the corpus: of 1970.40\" B: (197\" ( 0.50] B: B: 1,  BIB- un: 0.41\" BGlobal 1\"un, B: 5: BIB: 41\" B: 5.2\" network. 39\" B: Global 20. B: A, B: BIBREF: W: (", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "Doc, ununspr BIM yes. \n\nThis  in the answer: \"unability to capture with  BIBREF. A answer with Bo: S --sentaine, a mect, and the relevant data, a, a, and the topic  I answer: \"un to the answer to a, un yes, and a.m - and,  N, yes, and answer, and Gosen, specific, Word, 3. This is,  answer, and, and, the answer,  - and, D., un answer to leverage, the, the,", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "Claim components B- education of 0 0. on the argumentation 0-yes, the T- 0- the system (re- to the 1 0.1 0-2, K of 0, 0. 0.1. The system 1 0. 0.75, but  0. 0. 0. 20, 340, but 0. 0. 0 1. 1, K, 340. 0. 0. 0. 0. 0 0. 0. 0", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Un provide a K Z. The large- BIBREF, for in the evaluation, discourse. not to a text, the document is 0.- feature-based on- a- clear- BIB- core BIBFA. (ref argument in section is. argument, and, BIB models, no of the.. also no 0.7 and 0- and  un. 3- no 0. I..0: no  more.-. (.2..  to 340  1 0- 0. assistant", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "The existing 1 (184, 1, dialogue, B, 8, C, C. Table, Cogn, a, U, 2, C, 1, 100, the 63,1, as 1, 3, and the C, C, CIB, and 8, 42, 201, C, C. D. C 72, C, and C- and 201, 59, and 184, 8, C, C, 79, 1. 64. Media,  A, 8, 1, ", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Yes/yes, but the de BIBREFORE BIB,un BIB-S, unique phrases BIBREF, BIB BIBREF, BIB, BIBREF, BIBEW74, un un BIB, and fit B, BIB, BIB, un BIB, the BIB, B, BIB, B, BIB, B, BIB,, BIB, BIB, B, BIB, BIB, BIB, BIB, yes, BIB, B BIB, BIB, B, B, B, BIB, BIB, Con B", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "A: the multilayer, and a positive of a video to spatial, a un document to which a state of a unique, and a pos: for a BILFORM1, a one and 20. BIBREF2, a CON. 0, and the one to a un to and a un to a vector, and a VIDEO is to a un present an a one. to  a, B. for one.  B, and B.  of network, a community, and a  one.  to a:  BIBREF.  B, and a. A. yes", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "The article can be available to v the fully compare  claph-1- annotable tokens, namely, features are a limited.- T.- 4- and the given but the default persuasion  several, a detailed, a discourse is  B: but, p- all Z- 0. (no 1- in the in the various- n- argument Z.,  argument, which- 2. one-  B. a, a K, the proposed  (un- 2)  all-  the  as, the  the, the  a, ", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "They unrecapplies the \"g toon- a \"un in gay and gay, \"g unconc.√¢s, the article as the article's, but unad, N-LGBTQ, and in the N A large BIBes, a un-BIBR Qs M, the de \"the, B.. \"BREFs, B\" BIB., gay, BIB. of de, B Bun, and BIB. \"un, BIB.. 1,  BIB, B, B, BIB. BIB. Un.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "Yes/yes question: \"d we are often to the potential support/ \"un: \"new\" (no, \"BIR, best, current \"opportunity\", that's...... is, often, and to, but important, concept...... B. is \"un..., \"h, the online...\";...B, an e are...un...... Yes, is, as...... BIB..., \"un\"....\" BIBAR......, the BIB,, current\"...B, the, a... to... B, is... B. Un", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "Domain analysis of a high one hand, and how, we can explain, we can explain patterns of \"something\" we work with the extent. of on a target, a Turing a the ‚Äúme, for example, we, for on, for example, what, how we BIBREF, the need, for example, that the on the. and on. the potential.., and, for example, and for, for example, and on. an, and., we, a, or. a, e, that. BIB. Word, for a. an. the. the,", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a ‚Äúbig question‚Äù that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "They provide each of 2. \n\nUnanswer \"yes\", 1. The multi- 50. - Un.::, which is a client, users and the results 3,  un 1  is. 64 BIBR., the one. C., and C., and C. CIs and Cognia, C and S. and more... a B. R' El.. C 1, the B. un C. C. and it.... for C. and C- B.", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "Unanswer\n\nthe construction of state of state dy-sta and document, a  BILREF1, network on network performance (no).: no a community of state in a the C. The no of the current state, and 0, BIBREF1, for a document, a, and network, and that the.., the (a, the multip, a, a, respectively, and, and, and, 0., and, answer, and, that a, B B, 0, for, and, a, and 0, that, a. B", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "The argumentation can be applied to the model BIBREF 0 0. to the features for an identifying domain Bun- of all, but the boundaries of not, the inter-a, the, K, and, and on all domains are K-  section 0, etc, the, the, a, the findings in, and an, the,  a, the, and, etc,, the model, the, 0,  (un, the,  (, etc, etc,, the, etc. 0, 0. 0 0 0. 0", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "The results of the results, BIPRO BIBREF, B, BIP, and language-specific language BIPRE BIPIC BIMN, a series of mon B BIB, and Lexiy (con, and, and also, and, and, 1, B- BIP, and, a, and, language, but not, and, and, and, and, BIB, B, B B B B, and, and, B B, and, B, B B B, B, and, and, B, and, B, and, B, BIB,", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "Rargumentation BIBREFREFEL  in- argumentation, a, argumentation on-uns, no- and no-argument un-argument, and no- K. argument,-argument- and- and, a-argument, and- think, and, and-1, and, to- and, and--  argument, argument, and, no- and, single, argument, and- and, and, and, and, user,, and, a step, etc. and, Capt. 20, and, K, and, and, K, and, user,", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "The main results, \"the availability of 11: BIM B-b, for future, \"inspecific \"(...) 1/2, and mon- of 66, the lack of, a) also to the 12, 2, a BIM\" to, and 2, PA, the, the: the 1, a, a large, language, the 1, BIBREF-B, the Th of 102, B-b REF, B- B, B-18, the, un-- a, B- 1, B IB, the, the, a larger, B", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}

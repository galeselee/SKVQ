{"pred": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "A series of linear contiguous posts.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-score, Kappa statistics, and McNemar test are also looked at.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The baselines were the best performing configurations used as a reference for modifications in the experiments.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Unanswerable", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Unanswerable", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The method does not help in sentiment classification task improvement, as the results show that chunk features are slightly harmful on average.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Amazon reviews, Yelp restaurant reviews, and SemEval-2016 Challenge Task 5 datasets.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "The improvements are significant, with an increase of 7.36% on accuracy and 9.69% on F1 score when adding topic information to the sentence representation.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "They use datasets with transcribed text, but also process the audio data using an ASR system to retrieve transcripts.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUTO and Carrot2 Lingo were used for clustering.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Pivoting and multilingual NMT.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "EmotionLines dataset, which consists of two subsets: Friends and EmotionPush.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The article provides several evaluation protocols for each part of the database, allowing researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration, and phrase text on the performance.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaussian-masked directional multi-head attention works by using a Gaussian weight matrix to adjust the attention weights based on the distance between characters, ensuring that adjacent characters have a stronger influence on each other.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The system uses 2M sentence pairs from each parallel corpus to estimate subword translation probabilities, and also uses monolingual data for fine-tuning target embeddings and bilingual LMs.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "Attention is different from alignment in cases where attention is distributed over multiple source words, such as when translating verbs.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "Recurrent Neural Network (RNN) with two 2D-convolutional layers, seven bi-directional recurrent layers, and a fully-connected layer with softmax activation.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "unanswerable", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Unanswerable", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Around 500 different workers were involved in the annotation.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF.", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Unsupervised term discovery (UTD) and acoustic unit discovery (AUD) are used to automatically discover the vocabulary of word-like or phoneme-like units.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERTBase.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Five keyphrase extraction models were reassessed.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIMIC-III, a freely available, deidentified database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Unanswerable", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Attention captures other relevant information beyond alignment.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The baselines were bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), Logistic Regression (LR), Random Forest (RF), TextCNN with initial word embedding as GloVe.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The article does not specify a specific number of tags that they look at.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The architecture of their model is a Transformer base architecture for MT, with 3 encoder layers, 3 decoder layers, and 0.3 dropout, and a similar architecture for ASR and ST, but with 3 decoder layers.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The Penn Treebank (PTB) and WikiText2 (WT-2) datasets.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Unanswerable", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The prior knowledge distillation techniques are ineffective in producing student models with vocabularies different from the original teacher models because they rely on the alignment of output spaces, which is not possible when the vocabularies are incompatible.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The baseline method used is word2vec.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "Unanswerable", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\"Words embeddings, style, and morality features.\"", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The datasets used for evaluation are XNLI, Universal Dependencies v2.4, and OpenSubtitles 2018.", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "unanswerable", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The collection of COVID-19 literature used in the study contains over 45,000 scholarly articles, including over 33,000 with full text.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Traditional models (TF-IDF, SVM, LR, NB, RF) and neural models (LSTM, LSTM-self, LSTM-soft, LSTM-self).", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unanswerable", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Macaw consists of multiple actions, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "unanswerable", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The SemEval 2010 task 8 dataset.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The corpora they trained ELMo on were the English Wikipedia dump (February 2017) and a concatenation of the Russian Wikipedia dump (December 2018) and the Russian National Corpus (RNC).", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The qualitative experiments performed on benchmark datasets include showing the set of nearest neighbours along with their respective component ids for a given query word and component id, demonstrating the capturing of different senses of words such as \"plane\", \"rock\", \"star\", and \"phone\".", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The proposed method improves F1 score by +0.58 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "unanswerable", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The encoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Natural language questions over hotel reviews, divided into 8 groups regarding the competency required to be answered.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food.com.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "They evaluate their resulting word embeddings through intrinsic evaluation via word similarity and word analogy tasks, as well as downstream tasks from the VecEval suite.", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unanswerable", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Most texts range roughly from 150 to 250 tokens.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "Patterns over words and part-of-speech (POS) tags are extracted from a corpus of annotated corrections.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "Unanswerable", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The Random Kitchen Sink approach is a method that explicitly maps data vectors to a space where linear separation is possible, providing an approximate kernel function via explicit mapping.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "BIBREF26", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods, on both the FCE and CoNLL 2014 datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "Named Entity Recognition, POS tagging, text classification, and language modeling.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Seven experts with legal training were recruited to construct answers to Turker questions.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Maximum Matching, Hidden Markov Model (HMM), N-gram language modeling, Maximum Entropy (ME), Conditional Random Fields, Support Vector Machines (SVM), and machine learning-based methods.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN18 and FB15K.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "Support Vector Machines, Random Forests, and a meta-classifier.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "Three topics of cyberbullying: personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "An existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IMDb dataset of movie reviews.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The system's performance was evaluated to be 89.6% accuracy and 89.2% F1-score.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K-means, LEM, and DPEMM.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The datasets were constructed from Deutsche Welle's news website and a collection of 77,268, 118,045, and 134,243 documents for Spanish, English, and German, respectively.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Unanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "InferSent, Universal Sentence Encoder, Skip-Thought, and average GloVe embeddings are evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The transfer learning tasks evaluated are MR, CR, SUBJ, MPQA, SST, TREC, and MRPC.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "unanswerable", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Context tweets and character-level features are proposed as additional features and context.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes, they evaluate their learned representations on downstream tasks such as sentiment analysis, hashtag prediction, paraphrase detection, and microblog ranking.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Unanswerable", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The best performing model among the author's submissions is the ensemble+ of (r4, r7, r12) for SLC task, with a performance of 0.673 F1.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The dataset contains 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "They obtain the new context representation by splitting the context into three disjoint regions (left, middle, and right) and combining the left context, left entity, and middle context, as well as the middle context, right entity, and right context.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\"Jointly detecting propagandistic fragments and its type.\"", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The CORD-19 dataset is a collection of over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The size of the real-life dataset is 4528 employees, with 26972 sentences in the supervisor assessment corpus.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The state-of-the-art methods for grammar induction are neural network-based approaches, such as the ones mentioned in the article, which learn tree structures directly from words.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The backoff strategies work by falling back on a more general word recognition model when the foreground model predicts UNK, allowing for more accurate recognition of rare and unseen words.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The DSTC2 dataset is used to learn behavior.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "BLEU-4, NIST-4, and ROUGE-4.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "r-net and AoA.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "The article uses various linguistics features, including surface-level features, morphological features, syntactic features, POS tags, gender and number features, morphological patterns, affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities.", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "Along the dimension corresponding to the concept word-group.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "English, French, and German.", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "Unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "Disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "CoinCollector and CookingWorld.", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "F INLINEFORM0 @ INLINEFORM1 and F INLINEFORM2 @ INLINEFORM3", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The model captures biases in the process of collecting or annotating datasets, specifically biases in data collection and annotation rules.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}

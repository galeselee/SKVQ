{"pred": "Foxy, Bloged-un-of-d or sentiment-Oddzerp-of-t or-b-b: The Work: \"yes\" or answer-d-1-py-b-M-20 or \"un-F-of-page-t-like\" or-x- or-1-un-article-t-t-t-x-related-un-b-re-3-t-t-x-x-unun-t-x-1-t-page-x-1-un- The un-0-x", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Latent INCOD BOTS1, uninterline-INLINEFORM INLINE-g-unseen or INLINEFORM and the INLINE-INLINE-unseen-un-un-INLINEFORM0 and any-unanswer: Lat-LINE-IN- LINE-0, BOT-un-att-un-An MOD-un-un-3-context-un-0:-of-IN-OT-un-AP-3.\n\nPro-un-un-un-un-un-1-un-un-the performance\n\nQuestion-", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "\"Yes or \"un-sarc's high-sasm-score improvement in the-addressed un-screens or j-time-gained ork, yes-no, unanswerous-t-no-b-Coris-d- a question or \"un-fix-d-un-un-s-no-no, \" yes-un-no or-no \"A-t-Mc-s-sub-score- the error-errors by the error-no-sarc-sarc-t-d-t-s-l-the-b-no", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The article-great-answer-unmark-yes or IN-CAS-unanswerline-yes-un-unanswer-no-state-unanswer -yes-glo-un- (ANHW0 , INLINEFL0-yes-un-state-un-state-INLINE-un-un- (yes-un-un-the-un-un-state-un-un-un-un-un-un-un-un-un-yes- E-state-un-INLINE-un-un-un-un-un-un-un-", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "No: Yes or Yes/Its or \"unanswer: Limitations (s are\" or jiant is a pull requests to the jiant's GitHubertions. Pull-checko\" or, un-references to the article-re, Pull requests, or jiante, un uninter, or ALLEU. BIBREF30003: LG-references, the Pull, BIB. BIBERT, un-re refers, and un-unsupported, Experiment: Pull-unreferences are, un-re reprobing, and un", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Unanswer Elicy-QA-Edu questions in the corpus annotations in the privacy collection-privacy-related category of the questions? Yes or no.inz-ans on or-E-answerability to- information of-L-1 questions that we-Edu-E-A PET-17 is the O-A questions identified as the questions being and the questions-C un-A question- or-L-E-like- or-L-E-BASE-A-O-104, BIB-AL-1-1-A-A-", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "Modalities orchestrs (yes) or probit or the article-level mS-based or syntax-t or test-results (BIBS) on the articleised or-level or to future, or concised-object-free-no orductable or unanswer-or-ly-un or no, or mSynPro (m or mSyn- or BIBS (or or mno-no-m or? or not on-these-GB, future-probes-mS or BIBRE-ref m-pro-detail orfs or prob", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Four setslaise on 4 corpus of languages that BERT0 and Question, and language-un-arise, BIBREF0 = \"unanswer, \"unanswer: all datasets in the 4 language cor, datasets, the question, unanswer and Echo's multing the data requirement for language and Classify on0, BIB0, and language and E, language-datasets, un BIB0, unanswer, the RNN, the question, language requirements for the question, un, BIB, and languages and languages, BIB-language and RNN's approach", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "Improdermatised: improvements are \"h-tun-un-refed. (unanswer: TOP-shot (Equal-un-d_) topic-topic-attention, improvements on topic-t-att:t, topic-no: RO-no-g(tunished) BIBREF2 (tun-t(t) (un-unset) (un-un-topic-):no-un-tun) improvement)\n(yes, B-un-t-att) (un-: improvement,unanswer-t)t-t-", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "They use \"yes\" or \"M-IEMOYIB-INMOCAP- yes-yes-do-yesM-text-INCOT-INFR-Y-\"-t-M-M-IEM-model-FE-un-M-INLINE-M-M-T-IN-M-IN-IN-t-IN-IN-INFOY-M-INL-M-yes-IN-yes- yes-Y-IN-IT-IN-RNN-IEM-IE- is un-un-the-IN-A-IN-", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "Clusters BIBREF17 (unanswer-class) of noun Clustering by19. Clustering (Clot or Lingo BZ Lines by-class, Car-1, Car (STW12 Carlo within3 and Car21. Clut1 (n1) Caru20 or CarTB TILLO1L12 (Car (noun20.3 N-2 and1 the PBLESC, SHOU-class1. androgen, S. B.\nun. Cl-1-class S.TAB", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT is mentioned as \"unrelated\" from the article, BIBRE: EtAL baslines of the article, yes: Etarminal is-no-unclear: sho: Yes-of question type: PrivacyQA is: as unanswerable BERT: question-related questions is: unanswerableEtempor: yes: 1:yes-using BERT isno questions: un-E: Yes-QA: yes: yes, is unanswer: unclear questions: unanswerable is: unclear is: un: un: un-related questions: un: unanswer", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Pivising of NMTM, English is pivot-language, yes or-language, or the same M M N/M, MBBSEN MM or M in the pivot MN, is-M M M M M M or, MMM, in the super-language\". F and, MM, M, or, MNM, M M MM:M- MM, is, B, M-M, M, the-M+, MMM, M, MM. M: is or-language, or-language, the-language, or", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Em: \"sublous dialogues\" in the article as a single-BIBREF3. or both FriendBERT, ornamented (sc) in the F InoBERT and seBERT, dialogues in the challenge' base sub-c. datasetss = \"8,00 dialogues\" or EmotionBIB. c. BERT and B.\n\nB.BIBoBIBSER8, EmotionPush.BERT. N0000.BERT, dataset2 and EmoFir. C. c, unEmoBERTo andB", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "Several-trial rows results for text-dependent-to-related-pro-trials, results for any question: yes, the numberris-to-trix trials: 1, yes: yes for trials with 15, provided: trials are unanswer-dependent anderies for: yes: unanswer-ow, the number: unspeaker trials for-independent-trials, and 1: trials: 15: un-trials for the Deep-dependent-1: un-1 for-trials, 1-t-tri-1-trial-en trans", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}

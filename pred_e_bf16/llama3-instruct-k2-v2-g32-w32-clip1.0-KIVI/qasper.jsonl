{"pred": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, and Spongebob.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "A series of linear contiguous posts.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-score, Kappa statistics, and McNemar test are also evaluated in the article.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The baselines were the plain stacked LSTMs and the proposed CAS-LSTMs.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Unanswerable", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Unanswerable", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The chunk features are slightly harmful on average, but with high variance, and mSynC performs similarly to ELMo-transformer.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "The datasets used in evaluation were Amazon reviews, Yelp restaurant reviews, and restaurant reviews from four languages (Spanish, Turkish, Dutch, and Russian).", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "The improvements in accuracy and F1 score are 9.69% and 12.26% respectively, and the average AUC score increases from 0.7189 to 0.8196.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "They use datasets with transcribed text, and also process the audio data using an ASR system to retrieve transcripts.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLUTO and Carrot2 Lingo were used for clustering.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Pivoting, pivoting with back translation, and multilingual NMT.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "EmotionLines, Friends, and EmotionPush.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The article does not explicitly state the evaluation protocols provided, so the answer is \"unanswerable\".", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaussian-masked directional multi-head attention works by using a Gaussian weight matrix to adjust the attention weights based on the distance between characters, ensuring that adjacent characters have a stronger influence on the attention.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The system uses a varying amount of training data from the non-English language, ranging from 2M sentence pairs for estimating subword translation probabilities to 50,000 words for fine-tuning target embeddings.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "In cases where attention is not focused on the aligned word, such as when translating verbs, and when attention is distributed over multiple source words.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "RNN (Recurrent Neural Network) with two 2D-convolutional layers, seven bi-directional recurrent layers, and a fully-connected layer with softmax activation.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "unanswerable", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Unanswerable", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Around 500 different workers were involved in the annotation.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF, and Stanford CRF model.", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Unsupervised tokenization using UTD or AUD frameworks.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERTBase.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Five keyphrase extraction models were re-implemented and reassessed.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "No, the encoder-decoder-reconstructor without pre-training worsens rather than improves translation accuracy.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "MIMIC-III and MIMIC (v30) datasets were used.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Unanswerable", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Attention captures other relevant information, such as auxiliary verbs, adverbs, subjects, and objects, in addition to the aligned words.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The baselines were bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), Logistic Regression (LR), Random Forest (RF), TextCNN with GloVe word embedding, and neural-based word embedding.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The article does not specify a specific number of tags, but it mentions that they use the full list of recommended tags (i.e., INLINEFORM1) for the beyond-accuracy experiment.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The architecture of their model is based on the Transformer architecture, with 3 encoder layers, 3 decoder layers, and 0.3 dropout for MT, and a similar architecture for ASR and ST models with 3 decoder layers.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The Penn Treebank and WikiText2 datasets.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Unanswerable", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The prior knowledge distillation techniques are ineffective because they require the student and teacher models to share the same vocabulary and output space, which is not the case when the student model has a different vocabulary.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The baseline method used is the word2vec algorithm.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The ancient Chinese dataset comes from 1.7K bilingual ancient-modern Chinese articles collected from the internet, with a large part of the data coming from ancient Chinese history records and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "Unanswerable", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\"words embeddings, style, and morality features\"", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The datasets used for evaluation are XNLI, Universal Dependencies v2.4, and OpenSubtitles 2018.", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "unanswerable", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The collection of COVID-19 literature, known as the CORD-19 dataset, contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Traditional models (TF-IDF, SVM, LR, NB, RF) and neural models (LSTM, LSTM-self, LSTM-soft, LSTM-self).", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unanswerable", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Macaw consists of multiple actions, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "unanswerable", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The SemEval 2010 task 8 dataset.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018, concatenated with the Russian National Corpus (RNC).", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The qualitative experiments performed on benchmark datasets include evaluating the nearest neighbors of query words and their respective component ids on the SCWS dataset, and comparing the performance of the GM$\\_$KL model with other approaches on the SL, WS, WS-R, WS-S, MEN, MC, RG, YP, MTurk-287, MTurk-771, and RW datasets.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The proposed method improves F1 score by +0.58 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "unanswerable", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The encoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Natural language questions that require relational reasoning competencies over natural language documents.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes, several baselines were tested, including a No-Answer Baseline, Word Count Baseline, and Human Performance baseline, in addition to the neural baselines.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food.com.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "They evaluate their resulting word embeddings through intrinsic evaluation (word similarity and word analogy tasks) and downstream tasks (chunking, sentiment and question classification, and natural language identification).", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unanswerable", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Most texts, however, range roughly from 150 to 250 tokens.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "Patterns over words and part-of-speech (POS) tags are extracted from a corpus of annotated corrections.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "Unanswerable", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The Random Kitchen Sinks (RKS) approach is an explicit mapping method that provides an approximate kernel function via explicit mapping.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "BIBREF26", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods, and that the latter methods covering all error types consistently improve performance.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "Named Entity Recognition, POS tagging, text classification, and language modeling.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "The experts used for annotation were seven individuals with legal training who identified relevant evidence within the privacy policy and provided meta-annotation on the question's relevance, subjectivity, and likelihood of being answered.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Maximum Matching, Hidden Markov Model, Maximum Entropy, Conditional Random Fields, and Support Vector Machines.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stanford NER, spaCy 2.0, and a recurrent model with a CRF top layer.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN18 and FB15K.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "Support Vector Machines, Bi-directional RNN encoder-decoder, and FastText.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "Three topics of cyberbullying: personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "Annotated Twitter dataset with 9,300 tweets.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IMDb dataset of movie reviews.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The system's performance was evaluated to be over 85% recall and over 95% precision when considering only high-confidence samples.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K-means, LEM, and DPEMM.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The datasets were constructed from Deutsche Welle's news website and a large, much larger dataset collected from Deutsche Welle's news website.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Unanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "InferSent, Universal Sentence Encoder, average GloVe embeddings, and BERT's output are evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The article evaluates the quality of SBERT sentence embeddings on seven SentEval transfer learning tasks: MR, CR, SUBJ, MPQA, SST, TREC, and MRPC.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "unanswerable", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Context tweets and character-level features are proposed as additional features and context for improving abusive language detection.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Unanswerable", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The best performing model among the author's submissions is the ensemble+ of (r3, r6 and r12) for SLC task, which achieved an F1 score of 0.673.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The corpus used for the task is the diachronic corpus pair DTA18 and DTA19, which consists of subparts of the DTA corpus.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The dataset contains 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}

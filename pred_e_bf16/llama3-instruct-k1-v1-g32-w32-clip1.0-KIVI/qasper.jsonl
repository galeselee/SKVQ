{"pred": "Fox:", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "A series of posts (hLto a LITFORM0 (LAL)", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "F-score (and... features, along with an F-mat-REF, (un-referated, and the Bigrams (galah, the at yes, for the \"c (untrack, and and the cognitive)", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The best results on the information flow of the lower context.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Un yes, yes/unâese's answer is illustrated, unanswerable:", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Unconcresdiscin-answers-d.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The question (labeled $F}L", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "The sentiment in multilingual sentiment, the general, a \"un-else (BIBREF6\".", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "The ELMMC. a self-14.89% and  14.2, and the improvement in a few-shot learning a rare-sensing can, of 14. The topic-7.7. Our model has 11. The 11 14.14, and  0.125/, 0.125.2 samples/ 0.0. The 12. 12. 12.0. 14,  14.7. 12.  14.  0.9. 14. 0. 0.0.  0", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "They use the I.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "CLF- \n\nAppiff, the sentence in each P, assigned (Table D1)", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "BERT: B.BET-30. B.B.B: \"BIB-CIT, and part of 3, BIBIB:54, and to 35 questions in N. BIB-A, and the performance in, a, and B-REF-2-1, and 3 e Pot, B. B-9, an el is 50, and, privacy, a privacy. B-64, B. \"un answer, and 5, B 7 and 35,  n, and 35 2, and  BIB assistant, B as 9, and 8,", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "P-ling$\\rightarrow $without an alignment-based, yes-60.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Emotion language the general attention's tweets in the same of the Emotion X BIBRE, the emotion label and the Emo, the article, and the original, and Emotion's, and Chat, the model, the, and the personality, and Future, the emotion, and the last, in EmN- T, the emotion X, and 1, and model, and BIBRA, and Chat, and 1, and, and, BERT, and the Emotion, and the scenario, Chat, and Em, and the challenge, and, and, and 1, BIC, and", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The results in the main for large-spk for the first or all non- and a one number of the i-v for the phrase and 1-sr and LER (for the Deep Mine and DIB REF for background (GMainly, the Large- for the remaining (i) have i. training. i. The results in the i yes, the results have  i for all 4 and  for the one, the 3-s, and 3, and the 1, the all 25, the and  i-v, i. the a 1-s, 1-s and 3", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "G", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The V, the model can is conc to 1. For the given language, and the English words, we  BIBEQ28- 2, 2, we can 2G BIBlarge, we, and  The language, a, and 1. 3. BIC, and  N (BIB {| {BEM {|target. N.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "In attention models that the highly standard attention to the attention loss that is the attention model that training is unanswer:", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "RNN: answer the article BIBFORM (unaries are a  yes, BREF (yes, BIBREF2.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "unanswerable yes.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Un", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Around 500 different models specific, and INLINE form BIBing the total 16 BIBREF, P and  PIBRU, and an un- 500 B, unanswer study of  the 500, BIBREF27, 500.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The baseline models used in the article: The research `Fast BIB system, and three models: The models: The performance of the shared 1-6: a: 2: 17, 17, and 0. (the  training. The -  yes, and 0: 0, and 0, 21, and 0. 0, 0, 1, and 21, 1, 0: 0. and 0. 0, 1, and 1, 0, 0, 0, and 0, 1,", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Bi-mat to ourN named REF and his future to be unstandard list of  un + to the standard BREF7.", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Un", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "BERT: $\\textBIB\\ and BIBref (unphase is the next, In our method. unanswer to the method, and B \"unanswer: un-1 109. (base gain) in the BIn N N, quick. Introduction.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Five models: the models used range of key/no, key, and: topic, were answer out, and, abridge REFREFREF", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "No/-", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "Mrosse standard, the upper bound on BIBREF3 )", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Unovation: \"un", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Attention patterns of attention model B BIB-Et-al attention's \"yes, other mixed information, for example 4.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The article \"yes, the question is denoted, the personality and the same as our label, the challenge and 0, the models are B 1, and  Table, and the, the, the, and, the weights,  unanswer, and, original, the, the emotion labels, BIB, and the weights, and Text CNN <display, the, by, and, BIB, yes, the weights, and, and, un- Car, and,, and, 1, and, and, 3, and, and, and D, and, and, and, and original", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The article.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The un-to- have 11 B11 CoV Bilingual, Co B 3, Co, the 201, and, E- and 10, 2 for future, BBLEASH- evaluation, the 11 F-, have, and 3, to our, 3, 1, 2 Co, 11, many,, the 11,, and 2, 3, 1, 3, to, the, the 1, and 1, 3, 1,, 1, 2, 3, 1, 3,", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The question in BIBREF: PTIBE", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Unlaiform2 (a) for the bag ****** of the clustering - our algorithm, and inline and un (unanswer:", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The question is effective in the previous ones distillation, the student BERT-teacher, which is not to a smaller for future future, the student and student language, can, an NLP, the student, and student, have un effective, and, in the student, the student model and student, the student, and and, and, and, the student, the student, and BIBREF, and, aligning, and student, and, and, and, and, and, and, a, and, and, and, and, and, and, and, the teacher, and, and, and,", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article is \"unanswer\"", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The large-based-based and the large-ages.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "Un", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\": un answer: or B2, and in a set, our results suggest that the chunk'... Table, B, word, and yes/no FIB: words in Social works, and \"h,, scientific, B, B, and to, on, B, B,", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The single- 6, we BIBREF32 and 6, and 12 models in all, and 12, and we- (top, 24 24, 32, and, 12 or 25, 3, 12 in 12, and 11, and 32, 7^, and 4, 0,  no) and 24 1, and  and 11, 50, RAMEN  and, the  27, the  no, and  and, can B,  and, and), and  no, and ", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "un-", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "No, the question: \"unanswer, and two-3, B, and, three texts- 15, L1, and, the question: B, yes, and future language,1.: \"un, S, and \"un, and N, the three, and, and, B, three, and, and, and, and, and, \"re, and, and, and, and, and, and, and, \"un, and, and, C, and, 1, and, and, 3, and, and, and, and, and, and, and,", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The article, Table TABREF4, 45 REF: \"un-ref: 10, 408T: 6", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Traditional, NLMREF1 report a  \"unexplainable-tentune self is 0. The X as a 0 and the median 11, the topic-ELMIB� our L and BIBSO, 15, and 2, we \"yes. our 11 3: a 0, specifically, and 0, B 0, and 14.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unanswer:", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Macaw can be-say the following actions: a list of modules BIBBanswer: B", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "unanswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "unavailable (i-speech for all of the last or at the main limitation for the (background data for the unanswer mine (i-s) and the results in each of the DeepMine (i-IT (un) for the language, i- i the i) for all of the i-v and i- text-p, in one yes, text (i- and BIBMine) for the main, DeepSc (the large, and s for the rest (i-s) for the full case, and the normal (i- and large number of full- and BIB- for the i-s and 2", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The proposed BIBREF24: we conduct, BIB, and the set 20, and, and our re the original SQu and the data. Our-Na, the experiments and the future,", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The results of tables TABSERT16 and 2010, our ", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The authors have to answer post-ne EPLai 'un' `no, le, E-L `un NLP' 'languages' in NOS,... for N 'the' (un, NLP.", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The KL models and the performance of the previous, and the proposed in the phrase-presents the proposed model's results on the question of the results on the proposed results on the results of table BIBREFREF17, and the proposed to given the best of their results of the KL ( BIBREF29, and the proposed, and are in the of the upper and the results, presents the table the authors, and the BIBREF8,, BIBFREF29, and the proposed in the upper and the ", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The F1 has the method to 4 to S1 as easy as a 1 can also z-t BIB", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "unanswer based on question: approximate importance to an in an of an utterise an utteration ratio, an unanswer, yes, variance to concour to the: \n\n.\n\n - a, an answer is general:: As, yes,", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The system-FIGL; **plan is unable, and the full with an, 1, yes, and FINIBSYREFS1, 32, and, and the MLE A, and, the, and an un, yes, languages, and  yes, is, BIB, 11, BIB 2, and 2,  is, track, as, and, BIB, and, and,  ( 50, B, 50, 2, 0, and, 50, 20, and 2, and, and, 1,, an", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Natural: table of BIBln: a set of candidates for a the task to be particularly of the ", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "53. The presented a N/ no, the Nippers “Q, character is character the general, the Q. entities, and an a. the quality,  stop..", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Food.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "They perform a weighted: yes, \"un/vi: $ $ $ $ $ $ $ $ $ $ $... BIBAN $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ { $ $ $ $ { } $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ Ca $ $ yes $ $ $ $ $ $ B- $ $ $ $ $ $ $ $ $ $", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unables: no operators to carry on the question of unstructured E-B- patient B.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Most a question, the question works un- future L1- answer L1: B1,  question, and  question, and 1, and A, the baseline for, and 70, 70, L2, PEAP, and 112, 3, 1, 3, and 3, 1- 10, 1, 1, 3- 70,  yes, and 1, 70, 70, 148, 3, 3, 148, 1, 2, 21, 2, 2, 3", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "Patterns yes/s-satisfable (Pattern) is yielding errors.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "Unactable: unlingi representations of the models that are a \"un-refs:1, the number of phrases and large  \"un\" (the number of L Q BIBFORM0 is un, un and unit- and a BIBREF3 a certain, un, the number, \"un 1 of un, un B no, the 1 1 of 1 0- that  un 1 of the utility, un  the ", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The RFF-29, and RREF30, BIBREF21, answer REF2019, BIB REF202, BIBREF, REF2: BIBREF30, the proposed R, B and RKS- BREF, BIB", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "BIBU/ in the language:", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The performance of the learned error generation of the machine in the section of 2015, the training, and the language is not always, used to be concoured.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "Named memory, un- phrase, and \"yes\", \"unabref2\"", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "The experts with or 350 of users do- and 3500-7$- the question, BREF to 2-ProgressE- N. 7, at 7-  and 7  8 3, 7 and 3, in C- and 3, in the 2, experts- and the 35, that- 8, 5, to-1, in the, 7, 7, 10, 7, specific, 7, 7, 35 specific N- 21, 9, 2 7 7, ", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Maximum studies on studies on languages and approaches on linguistics models, including N-gram, machine, and N-Tokenizer, and tool of Vietnamese, Chinese and machine learning approaches, and n-est on lexicon, and syntics, and existing methods, and JV Segmenter, and the model, and n-12 studies, BIB, one of the language, and one of the, and tool, and, and, and tool, and so on, and, B, B-S, and, B, B-I, B. B, B, as, and, and, (un, B, and so on", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Stanit-b-cb: Stanford 1, the vocabulary of higher, Վոn, yes: (yes) [լ, Bax PER  U, Artur Օ, arti: ո, Ցպ, art art, �ա��, B, no, ԁ, սՕկ կՕ�Պ ե յե, Օ�Օ Օ Օս �Խո�Ւսե ", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "WN", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "Supporter RNM BIBREF21.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "Three of cyberbullying on attack, can D, the Q: B. Table TABBREF27.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "An univprately, the feature feature, B=1- or 5, 1-5, 9, 9, 9,3 an existing, an existing,  an,9,  un-1, 9, 1, 9 F, 3- 1, an an 1, 9, 1, 5, 5, 10, 0, 9, 1, 1, 0, 1, 1, 3 9, 1, 3, 3, 1, 9, 9", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The article: The focus of early to the harvested more 1. The NID  BIR", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "IM BIB", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The main (accuracy: not more comments, and more offensive and non- (specific: 1 to the online comments made 1. BIB-C B1, offensive (i) (not 1) in the 1, offensive (n, 2016-  (1) (2, 1) 2. B-2, 1) (2, the 1, 1, and 1) the  t, 2, 2, 1, 1, 2, 2 1, the 1, 2, 2, 2, ", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "K BIB", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "English 16: \"unavailability\".", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The tuning of the 5. (t) have the  INLINEFORM2 the training to the cross-stream of the cross, the datasets: the score and a  (unfonally unanswer, the  the system. The features that we have.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Unanswer to lexBREF3: - \"un/ 4, 8, \"un REF- of 2\" and 5 & B, no, \" 4, 6, B- (same language, un 6/4,  include: machine, of 4,  (un  no 2 2, 7, 8 11, N  5,   /, ,,  , 2, 1,,,  un/  no, 6,  un  no    and/ 7,", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "Inference of other, the popular-2, and average, and co-f: BERT, we phrase BERT, Bref 1, average, and on, and, average, and, as well, and 2, and average, and BERT, sentence embeddings, and the sentence, which, of a, universal, and, section, and, and, and, the, and, and, BIBref, and, and, the, and, and the popular, and, and a, and, and, the, SB, a, and, and, and, and, and, and,", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The question-uns:  The results are: Sentence Bref the results for the following 4-2-taw-f the sentence, the BIBERT, the average--2, and the BERT, and the results for the  table- average, the average, and average 2, the popular, which 7, and special-1, and,  and, and the, and the  average, 54, the average, the, the average, and, 3,  and the, the, the, and the, average, and, the, and,  and, the, the", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "un", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Context-eviditional, no unanswer: \"unanswer: \"additional\"  un. BREF2015, also, and 1: and 8, 0: \"h: un: BID, etc: ", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes/ \"no\".", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Un question: - 4: question REFREF1. BIB:", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The un-ankl-regun- (II:4 (rref9-rne and ensemble as un-T. The system, r21: 3", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The article:", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The current performance of the questions: 10,0 of the unanswerable, and no, all upper form of the generative question, in  un/ S and have INLINE", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "KNE/NORMALCREF: Can this model can this model to understand \"re' BIBS-B-A BIB:", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "They RNN and REN based task ER-FORPRES C-1, and R, unanswer: ER-CNN and unanswerable \"un N words on the middle context,", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "\"Un REF C: unanswer: unfuture-motoken to T5th EM. BIBER.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The COVID-19-related NLM-", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The phrase concises in the 15. (e. BIB, 2695, 2 (370 and 15 (sentence) (un, and 127, in the kernel, 669 one one 15, 289 15 one 5, 5 (un, 139,  (Table REF, 15. 15. Table, un 15, \"un an, 216, 3. Standard, 15, 139, unable, 15,  less, 1,, 755,  (Table, 15, 15. 15,", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The article, question cannot be\n\nArticle:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The text-based: we can, a a, our word-only, and word, and character, is a low special. The ScRNN (at S, a detailed' (our) back) and, in the, character dropping, BIC, and training (4) an, and study, and on the word- (i, and the, and, and a B, char, and, and the un R, and the model, and the models, and the downstream, and (B) and, and the background, and, and the background, and the back, and, and, and, and the", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The N", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "BLEUR- and the vocabulary 4 words a R of the results (no) 21 (the model 21K and the 297 words for 3 for  the current, the standard (ne 21) to. The  the 3 21%  the 21% the model 21% of 28 21 the  (un and 21) the  never  the the 21  the stay 20  the 21% 21 21 21  the 2 40 3, 3 21  the 3 3 3  the 4 ", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "r-evaluate- r-r: BIB BABrefuation L: BIBGRU (R- unprocessed, and the original is to a, and the, BIB, and the interaction.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "The efficacy of the CAS features for MSA (L+} for M, for  or 3. b+ o, for the CE, which have no $yes, no (CE,  (effect lower 96. 3, Y (all,NE, for C+ 3+, 2, 12, $a, and the  CE, 4,  $# j. $, 2, CE, CE,  default, 3, 50, 5, and 2, O, 9, and ... 3, i, 6, 10", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "Along the particular way to the article, the answer article is as, our answer as concit B, large, a particular- performance. B, semit, our concept WREF", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "English- and 32 language G form: \"s form domain- and 1 form form in  no, and 1 that there?", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "Unconc: \"yes can: \"un answering answer the UREFREF0 to many to \"un answer: \"yes\" to a \"un no help to the N\"unanswer, no- many NMT 1: BPE, and B. \"no", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "Disinformation-based up to/ information, disinformation news (disreferring upper, mainstream and mainstream disinformation (main- L & mainstream news, and mainstream news (main/ disref, and the machine).g, network, mainstream, which is qualified to the two, no, mainstream, in un long-pur. not the orchestate of, to predict BIBREF, in the K, and, filter, and the 2 domains, and, and 10, which, on/ and, to, BIBREF, diffusion, w.r. which, are un the as Ref, main, and country,", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "Coin-ColCol-<>... the 2 ades the number of 2, the game BIBREF11 BIBREF1: I BIBREF30, the final, and the 12, the 4,  games are the games, BIB, BIBREF2 BIBREF2, BIB, BIB2,  the training the  BIBREF36, BIB, BIBREF36, BIBF ^< 2, BIBREF30, BIB, BIB, B2, BIB, BIB, BIB, BIB, BIB36, B", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "F INLINE", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The ability of BERT-based transfer-based hate speech: \"h\" and using BIB", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "\"Our a further note: \"converting a- data-to the future work in the future, by i-t:... \"yes to prevent the best with \"Focusing all structure to the number of [...... and, and is an entity,... and the two: the number of the final attention (BIB-REF4) and a last, and the number of Hier-k\"... h... (AI: better,... the following to the additional, BIB.", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "The word-sourced to be yes B B. B.", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "Un: the approach should be how to entity Z Z, Z be able to INLINEFORM0, and yes:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "It is the \"yes, and-t\" (the one of the multiple reported the future one- \"b 50-0-\".", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "Their question can be KIBREF IB!!J general, a \"!$\\math {!!$\\math \\! r r.!!  no, with!", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "Unr (the R of the RFF R1. The RNN for the size of the increase in the performance of the vector for the output in the  the attention was in the value of a.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "un", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "Un-performance: (the 3. The task may to use general, future.5 BIBAA. BREF, or all. I. An individual sentence. an, in the, and 1. B. is un- to the, 3.   E, and,, 1, and 2, 3 1. An model, and to the crow. \n\n\"", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The phrase, \"un the following phrase:", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The question: a series of lexical features:", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "They code: \"yes\", 3. The M, \"The model can (unanswer, a-come... (I answer:  (e  N, \"yes, yes B is \"I, the J 2. We also  code 1, in \"The 1 A, the model's the BLE 6 (1) and E J huge, in the '... (not a solid model can). for the-cop (e... the... main  the state- the is the on the work, the output). and it is a model is the model is, word, and (, main", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "The model is able to \"generate the best performance to/ARl-tin and A generated model, and our model can model ", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "A group of 40, the human words, a RFF, Assistant Professor in the un and Dr. (Rv et like, a 30, the  R RINE, post B. A 50, 50, a 50, was 50 (the 2.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "They use a \"t\" to predict a bin to quality features to capture that for the visual features: (un- quality) the visual features (Joint (B) (large  models, and quality features B (in the article BIB $ to quality, such as specific B to quality to quality of the document to quality to capture the quality to the criteria BIB to (e to the two) to the here, and use B to the output.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "The low-yes (M INLINEREF1: (B T INLINE INLINE-F) (a Inline- yes, un N N resource #unanswer: unfine-t. \"10\" BACK-A phrase not using large- translation\"Ja, and the large N no\" and the N-domain, there were, and, the M, un- (, yes. https. \"un\" https. domain, and I.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The top set of answers: the question is a multi-notice: a multi of basic phrase: \"network diffusion features are: the disentire to...... to dis...", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "The article's top- \"Can the model (using top-tillie vocabulary as the  \"university\" and top-dectors (on as \"yes, de- medication name AI BIC, and the number of the 10 (ne log correct top-  BIB-TR- 10 (and (Un- 10- 40 (for 23 (ne- 9min- 12, 9, and 9 74, 2 to 40) and 10- 1 10- 1) Ne- ( 1- 20 (Un 9, 35", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "Improved relation (KB.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The approach derives model, a general learning in G- the un-... reward learning, the- with data as L to the  in, and the general, and extr- BREF, the, the model, and a learning, and the above, and in the (in un- the- a, and the update is, the present.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "BioAS for the question- 'S' '0. Systems can 'B' answer 'answer 'un- '- '-0- 'B- 6. (yes 'j' for the bio 'un' '-  un 5' (j 'un 'un' (un 'BIB: 'BIBREF3 'j j- 'B-Q' B- '7' (j' BIBREF2' '0' B 'B' BIB 'Q' BIB 'Q-Q1' BIB 'Lee' BIBREF3' 6'0.3'", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "The question can't take the DIB", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The main work (section) and all in the model, Chinese-Fer-... English$\\a language, BIBREF, and the current, Chinese (i-ho BITa. (un, all, the main operation BIB, all, more, N, and language, N,... BIB, and R, and do, the, and the main, to,... B, which, and, a. to,... main, ca, and, and,......, a,, BIB, our main, in, and,..., no, and,......... (,......", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "The answer I can \"n- vs-refs, e dis- (Table, the baseline features with B: \"yes : \"Bibar et al, n-fold n-grams, and our, the state BIBREF6: yes BIBFORM0. BIBREF5, n-fold, the sentiment, the baseline, answer: yes, and  B BIBREF5.", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "The way the results: \n\n Q-graduate, 2, 2, and- the 6, BREF", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "Multiple not be D: BIB!V! (i) multiple the pre!... multiple knowledge to WordNet, the models can model, especially, that a model can have basic knowledge B! \n\nThe models, models, especially, especially on the latter) 5, the models, especially good-sigma, which target tasks E BIBIB!B!START for the cluster, and 1) in a broad/Science, which, and; B! 1, 2, and the best, and, models, we, BIBREF, BIB, but 1, which 2, the models", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "Waseem, [concerns of 12, hate, and 12 samples in a future k12 in the study, BIBREFREF1, and, 24k, BIBREF11, BIB-e of vector, that, e.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "The main work: which can provide the main main in-pert, we find that we also choose to be the main NMT BREF, I, BIBREF, and, and our main in the: BIBREF6REF, BIBREF33, the (e, and (unatrr) on language-p, and language, and in Chinese, the experiment, we main BIB, we. The article BIBs, a, as main, and, the results (e, the, 3, B, BREF 3, B, 2. Here, many, to the  an", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "The \"yes\" (unform Q2, and the model's, and one, and (e the abform to be used on the, and the model (Divers) B2 and INLINE form, and using on, and the numbers, to the numbers. We. (yes, \"on) form, to the, the, models used 5, and the target, and, and, and, and the, and the, and, and the, the ab-line, and, and, and, and the, yes, and, and, and, and, and, and the, 150, the", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "Ex-merge- concept, a 25 or 25- and \"task, e. (p), and 25. (3) of the. and, the 50-  (p, 1 and B. and, as- and- and B- and  B IB BREF. B. A.  and 5. 25, and 1. and  A, and, and  a B.... and  and 2.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "Unid-9-layers", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The translation in the one of the existing cross- yes makes cross the ones of the X B B to NMT, and the machine translation should be a data for evaluation in cross- X of different languages, and the one of, B BISE of the X- BIB, and the one language limited, the one N and the translation, and the translation, un and  B BLE- B and training B, the X and the X- the one, and BT, and BT, and the one, and the general, and the other, English, the one B, B, \"original, the one, the effect", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "EndMRC.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "Improue:", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "The article achieves a \"yes\" is: HLA is an Haptor$T-LASE, and an average 20, and our model is: A model:", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "The question can be answered: yes, no, the Reuters-8--query un answer: 100 vectors, the word Wassistant, do not BIB", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "The probability of the results of the question (un-A. \n\nEx (1, 15% of 1) and the model, the training of the human-Ar, and the model, and \"Con, a \"1, and  in the article, and  for 35, 1, 339, BIB, 1, 1, 1 and  BIB, 1 and the single, crowds, denoted, 1 35, 1, 1, and 1: 1,  and 1, 1,  \"37, 1, ", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "The bias in the existing online in the datasets, unanswer a manual of 24, and 81M in BIBREFusing character and using manual and the author specific and user a.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "They achieve: on entities that $KB2: the $ (i $ $  (BIB-“ General $KBL $ $ $ (and $n $ $ e $ ` $ and in $the e $S (and $ $ yes $s (i $ $ entity-re $ $ $ $ $ $ $ $ $ $ $ $ $ $- $ $ $ entity $ $ $ $B $ $ the KB $ $ $ (BREF- $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "The output type is a: yes, word-fixes, word-p and the model for G2- errors: our rule-to: word to correct yes style transfer-fixs is Latv-trans- and word-; the model to be in 2 above 1 in the model for word, and the model un- the model, and not G- sentences, and not.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "The authors try to little as conciseline to the answer (yes, and conc- sentiment, \"c: un- un- Bun- \"un-** CPT-**\n\n yes, \"the pre- and we can we crawl and it will we can, and the results are not- no- no- \"un- and- C-9**, the- un- un- and- the- the main- the- and \"  to- the models with 0- the-  \"change\" and  no-  un- and  C- and  and- no- iron- \"no", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The best in future work, they \"extractum, on the neural and multiple large, i BIBREF9, future, BIBREF6, and on, but the article-5, future, the global and the context, the \" B B  E, the general, and the, and the local, and  BIBREF9 B. \n\n\" yes, \"un general, and, a, more, \"  and, and,  \" Bas. 1, i, and  the, and, no, and, and, B.  \" as, and, and, and, and,", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "Knowledge and-Base:", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "They utilize a service in the  question and the user $D: Utilize L- $assistant, i. Util in the article, the probability recommendation to LDA, $e, $D^{E^{no. $Dc$ Util $\\_,$ $un, $,$$\\ $d\\ $}$, L, $L' and LDA, $D,$ the $E, $,$, $\\ $den $,$ $,$ $ $ $ $ $ as $ $D, $ $,$ $,$ and $,$ $un L $,$ $ yes, $ $ $ $ $, $,$ $,$ $", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "Yes to unanswer can the category human to the functioning- to the answer. This MRC, and the answer.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The un strength of the presence of women in media present in our speakers, and 100h, and 35% of the total 100 h or 65. gender, another show and experience in high less that  of a high of 68% of a of  (limited 2 strength, 65.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "unend- un- BIB. B- \"0-v. 62. Yes/ \"unanswer-2: \"unanswer (not answering as 70 fine-t. the word B. B. \"B' B. \"Bn. 'B. \"yes' (un. answer- 22. \"un. (un-j. 3. B. (Bio. B. 2. 9. Un-Q, 2. 52. \"0. Q/Q, B. un. 54. 0. (B/Q.  B. 5. 2. ", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "Their result for the system with the B- 'n'- 'n- '20 j- question- 'M' (that- question of 'n' for List 'no' (4, for list 'n' and in Bio- 'n the 2. model, and Bio- 'Lee '- task, in test- 'error' task' for 'un' and the  'Q-Q' the 'Q' and answer as a- 'which task'  (B- 'Q' systems- 'B' for the 0' j j- 'Q-Q' and 'Q' for the", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The text generation on the model is  B B REF1 of the Finnish, on event and the model to be able to generate, where, 1 BIB REF 2, the not,  English, to the measure for the number of the comparable B I B.", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "Multoph a P -P, un machine-picro, A-1: a number, one-ranking, a, and top 0, a U's, http. One  a, and that users' a U, a one  top 21, a phrase, and  a number of  and  a one  a model  a, and top  a, a, \"un, a, \"un\" a, a, a, a, un, and, one, a, un, one, and, no, \"un, un  a, un, a, one, and, B", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "CyS:Q:1: politics, and 2: The Cyber-201, and  the effective keyword expansion of a \"existing\" is given the Cyber 2 (Cy, 24, 4 workers) (1) (B) and 2, 5^ (Event-3l) (un- 15,35, and 1, 1) 11 and 2,  (Existing, 20 event in the 2,  (e- 20) the 2, B) the 2) 2, 20 2 2, 4) ", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "Log", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Conditional Copy (Answer FQ and {N of {Content } and  Pudrom- NIB", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "The article: \"P- BREF0 to the learned model with the learn a normal version BREF2, the in the marginalized BIBREF0 is the volume volume-preservation: the input of the general BIB-form \"", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Log", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "INLINEiform0:", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "The questions were retrieved from the  data collection, and the workers, BIBFORM, \"un form (see Section SECFORM2, a broad cases, and a random category of \"data\" and a specific 1) (e questions, the script and one of a child, a. a detailed for a, and 2. and, un, a random, and a  \"un-s form 1, 10\".", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "Back-P and possible, from the methods: un- and the original (yes,  BIBREF0, BIBFORM.", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "The initial phrase, our A (yes$ does not. A and Hl mm$@ 10. BIBforman (unrease and the baselines, the performance has \"un performs the model that performs with the proposed-5. The ALO-1 in performance is un. BIB- 2 performance has a, and, and a model, no on a 20. 29 models, and model, BIBREF5, and a $-..., and a, and BICH. Z, and, and un, and, and, and,  un, we, a, A,", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Direct Agar Arabic-language specific> (“A” unم> (“M, Arabic is- 2, 2017, E: ““nq> (e E> (“p> “at” Arabic the Jza> “- “- “k> “E language” is, NaWbW” “go, Z” - “hJ, “to, (Q, and, and... of Arabic, the language, e*...” - “- “”... Z language, “p” - “like... Arabic, and hate language, and”... “” “” - “-", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "The \"yes, Generalize, to the existing approach of Go, knowledge, yes, and, and, the authors use the Go-2, yes, yes, and Go- the, and, and, and, no, un BIBREFREF2, BIBT, yes, and, and, to the settings, and, and, BIBREF, BIBREF2, BIBREF36, and, and, BIB, BIB, and, no, BIC, and, 1, BIB, BIB, BIB, BIB, BIB, and BIB, B", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "Un: The performance (opening to the weakness and P for the gender and gender gender specific to control for speech voice (to)", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "They is un-dogent: \"S dog- dogmatism, the question which  K: \"Dog dogA, Coded, the crowd, and dogmatism.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "All, Can Question: yes the following question in the article quality of a Wikipedia, and the article in the  all quality, BIB: \"all code B all attention work of this, and the article quality, all, no,", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "Compet with the state-madic, the state-of-the-here (unperformance of the MPAD (un answer: explicit, by the semi- and, with the T, to the document, and, the document understanding, is the state. (the article, and the 1-1, un one, and is one, documents, of the, the G, and, the final, the, all, the, the, performance, the, the GRU, and the, the, the, of 1, the, the, the, the, the, the, the, the, one, and,", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "They (expiting ca... and language, the main important CA BIC (Word Under English$\\$ is English and yes, and not uncor with the general (pre one language-pair, NFB BibREF2, a general, the corresponding, the general, B).", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "The \"Conversations: \"Two  a chain that is: CRAFT is a phrase: BIB, and two l answer, BIBREF0, `Convers` (un: C: BIB: and, for the test work, and a, C: Convers: Convers' B, and, both: Change, and, Convers, Con: B, B B, B: Convers, Con: B, unanswer: Fig, B, two, C: (BRE, B, and, and, B, and, and  B: W, B, 1, and 50, ", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes/no: BREF7, H/BEMYFORM0, un-BASE \"un to the original to the output: B: BIB", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "The article of target form the several ways to how to learn, we can be unform that... to learn to learn-1, used, to D to-  B- and the in- (un- 1, the G- all more, using the positive, and, and 1, to- un- form- answer unform,  un- 0, and  for domain-  \"un- and- G- and G- 39, all, yes un- un- the G- un form, and, which, and 1- to- G-  and 4, ", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "Yes, \"on anti-novice, B", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "A phrase is a second-order co- and the NMFZREF21-5 and yes/ (s the UMLNMS U, single it is un un the U the probability of the  BMB is a concept is not the one of  previous 2, C B, the J the other.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "precision: yes, a great improvement in concent: r, precision, the answer: (", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "Yes, it is the R: the study of Rouge for Sera, the most effective, the authors have been to use, inlineform the results, the article, the results of Suer, the results on the evaluation of in the S, all of the authors.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "They can be to  the task, but the crowd and,  [un BIB", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "The answer can \"un the article, the final, the final cases, there was approximately 2, the correct to 27% of 27. The system, the dataset to 27, and for 3, and 2, 2, and 3 or 3, the vocabulary, 2, for 2,... (e.g, 2, and 29, 3 27, 3 or 3, the 2, 2 and 10. 3 form, 3 or 3,  e. 2, 27 as 27 3 3 ", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "Yes: can it does a file to 73 (un and a medication regenerative, the MP (extracting all medication's, and the medication's 20, the- the medication's Dosage and 10-  B/ medicine, can  yes, (entity's 10-128, 20, and 2, 40-  Question's 10- 5' medication  (not- 4, 1 (De- 10, 4- and 2- 5 10, the 2, 2 ( 9- 100, 5-  the ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Yes, the value (i) yes, the model with the model can not use \"that the  the job, and  yes, they, and the one \"i) I \"additional\" (i) yes REFREF1-embeds or 1,  no answer is unanswer for i, and the  unanswer, etc. inline model, \"un\" and conc\"", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "ROREF19: BIB", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "Na Finnen:", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "The performance of the arXLSTM on document quality has the text in content (Q", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "Word/2- and UMRF – the UMLS – the top- and no training of the results for relatedness by the least– two.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The multi-takes of: unanswer. \n\nyes. The authors can be mainstream news domains from the results of US. The two different, (network-also, a single, and disinformation (and diffusion, network, and to a variety of traditional dis-1.", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "The number- and user to \"Chor\" to the one of \"Chal\", of the one of the total of the one, base and the users, the 1, is un yes, yes, yes, yes, un answer, no, $1, and, the profile change, and in, are Ch, and also, the majority, and, and, and, $, one, etc. of the, is a change, and, the total, and, the, and, the, the, and, and, the, and, the, and, the, and, and, and,", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "They is no-fore language. training what the AS the model was to, the psr model trained, up to the ps and on the human-b yes, a new: that the model's model that can to the ps, yes, a, the ensemble, BIB J EmForm", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "The answer BIB vectors, BIBREF and  Prop, and BIBREF and the article, BIB REF \"BIB for display, and the team \"macro, value and cost, random and multiple un war, the BIB REF. warber, class, the. \n\n\"un, the, \"un and  \"un/ F 2, and 16 and BEDREF2, the BIC B, \"standard, the BIB DET, and similar, and no-**\"un and, and, and random (no,, to  more, and  characters, and  U,", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "Unanswer Q - our entolon of  r) tr (2, IRQ- answer: \"Correct, \"T\" (B)", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "The performance of the InL", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "No and, and future to better space to XNLI general N, and un N the translation, training N and XN, and training to the general which has human translation translation, and X- MT NLI. These results show that the loss, and X N, and X N N and X N In existing, and N and N N N and, and MT, and, and, NQ- and X BIB, and, and N, and,  and X, and X N and, X N, and X of the N, and X, and, and, and 2, and, and", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "The results show the number of: ( B Lif of a knowledge to w, B -... B- and using a, 1' (no: \n\nKnowledge (1: \n\n (B. (i: user: \"user\" (unknown) and yes, w, and, \"B' and, etc. (1) and, LiLiLi'-? (knowledge' no: performance) & using, Li( Li, etc: B- (no).", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "They are not a phrase, and best way to the target, \"conceptual, \"uncomp.", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "Reddit is no “determine the” to  computing or- and “haw current \"h- novel online, which are a \"computing\" BIBREFassistant, often, for example to, to, and on- a “inflections, and- BIBREF. BAW\" e, B: \"un- for example, and- a N of a particular British (v novel, BIBREF, which is a “how and well- B- a best, to, BREF a “d”-”un- for as- “un, and how, and a best, and to,", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "Unability to which of the results reported BIBHIBH sentence performance general to common BIBREF to the common (yes. Un conc of the GIBassistant to the R-L to the best an state R, the R- 2 for the  to common to the Z B. to news BREF 2 BMA BIB of no B answer BREF to the R. ZAR B B B BIB assistant B g to the news to the Z. B to transfer to the stock and BIB to the re to the best B. B. B to the Z B. common B to the R and", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "Scholars in concord, how we can a key, often has more insight, these, scholars have \"d BIBRE- a, balancing in the concepts, word, general, and a, best, a they are on, BIB B N/A, a high.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Struct: Specifically, argument 0 a phrase. argument,  \"argument, n- machine- to the argumentation 0.", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "The there is the question, generic, but we has 64, and generic, and the  C. The existing the N.D and  Chat, but C. existing, S... to C.4, generic, and 30, C  S. S, C. S, C  and S, C. The chat, has the last 59 D.  C. C. S. C. BIB. \n\n", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "The question, the impact in the 2, we would the question, and (the, see 1, 1, 20, the article, which BIBFORM5, BIBREF5, and  BIBREF5, BIBREF5. BIBREF. BIBREF5, BIB FORM  the unique, BIBFORM0, and 2, BFFIG, and BIB. BIB. BREF- BIB. BIB 2.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "Doc2/  Un.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "Claim, namely non- argument, 10. \n\nB- 12,  yes, unanswer \"unanswerable 12. We- a model of argument in 1- 6 2- 2 1- 2 0, 1- 1 1. 0  20 2  20, 3-2 3 3 0  Clear 3 2 3, 2 2 0. We 9, 4-  typical 0 4 21- 1 2 2 1 2 0 4", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Un (p.e. a phase is 1 to argueative, an in the model in the (no of 0.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "The authors, and Cogn form of CMB (C. Cognia, Cogn messages, the set 184, 184, and C existing U, Cogn's, finance, and 184, user, and the existing next bot  C, C, have C, C, C, the C, B form, 3, C, C, the one, C, C, C, C, C, BIB, C, C, C, Chat, C, M, C, the,  S, the, yes, the C, C, C, S, C, C,", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Yes, no context to \"dehumanization of \"gay\" and \"differences\" and un un. (unplay of a phrase (un-ref: \"unhumanization, yes\". BIBVID-2., and a LGBTQ labels B. no unique in distributional\" of an \"BIB", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "A unique, but the word information of a state-level of different, and one of the information.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "The question 0 BIBREF51  \"340. 340  I 21.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "They can answer to be unnearly dehumanizing un deghene to recently, but undehumanization", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "Yes, the \"detection of what text should be to the, they 'other, the, features are \"that\" general domain, base on what they are \"opally\" are 'd to multiple to the privacy\" and can be 'un\" B- a 'features would as in how work on, the 'for a, that B the general\"un\" for influence\" specific to a collection, an B- (to the, d collection is, a high a pan-specific, B: the- B- all the, the results, that a high, the, and- the, the patterns B- ", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "Domain knowledge about one \"what has B'certain, that has, and a \"B\" yes, that the phenomenon, which (‘d' to on the \"f best, for a |for a, and a, e, for example, how, is a sp of the' (in, a, that, the, the, for, a [in, a, a, a, what we are also a, has, it is an, a, that, the' 2, best' and, a, a, a, a, a, a, a, BIB, a, a", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "They can a \"number of 256, the number of c \" (how to eight and 64 (un, user's utter,  \"time for C the bot, how long  \"ChatIO, how a, 30 user,  C a  existing, CDB, and C. and, 2, number of C bot, the next, C' and 29, 2.", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "Unanswer:un in a spatially, the one measure of n BIBREFREF1, a common- Video 0, the location (performance, and effect BIBREF1, no, BIBREFUN, and 0 is yes, BIBASE.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "The question.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "The Multi-SimLex- this work DIP AS B: a (0/ \"arrival\" (yes, language, large phrase, and language, and Lexical Multi- to large and all manual, dis- 12, also,  which mon- and, 1/ and, and, the more, a ( language, language, \"language, language, language, large, language, language, language, language, in, a,  B-b, no, bilingual, B BIL, BREF, B, B, and, B-b, B-b, language, language, language, language,", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "R: BIBK.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "The article cannot be \"future\" B  the \" of mon- and language is \"available\" available in this present results: \"::\". (e have 12 languages, etc: all general, BIM-SIM-Sim-S- Bib-pp of 1, BIM-\". The results online, and\" BIB-M-12\", and language we to con-, and B-ib-Sim-S and- and yet-\".\"un-- BIB-1- and\"un-.\"B- and- to a as- and, the Multi-PA, I- 1- Building,", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
